{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "module_path = home_dir + '/code/modules/'\n",
    "trained_networks_path = home_dir + '/trained_networks/'\n",
    "fig_dir = 'figures/'\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "import time\n",
    "import cv2\n",
    "import datetime\n",
    "import importlib\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from keras.models import load_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport distance_metrics\n",
    "%aimport data_processing\n",
    "%aimport plotting\n",
    "# %aimport pso\n",
    "%aimport pso_parallel_training_queue\n",
    "from distance_metrics import minkowski_distance\n",
    "from data_processing import *\n",
    "from plotting import *\n",
    "# from pso import *\n",
    "from pso_parallel_training_queue import *\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set plot variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard plots\n",
    "model_name = '6x6_tanh_xi10_loss_point9_cutoff_no_empty_bin_punish_nbin_weighted_loss'\n",
    "mode_of_progress = 'validation' # 'training', 'validation'\n",
    "\n",
    "data_type = 'val' # 'train', 'val, 'test'\n",
    "iteration = '691'\n",
    "\n",
    "plot_full_range = True\n",
    "\n",
    "model = load_model('trained_networks/{}/{}_best/iter_{}.h5'.format(model_name, mode_of_progress, iteration))\n",
    "training_data_dict = pickle.load(open('trained_networks/{}/{}_best/training_data_dict.p'.format(model_name, mode_of_progress), 'rb'))\n",
    "\n",
    "predicted_points = predict_points(model, training_data_dict, mode=data_type, original_units=False)\n",
    "title = 'Iteration {}, best {} weights, {} data points shown'.format(iteration, mode_of_progress, data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get reinforcement learning plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig858 = get_smf_ssfr_fq_plot(model, training_data_dict, galaxies=None, title=title, data_type=data_type, full_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig73 = get_ssfr_plot(model, training_data_dict, unit_dict, galaxies=None, title=None, data_type=data_type, full_range=plot_full_range)\n",
    "fig55 = get_smf_plot(model, training_data_dict, unit_dict, galaxies=None, title=None, data_type=data_type, full_range=plot_full_range)\n",
    "fig33 = get_fq_plot(model, training_data_dict, unit_dict, galaxies=None, title=None, data_type=data_type, full_range=plot_full_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig73.savefig(fig_dir + 'ssfr_plot_3_loss_func_4x4_400_iters_Z00.png', bbox_inches = 'tight')\n",
    "fig55.savefig(fig_dir + 'smf_plot_3_loss_func_4x4_400_iters_Z00.png', bbox_inches = 'tight')\n",
    "fig33.savefig(fig_dir + 'fq_plot_3_loss_func_4x4_400_iters_Z00.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get standard pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = get_pred_vs_real_scatterplot(model, training_data_dict, unit_dict, data_keys, 'Stellar_mass', pso=True, title=title, data_type=mode,\n",
    "                                   predicted_points = predicted_points, galaxies=galaxies)\n",
    "\n",
    "fig2 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, pso=True, predicted_feat = 'Stellar_mass', \n",
    "                                binning_feat = 'Halo_mass', title=title, data_type=mode,\n",
    "                                predicted_points = predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig3 = get_halo_stellar_mass_plots(model, training_data_dict, no_true_plots=True, title=title, y_max = None, y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type=data_type, predicted_points = predicted_points)\n",
    "\n",
    "fig4 = get_stellar_mass_sfr_plots(model, training_data_dict, no_true_plots=True, title=title, y_max = None, y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type=data_type, predicted_points = predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3.savefig(fig_dir + 'halo_stellar_mass_plot_6x6_tanh_xi10_loss_point9_cutoff_no_empty_bin_punish_nbin_weighted_loss.png', bbox_inches = 'tight')\n",
    "fig4.savefig(fig_dir + 'stellar_mass_sfr_plot_6x6_tanh_xi10_loss_point9_cutoff_no_empty_bin_punish_nbin_weighted_loss.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig5 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, pso=True, predicted_feat = 'SFR', \n",
    "                                binning_feat = 'Stellar_mass', title=title, data_type=mode,\n",
    "                                predicted_points = predicted_points)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6 = get_real_vs_pred_same_fig(model, training_data_dict, unit_dict, x_axis_feature='Halo_mass', \n",
    "                                 y_axis_feature = 'Stellar_mass', pso=True, title=title, data_type=mode, marker_size=20, predicted_points=predicted_points,\n",
    "                                 y_min=None, y_max=None, x_min=None, x_max=None)\n",
    "# fig7 = get_real_vs_pred_same_fig(model, training_data_dict, unit_dict, x_axis_feature='Stellar_mass', \n",
    "#                                  y_axis_feature = 'SFR', pso=True, title=title, data_type=mode, marker_size=20,\n",
    "#                                  y_min=None, y_max=None, x_min=None, x_max=None)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6.savefig(fig_dir + 'proof_of_concept_3x3_net_one_output_no_weighing.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create progress figures and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lin_loss_func_slope_10_point9_cutoff_empty_bin_punish'\n",
    "\n",
    "modes_of_progress = ['validation', 'training'] # 'training', 'validation'\n",
    "data_types = ['val', 'train']\n",
    "\n",
    "plot_types = ['smf', 'ssfr', 'fq']\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "frame_rate = 1\n",
    "\n",
    "for mode_of_progress, data_type in zip(modes_of_progress, data_types):\n",
    "    \n",
    "    model_dir = 'trained_networks/{}/{}_best/'.format(model_name, mode_of_progress)\n",
    "    model_fig_dir = 'trained_networks/{}/figures_{}_weights/{}_data/'.format(model_name, mode_of_progress, data_type)\n",
    "\n",
    "    model_list = []\n",
    "    for file in os.listdir(model_dir):\n",
    "        if file == 'training_data_dict.p':\n",
    "            training_data_dict_model = pickle.load(open(model_dir + file, 'rb'))\n",
    "        else:\n",
    "            nr = file[5:-3]\n",
    "            model_list.append([file, int(nr)])\n",
    "\n",
    "    model_list.sort(key=itemgetter(1))\n",
    "\n",
    "    for model_file, iteration in model_list:\n",
    "        model = load_model(model_dir + model_file)\n",
    "\n",
    "        title = 'Iteration {}, best {} weights, {} data points shown'.format(iteration, mode_of_progress, data_type)\n",
    "        if 'fq' in plot_types:\n",
    "            fq_file_path = model_fig_dir + 'fq/' + 'iteration_{}.png'.format(iteration)\n",
    "            get_fq_plot(model, training_data_dict_model, galaxies=None, title=title, data_type=data_type, full_range=True, save=True, file_path=fq_file_path)\n",
    "            \n",
    "        if 'smf' in plot_types:\n",
    "            smf_file_path = model_fig_dir + 'smf/' + 'iteration_{}.png'.format(iteration)\n",
    "            get_smf_plot(model, training_data_dict_model, galaxies=None, title=title, data_type=data_type, full_range=True, save=True, file_path=smf_file_path)\n",
    "            \n",
    "        if 'ssfr' in plot_types:\n",
    "            ssfr_file_path = model_fig_dir + 'ssfr/' + 'iteration_{}.png'.format(iteration)\n",
    "            get_ssfr_plot(model, training_data_dict_model, galaxies=None, title=title, data_type=data_type, full_range=True, save=True, file_path=ssfr_file_path)\n",
    "            \n",
    "    for plot_type in plot_types:\n",
    "        image_folder = 'trained_networks/{}/figures_{}_weights/{}_data/{}/'.format(model_name, weights, data_type, plot_type)\n",
    "        video_path = image_folder + 'progress_video.avi'\n",
    "\n",
    "        images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "        images_with_nrs = []\n",
    "        for image in images:\n",
    "            number = image.split('_')\n",
    "            number = number[-1][:-4]\n",
    "            images_with_nrs.append([image, int(number)])\n",
    "\n",
    "        images_with_nrs.sort(key=itemgetter(1))\n",
    "\n",
    "        images = [image_folder + items[0] for items in images_with_nrs]\n",
    "\n",
    "        frame = cv2.imread(images[0])\n",
    "        height, width, layers = frame.shape\n",
    "\n",
    "        video = cv2.VideoWriter(video_path, fourcc, frame_rate, (width,height))\n",
    "\n",
    "        for image in images:\n",
    "            img = cv2.imread(image)\n",
    "            video.write(img)\n",
    "\n",
    "        video.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie with all three loss conditions in same fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00_tanh_Halo_mass-Halo_mass_peak-Scale_peak_mass-Scale_half_mass-Halo_growth_rate_to_Stellar_mass-SFR_test_score5.48e-04'\n",
    "training_material = 'mock_observations' # 'mock_observations', 'real_observations'\n",
    "training = 'backprop_and_pso_trained' # 'pso_trained', 'backprop_trained', 'backprop_and_pso_trained'\n",
    "mode_of_progress = 'validation' # 'training', 'validation'\n",
    "data_type = 'val' # 'train', 'val'\n",
    "produce_images = False\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "frame_rate = 1\n",
    "\n",
    "# trained_networks/pso_trained/mock_observations/6x6_2.9e+04points_redshifts00_tanh_exp30_loss_minFilledBinFrac000_fq-ssfr-smf-shm_weights_0-1-1-2/validation_best/\n",
    "\n",
    "model_dir = '{}/trained_networks/{}/{}/{}/'.format(home_dir, training, training_material, model_name)\n",
    "folder = '{}figures_{}_weights/{}_data/all_losses/'.format(model_dir, mode_of_progress, data_type)\n",
    "\n",
    "dirs = list(os.walk(folder))\n",
    "print(dirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00-01-02-05-10-20-30-40-60-80_tanh_Halo_mass-Halo_mass_peak-Scale_peak_mass-Scale_half_mass-Halo_growth_rate-Redshift_to_Stellar_mass-SFR_test_score5.91e-06'\n",
    "training_material = 'mock_observations' # 'mock_observations', 'real_observations'\n",
    "training = 'backprop_and_pso_trained' # 'pso_trained', 'backprop_trained', 'backprop_and_pso_trained'\n",
    "modes_of_progress = ['validation'] # 'training', 'validation'\n",
    "data_types = ['val'] # 'train', 'val'\n",
    "produce_images = False\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "frame_rate = 1\n",
    "\n",
    "# trained_networks/pso_trained/mock_observations/6x6_2.9e+04points_redshifts00_tanh_exp30_loss_minFilledBinFrac000_fq-ssfr-smf-shm_weights_0-1-1-2/validation_best/\n",
    "\n",
    "model_dir = '{}/trained_networks/{}/{}/{}/'.format(home_dir, training, training_material, model_name)\n",
    "\n",
    "for mode_of_progress, data_type in zip(modes_of_progress, data_types):\n",
    "    mode_of_progress_networks_dir = '{}{}_best/'.format(model_dir, mode_of_progress)\n",
    "    model_fig_dir = '{}figures_{}_weights/{}_data/all_losses/'.format(model_dir, mode_of_progress, data_type)\n",
    "    \n",
    "    root_path, redshift_dirs, files = list(os.walk(model_fig_dir))[0]\n",
    "    redshift_paths = [root_path + redshift_dir + '/' for redshift_dir in redshift_dirs]\n",
    "    for redshift_path in redshift_paths:\n",
    "    \n",
    "        if produce_images:\n",
    "\n",
    "            model_list = []\n",
    "            for file in os.listdir(redshift_path):\n",
    "                if file == 'training_data_dict.p':\n",
    "                    training_data_dict_model = pickle.load(open(mode_of_progress_networks_dir + file, 'rb'))\n",
    "                else:\n",
    "                    nr = file[5:-3]\n",
    "                    model_list.append([file, int(nr)])\n",
    "\n",
    "            model_list.sort(key=itemgetter(1))\n",
    "\n",
    "            for model_file, iteration in model_list:\n",
    "                model = load_model(mode_of_progress_networks_dir + model_file)\n",
    "\n",
    "                title = 'Iteration {}, best {} weights, {} data points shown'.format(iteration, mode_of_progress, data_type)\n",
    "                fig_file_path = redshift_path + 'iteration_{}.png'.format(iteration)\n",
    "                get_smf_ssfr_fq_plot(model, training_data_dict_model, galaxies=None, title=title, data_type=data_type, full_range=True, save=True, file_path=fig_file_path)\n",
    "\n",
    "        video_path = redshift_path + 'progress_video.avi'\n",
    "\n",
    "        images = [img for img in os.listdir(redshift_path) if img.endswith(\".png\")]\n",
    "        images_with_nrs = []\n",
    "        for image in images:\n",
    "            number = image.split('_')\n",
    "            number = number[-1][:-4]\n",
    "            images_with_nrs.append([image, int(number)])\n",
    "\n",
    "        images_with_nrs.sort(key=itemgetter(1))\n",
    "\n",
    "        images = [redshift_path + items[0] for items in images_with_nrs]\n",
    "\n",
    "        frame = cv2.imread(images[0])\n",
    "        height, width, layers = frame.shape\n",
    "\n",
    "        video = cv2.VideoWriter(video_path, fourcc, frame_rate, (width,height))\n",
    "\n",
    "        for image in images:\n",
    "            img = cv2.imread(image)\n",
    "            video.write(img)\n",
    "\n",
    "        video.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between new high scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00_tanh_Halo_mass-Halo_mass_peak-Scale_peak_mass-Scale_half_mass-Halo_growth_rate_to_Stellar_mass-SFR_test_score5.48e-04'\n",
    "\n",
    "model_dir = trained_networks_path + 'backprop_and_pso_trained/mock_observations/{}/'.format(model_name)\n",
    "os.makedirs(os.path.dirname(model_dir + 'swarm_best_distance_moved.p'), exist_ok=True)\n",
    "distance_dict = pickle.load(open(model_dir + 'swarm_best_distance_moved.p', 'rb'))\n",
    "\n",
    "print(distance_dict.keys())\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.plot(distance_dict['swarm_best_distance_moved_p_point_one'])\n",
    "plt.plot(distance_dict['swarm_best_distance_moved_p_one'])\n",
    "plt.plot(distance_dict['swarm_best_distance_moved_p_two'])\n",
    "plt.plot(distance_dict['swarm_best_distance_moved_p_inf'])\n",
    "plt.legend(['p = 0.1', 'p = 1', 'p = 2', 'p = inf'], loc='upper right', fontsize='xx-large')\n",
    "# plt.plot([dist[1][0] for dist in distance_dict['swarm_best_distance_moved_p_point_one']])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig.savefig(fig_dir + '4x4_relu_xi10_loss_point9_cutoff_no_empty_bin_punish.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(distance_dict['swarm_best_distance_moved_p_one'][10:])/np.array(distance_dict['swarm_best_distance_moved_p_point_one'][10:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interparticle distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00-01-02-05-10-20-30-40-60-80_tanh_Halo_mass-Halo_mass_peak-Scale_peak_mass-Scale_half_mass-Halo_growth_rate-Redshift_to_Stellar_mass-SFR_test_score5.91e-06'\n",
    "\n",
    "score_hist_path = trained_networks_path + 'backprop_and_pso_trained/mock_observations/{}/score_history.p'.format(model_name)\n",
    "score_hist_dict = pickle.load(open(score_hist_path, 'rb'))\n",
    "\n",
    "print(score_hist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00-01-02-05-10-20-30-40-60-80_tanh_Halo_mass-Halo_mass_peak-Scale_peak_mass-Scale_half_mass-Halo_growth_rate-Redshift_to_Stellar_mass-SFR_test_score5.91e-06'\n",
    "training_material = 'mock_observations' # 'mock_observations', 'real_observations'\n",
    "training = 'backprop_and_pso_trained' # 'pso_trained', 'backprop_trained', 'backprop_and_pso_trained'\n",
    "figure_scale = 'linear' # 'linear', 'log', 'symlog', 'logit'\n",
    "iterations_skipped = 1\n",
    "distance_color = 'tab:blue'\n",
    "score_improvements_color = 'tab:red'\n",
    "part_dist_sym = 'o-'\n",
    "score_dist_sym = 's'\n",
    "train_score_sym = '+'\n",
    "val_score_sym = '^'\n",
    "\n",
    "score_distance_path = trained_networks_path + '{}/{}/{}/swarm_best_distance_moved.p'.format(training, training_material, model_name)\n",
    "score_distance_dict = pickle.load(open(score_distance_path, 'rb'))\n",
    "dist_moved_p_point_one = np.log10(score_distance_dict['swarm_best_distance_moved_p_point_one'][1:])\n",
    "dist_moved_p_one = np.log10(score_distance_dict['swarm_best_distance_moved_p_one'][1:])\n",
    "dist_moved_p_two = np.log10(score_distance_dict['swarm_best_distance_moved_p_two'][1:])\n",
    "\n",
    "score_hist_path = trained_networks_path + '{}/{}/{}/score_history.p'.format(training, training_material, model_name)\n",
    "score_hist_dict = pickle.load(open(score_hist_path, 'rb'))\n",
    "training_score_improvements = []\n",
    "validation_score_improvements = []\n",
    "for i in range(len(score_hist_dict['train_score_history']) - 1):\n",
    "    training_score_improvements.append(score_hist_dict['train_score_history'][i] - score_hist_dict['train_score_history'][i+1])\n",
    "for i in range(len(score_hist_dict['val_score_history']) - 1):\n",
    "    validation_score_improvements.append(score_hist_dict['val_score_history'][i] - score_hist_dict['val_score_history'][i+1])\n",
    "    \n",
    "training_score_improvements = np.log10(training_score_improvements)\n",
    "validation_score_improvements = np.log10(validation_score_improvements)\n",
    "\n",
    "interparticle_dict_dir = trained_networks_path + '{}/{}/{}/interparticle_distances/'.format(training, training_material, model_name)\n",
    "\n",
    "dict_list = []\n",
    "for file in os.listdir(interparticle_dict_dir):\n",
    "    nr = file[5:-2]\n",
    "    dict_list.append([file, int(nr)])\n",
    "\n",
    "dict_list.sort(key=itemgetter(1))\n",
    "\n",
    "p_point_one_list = []\n",
    "p_one_list = []\n",
    "p_two_list = []\n",
    "iteration_list = []\n",
    "\n",
    "for dict_file, iteration in dict_list[iterations_skipped:]:\n",
    "    interparticle_distance_dict = pickle.load(open(interparticle_dict_dir + dict_file, 'rb'))\n",
    "    \n",
    "    p_point_one = []\n",
    "    p_one = []\n",
    "    p_two = []\n",
    "    \n",
    "    for i in range(np.shape(interparticle_distance_dict['p_point_one'])[0]):\n",
    "        for j in range(i+1, np.shape(interparticle_distance_dict['p_point_one'])[1]):\n",
    "            p_point_one.append(interparticle_distance_dict['p_point_one'][i,j])\n",
    "    for i in range(np.shape(interparticle_distance_dict['p_one'])[0]):\n",
    "        for j in range(i+1, np.shape(interparticle_distance_dict['p_one'])[1]):\n",
    "            p_one.append(interparticle_distance_dict['p_one'][i,j])\n",
    "    for i in range(np.shape(interparticle_distance_dict['p_two'])[0]):\n",
    "        for j in range(i+1, np.shape(interparticle_distance_dict['p_two'])[1]):\n",
    "            p_two.append(interparticle_distance_dict['p_two'][i,j])\n",
    "            \n",
    "    p_point_one_list.append(np.log10(p_point_one))\n",
    "    p_one_list.append(np.log10(p_one))\n",
    "    p_two_list.append(np.log10(p_two))\n",
    "    \n",
    "    iteration_list.append(iteration)\n",
    "    \n",
    "mean_dist_p_point_one = []\n",
    "std_p_point_one = []\n",
    "mean_dist_p_one = []\n",
    "std_p_one = []\n",
    "mean_dist_p_two = []\n",
    "std_p_two = []\n",
    "\n",
    "for i_iteration, iteration in enumerate(iteration_list):\n",
    "    \n",
    "    mean_dist_p_point_one.append(np.mean(p_point_one_list[i_iteration]))\n",
    "    mean_dist_p_one.append(np.mean(p_one_list[i_iteration]))\n",
    "    mean_dist_p_two.append(np.mean(p_two_list[i_iteration]))\n",
    "    \n",
    "    if figure_scale == 'log':\n",
    "        std_p_point_one.append(np.std(np.log(p_point_one_list[i_iteration])))\n",
    "        std_p_one.append(np.std(np.log(p_one_list[i_iteration])))\n",
    "        std_p_two.append(np.std(np.log(p_two_list[i_iteration])))\n",
    "    else:\n",
    "        std_p_point_one.append(np.std(p_point_one_list[i_iteration]))\n",
    "        std_p_one.append(np.std(p_one_list[i_iteration]))\n",
    "        std_p_two.append(np.std(p_two_list[i_iteration]))\n",
    "        \n",
    "interparticle_plot_pairs = [[mean_dist_p_point_one, std_p_point_one], [mean_dist_p_one, std_p_one], [mean_dist_p_two, std_p_two]]\n",
    "score_dist_plots = [dist_moved_p_point_one, dist_moved_p_one, dist_moved_p_two]\n",
    "titles = ['Interparticle minkowski distances x for p = 0.1', 'Interparticle minkowski distances x for p = 1', 'Interparticle minkowski distances x for p = 2']\n",
    "legend_entries = ['interparticle_distances', 'score $\\Delta x$', 'training loss improvements', 'validation loss improvements']\n",
    "x_label = 'iteration'\n",
    "y_label_interparticle_dists = '$log_{10}(x)\\pm\\sigma_{log_{10}(x)}$'\n",
    "y_label_scores = '$log_{10}(\\Delta_{score})$'\n",
    "    \n",
    "fig_dist = plt.figure(figsize=(20,10))\n",
    "\n",
    "for i, (mean_dist, stds) in enumerate(interparticle_plot_pairs):\n",
    "    ax1 = plt.subplot(2,2,i+1)\n",
    "    interpart_dist_line = ax1.errorbar(iteration_list, mean_dist, yerr=stds, fmt=part_dist_sym, markersize=5, color=distance_color)\n",
    "    score_dist_line, = ax1.plot(score_hist_dict['iterations_train_best'][1:], score_dist_plots[i], score_dist_sym, color=distance_color)\n",
    "#     ax1.set_yscale(figure_scale)\n",
    "    ax1.tick_params(axis='y', labelcolor=distance_color, labelsize=15)\n",
    "    ax1.set_ylabel(y_label_interparticle_dists, color=distance_color, fontsize=15)\n",
    "    ax1.set_xlabel(x_label, fontsize=15)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    train_imp_line, = ax2.plot(score_hist_dict['iterations_train_best'][1:], training_score_improvements, train_score_sym, color=score_improvements_color)\n",
    "    val_imp_line, = ax2.plot(score_hist_dict['iterations_val_best'][1:], validation_score_improvements, val_score_sym, color=score_improvements_color)\n",
    "    ax2.tick_params(axis='y', labelcolor=score_improvements_color, labelsize=15)\n",
    "    ax2.set_ylabel(y_label_scores, color=score_improvements_color, fontsize=15)\n",
    "    \n",
    "    plt.legend([interpart_dist_line, score_dist_line, train_imp_line, val_imp_line], legend_entries)\n",
    "    \n",
    "    plt.title(titles[i], fontsize=20)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dist.savefig(trained_networks_path + '{}/{}/{}/distance_fig.png'.format(training, training_material, model_name), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,6,7]\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(model_dir + 'interparticle_distances/iter_{}.p'), exist_ok=True)\n",
    "distance_dict = pickle.load(open(model_dir + 'interparticle_distances.p', 'rb'))\n",
    "\n",
    "\n",
    "print(distance_dict.keys())\n",
    "print(distance_dict['p_point_one'])\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.plot(distance_dict['p_point_one'], 'bo')\n",
    "plt.plot(distance_dict['p_one'], 'rx')\n",
    "plt.plot(distance_dict['p_two'], 'g.')\n",
    "plt.legend(['p = 0.1', 'p = 1', 'p = 2'], loc='upper right', fontsize='xx-large')\n",
    "# plt.plot([dist[1][0] for dist in distance_dict['swarm_best_distance_moved_p_point_one']])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2],[3,4],[10,2]]\n",
    "b = minkowski_distance(a, p=20)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(0, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non parallel PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "network.pso_swarm.set_best_weights('train')\n",
    "norm_score = network.pso_swarm.evaluate_model(mode)\n",
    "tot_score = norm_score\n",
    "model = network.model\n",
    "title = 'Inputs: %s\\ntest mse %.3e, %s data' % (', '.join(input_features), tot_score, mode)\n",
    "\n",
    "fig1 = get_pred_vs_real_scatterplot(model, training_data_dict, unit_dict, data_keys, 'SFR', title=title, mode=mode)\n",
    "fig2 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, predicted_feat = 'Stellar_mass', \n",
    "                                binning_feat = 'Halo_mass', title=title, mode=mode)\n",
    "fig3 = get_scatter_comparison_plots(model, training_data_dict, unit_dict, x_axis_feature = 'Halo_mass', \n",
    "                                    y_axis_feature = 'Stellar_mass', title=title, y_max = None, y_min = None,\n",
    "                                    x_min = None, x_max = None, mode=mode)\n",
    "fig4 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, 'SFR', \n",
    "                                binning_feat = 'Stellar_mass', title=title, mode=mode)\n",
    "fig5 = get_scatter_comparison_plots(model, training_data_dict, unit_dict, x_axis_feature = 'Halo_mass', \n",
    "                                    y_axis_feature = 'SFR', title=title, y_max = 10, y_min = None,\n",
    "                                    x_min = None, x_max = None, mode=mode)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "total_set_size = 2.9e4 # how many examples will be used for training+validation+testing\n",
    "train_size = 2e4\n",
    "val_size = .5e4\n",
    "test_size = .4e4\n",
    "input_features = ['Halo_mass', 'Halo_mass_peak', 'Scale_peak_mass', 'Scale_half_mass', 'Halo_growth_rate']#, 'Redshift']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "redshifts = [0]#,.1,.2,.5,1,2,3,4,6,8]\n",
    "same_n_points_per_redshift = False # if using the smf in the objective function, must be false!\n",
    "outputs_to_weigh = ['Stellar_mass']\n",
    "weigh_by_redshift = True\n",
    "\n",
    "reinforcement_learning = True\n",
    "real_observations = False\n",
    "\n",
    "verbatim = True\n",
    "\n",
    "network_name = 'testing2'\n",
    "# network_name = '{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "save_all_nets = True\n",
    "\n",
    "### Network parameters\n",
    "nr_hidden_layers = 4\n",
    "activation_function = 'tanh'\n",
    "output_activation = {'SFR': None, 'Stellar_mass': None}\n",
    "nr_neurons_per_layer = 4\n",
    "regularisation_strength = 1e-2\n",
    "std_penalty = False\n",
    "norm = {'input': 'zero_mean_unit_std',\n",
    "        'output': 'zero_mean_unit_std'} # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "\n",
    "### PSO parameters\n",
    "nr_processes = 30\n",
    "nr_iterations = 1000\n",
    "min_std_tol = 0.01                # minimum allowed std for any parameter\n",
    "pso_param_dict = {\n",
    "    'nr_particles': 3 * nr_processes,\n",
    "    'inertia_weight_start': 1.4,\n",
    "    'inertia_weight_min': 0.3,\n",
    "    'exploration_iters': 600,\n",
    "    'patience': 10000,\n",
    "    'patience_parameter': 'train',\n",
    "    'restart_check_interval': 200\n",
    "}\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the selected galaxyfile\n",
    "galaxies, data_keys, unit_dict = load_galfiles(redshifts=redshifts, equal_numbers=same_n_points_per_redshift)\n",
    "    \n",
    "# prepare the training data\n",
    "training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, redshifts, weigh_by_redshift, outputs_to_weigh,\n",
    "                                       int(total_set_size), train_size=int(train_size), val_size=int(val_size), test_size=int(test_size), pso=True)\n",
    "training_data_dict = normalise_data(training_data_dict, norm, pso=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network = Feed_Forward_Neural_Network(nr_hidden_layers, nr_neurons_per_layer, input_features, output_features, \n",
    "                                      activation_function, output_activation, regularisation_strength, network_name)\n",
    "network.setup_pso(pso_param_dict, reinf_learning=reinforcement_learning, real_observations=real_observations, nr_processes=nr_processes)\n",
    "network.train_pso(nr_iterations, training_data_dict, std_penalty=std_penalty, verbatim=verbatim, save_all_networks=save_all_nets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
