{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "module_path = home_dir + '/code/modules/'\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "fig_dir = 'figures/'\n",
    "import time\n",
    "import importlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LeakyReLU, concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "#import model_management\n",
    "from scipy.special import comb\n",
    "import datetime\n",
    "import codecs, json\n",
    "from itertools import combinations\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport environmental_density\n",
    "%aimport data_processing_testing\n",
    "%aimport plotting\n",
    "%aimport keras_objects\n",
    "from environmental_density import *\n",
    "from data_processing_testing import *\n",
    "from plotting import *\n",
    "from keras_objects import *\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)\n",
    "\n",
    "#loss_func_dict = {\n",
    "#    'mse': 'mse',\n",
    "#    'mae': 'mae',\n",
    "#    'weighted_mse_1': weighted_mse_1,\n",
    "#    'stellar_mass_weighted_mse': stellar_mass_weighted_mse\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_on_cpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameter string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set name ending with parameters for figures to be saved\n",
    "param_string = 'nLayers_%d_nNeurons_%d_actFun_%s_lossFunc_%s_nTrainSamples_%d_nEpochs_%d_batchSize_%d' % (\n",
    "    nLayers, neuronsPerLayer, activationFunction, loss_function, train_size, nEpochs, batchSize)\n",
    "print(param_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y_test_norm, 0))\n",
    "print(np.std(y_test_norm, 0))\n",
    "print(np.min(x_test_norm, 0))\n",
    "print(np.max(x_test_norm, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a feel for the data\n",
    "for i in range(len(input_features)):\n",
    "    print(input_features[i],': min: %.2e, max: %.2e.' % (np.min(x_train[:,i]), np.max(x_train[:,i])))\n",
    "for i in range(len(output_features)):\n",
    "    print(output_features[i],': min: %.2e, max: %.2e.' % (np.min(y_train[:,i]), np.max(y_train[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualisation for when we have 2 input features\n",
    "%matplotlib notebook\n",
    "input_feat_1 = 0\n",
    "input_feat_2 = 1\n",
    "output_feat = 1\n",
    "\n",
    "fig = plt.figure(1, figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train_norm[:500,input_feat_1], x_train_norm[:500,input_feat_2], \n",
    "           y_train_norm[:500,output_feat])\n",
    "ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_1]))\n",
    "ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_2]))\n",
    "ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[output_feat]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search for the model that you want\n",
    "importlib.reload(model_management)\n",
    "search_dict = {\n",
    "    'training_method': 'backprop'\n",
    "}\n",
    "[model_dicts, description_dicts] = model_management.SearchModel(search_dict, get_hits=True)\n",
    "print(description_dicts)\n",
    "print('\\n')\n",
    "for key in model_dicts:\n",
    "    print(key)\n",
    "    print(model_dicts[key])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model_management)\n",
    "model, model_dict, description = model_management.LoadModel(search_dict, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the standard pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test' # 'train', 'val, 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_scores = model.evaluate(x=training_data_dict['input_test_dict'], y=training_data_dict['output_test_dict'],\n",
    "                                               verbose=1)\n",
    "tot_score = norm_scores[0]\n",
    "title = 'Inputs: %s\\ntest mse %.3e, %s data' % (', '.join(input_features), tot_score, mode)\n",
    "fig1 = get_pred_vs_real_scatterplot(model, training_data_dict, unit_dict, data_keys, 'SFR', title=title, data_type=mode)\n",
    "fig2 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, predicted_feat = 'Stellar_mass', \n",
    "                                binning_feat = 'Halo_mass', title=title, data_type=mode)\n",
    "fig3 = get_scatter_comparison_plots(model, training_data_dict, unit_dict, x_axis_feature = 'Halo_mass', \n",
    "                                    y_axis_feature = 'Stellar_mass', title=title, y_max = None, y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type=mode)\n",
    "fig4 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, 'SFR', \n",
    "                                binning_feat = 'Stellar_mass', title=title, data_type=mode)\n",
    "fig5 = get_scatter_comparison_plots(model, training_data_dict, unit_dict, x_axis_feature = 'Halo_mass', \n",
    "                                    y_axis_feature = 'SFR', title=title, y_max = 10, y_min = None,\n",
    "                                    x_min = 5, x_max = None, data_type=mode)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig6 = get_real_vs_pred_same_fig(model, training_data_dict, unit_dict, x_axis_feature='Halo_mass', \n",
    "                                 y_axis_feature = 'Stellar_mass', title=title, data_type=mode, marker_size=5)\n",
    "fig7 = get_real_vs_pred_same_fig(model, training_data_dict, unit_dict, x_axis_feature='Halo_mass', \n",
    "                                 y_axis_feature = 'SFR', title=title, data_type=mode, marker_size=5)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the figures\n",
    "fig1.savefig(fig_dir + '_'.join(input_features) + '_true_pred_sfr_scatter.png', bbox_inches = 'tight')\n",
    "fig2.savefig(fig_dir + '_'.join(input_features) + '_boxplot_stellar_mass.png', bbox_inches = 'tight')\n",
    "fig3.savefig(fig_dir + '_'.join(input_features) + '_scatter_comp_halo_vs_stellar_mass.png', bbox_inches = 'tight')\n",
    "fig4.savefig(fig_dir + '_'.join(input_features) + '_boxplot_sfr.png', bbox_inches = 'tight')\n",
    "fig5.savefig(fig_dir + '_'.join(input_features) + '_scatter_comp_stellar_mass_vs_sfr.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "nr_steps = 1e5\n",
    "batch_size = 1e4\n",
    "total_set_size = 3e4 # how many examples will be used for training+validation+testing\n",
    "train_size = 1e4\n",
    "val_size = 1e4\n",
    "test_size = 1e4\n",
    "norm = 'none' # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "input_features = ['Halo_mass', 'Halo_mass_peak', 'Scale_half_mass', 'Scale_peak_mass', 'Scale_last_MajM']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "weighted_output_features = ['Stellar_mass', 'SFR']\n",
    "\n",
    "early_stop_patience = 2000\n",
    "early_stop_monitor = 'val_loss'\n",
    "early_stop_min_delta = 1e-16\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "\n",
    "validation_data = 'val' #'val' is normally used, use 'train' to check overfitting potential\n",
    "\n",
    "### Network parameters\n",
    "nLayers = 8\n",
    "activationFunction = 'tanh'\n",
    "neuronsPerLayer = 10\n",
    "reg_strength = 0#1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the selected galaxyfile\n",
    "galaxies, data_keys, unit_dict = load_galfile()\n",
    "    \n",
    "# prepare the training data\n",
    "training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, \n",
    "                                       int(total_set_size), int(train_size), int(val_size), int(test_size))\n",
    "#galaxies = None\n",
    "training_data_dict = normalise_data(training_data_dict, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "halo_mass_input = Input(shape=(1,), name = 'halo_mass_input')\n",
    "others_input = Input(shape=(len(input_features)-1,), name = 'others_input')\n",
    "\n",
    "for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "    if i == 0:\n",
    "        x = concatenate([halo_mass_input, others_input])\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(x)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "    else:\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(x)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "        \n",
    "output_layers = []\n",
    "for feat in output_features:\n",
    "#    out_pre_act = Dense(1)(x)\n",
    "#    output_layers.append(LeakyReLU(alpha = 0, name = feat)(out_pre_act))\n",
    "    output_layers.append(Dense(1, kernel_regularizer=regularizers.l2(reg_strength), name = feat)(x))\n",
    "\n",
    "model = Model([halo_mass_input, others_input], output_layers)\n",
    "model.compile(optimizer = 'adam', loss = halo_mass_weighted_loss_wrapper(halo_mass_input))\n",
    "\n",
    "earlystop = EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, patience=early_stop_patience, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "history = model.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                    validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n",
    "                    training_data_dict['output_'+validation_data+'_dict']), \n",
    "                    epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-259c2fb96619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0msfr_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_strength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0msfr_weigh_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeighted_loss_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhalo_mass_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfr_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfr_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mstellar_mass_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_strength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/modules/keras_objects.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWeighted_loss_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "# Create model TESTING\n",
    "inputs = []\n",
    "\n",
    "main_input = Input(shape=(len(input_features),), name = 'main_input')\n",
    "halo_mass_input = Input(shape=(1,), name = 'Halo_mass')\n",
    "sfr_input = Input(shape=(1,), name = 'SFR')\n",
    "stellar_mass_input = Input(shape=(1,), name = 'Stellar_mass')\n",
    "inputs.append(main_input)\n",
    "inputs.append(halo_mass_input)\n",
    "inputs.append(sfr_input)\n",
    "inputs.append(stellar_mass_input)\n",
    "#for i_feat, feat in enumerate(weighted_output_features):\n",
    "#    inputs.append(Input(shape=(1,), name = feat))\n",
    "    \n",
    "for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "    if i == 0:\n",
    "        #x = concatenate([halo_mass_input, others_input])\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(main_input)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(main_input)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "    else:\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(x)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "        \n",
    "output_layers = []\n",
    "\n",
    "sfr_output = Dense(1, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "sfr_weigh_loss = Weighted_loss_layer()([halo_mass_input, sfr_input, sfr_output])\n",
    "\n",
    "stellar_mass_output = Dense(1, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "stellar_mass_weigh_loss = Weighted_loss_layer()([halo_mass_input, stellar_mass_input, stellar_mass_output])\n",
    "\n",
    "output_layers.append(sfr_weigh_loss)\n",
    "output_layers.append(stellar_mass_weigh_loss)\n",
    "\n",
    "#out = Weighted_loss_layer()([halo_mass_input, ])\n",
    "    \n",
    "\n",
    "#model_core = Model(main_input, output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/100000\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 69.1562 - Stellar_mass_loss: 68.6034 - SFR_loss: 0.5527 - val_loss: 69.1057 - val_Stellar_mass_loss: 68.4559 - val_SFR_loss: 0.6498\n",
      "Epoch 2/100000\n",
      "10000/10000 [==============================] - 0s 2us/step - loss: 68.7254 - Stellar_mass_loss: 68.1562 - SFR_loss: 0.5692 - val_loss: 68.6053 - val_Stellar_mass_loss: 67.9344 - val_SFR_loss: 0.6710\n",
      "Epoch 3/100000\n",
      "10000/10000 [==============================] - 0s 2us/step - loss: 68.2259 - Stellar_mass_loss: 67.6364 - SFR_loss: 0.5895 - val_loss: 68.0836 - val_Stellar_mass_loss: 67.3894 - val_SFR_loss: 0.6942\n",
      "Epoch 4/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 67.7048 - Stellar_mass_loss: 67.0931 - SFR_loss: 0.6117 - val_loss: 67.5204 - val_Stellar_mass_loss: 66.8001 - val_SFR_loss: 0.7203\n",
      "Epoch 5/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 67.1424 - Stellar_mass_loss: 66.5056 - SFR_loss: 0.6368 - val_loss: 66.9123 - val_Stellar_mass_loss: 66.1623 - val_SFR_loss: 0.7501\n",
      "Epoch 6/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 66.5351 - Stellar_mass_loss: 65.8695 - SFR_loss: 0.6655 - val_loss: 66.2655 - val_Stellar_mass_loss: 65.4819 - val_SFR_loss: 0.7836\n",
      "Epoch 7/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 65.8889 - Stellar_mass_loss: 65.1911 - SFR_loss: 0.6978 - val_loss: 65.5907 - val_Stellar_mass_loss: 64.7702 - val_SFR_loss: 0.8204\n",
      "Epoch 8/100000\n",
      "10000/10000 [==============================] - 0s 2us/step - loss: 65.2149 - Stellar_mass_loss: 64.4814 - SFR_loss: 0.7335 - val_loss: 64.9102 - val_Stellar_mass_loss: 64.0505 - val_SFR_loss: 0.8597\n",
      "Epoch 9/100000\n",
      "10000/10000 [==============================] - 0s 2us/step - loss: 64.5350 - Stellar_mass_loss: 63.7635 - SFR_loss: 0.7715 - val_loss: 64.1986 - val_Stellar_mass_loss: 63.2954 - val_SFR_loss: 0.9032\n",
      "Epoch 10/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 63.8244 - Stellar_mass_loss: 63.0107 - SFR_loss: 0.8137 - val_loss: 63.4479 - val_Stellar_mass_loss: 62.4961 - val_SFR_loss: 0.9518\n",
      "Epoch 11/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 63.0747 - Stellar_mass_loss: 62.2137 - SFR_loss: 0.8609 - val_loss: 62.6596 - val_Stellar_mass_loss: 61.6537 - val_SFR_loss: 1.0059\n",
      "Epoch 12/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 62.2874 - Stellar_mass_loss: 61.3739 - SFR_loss: 0.9135 - val_loss: 61.8353 - val_Stellar_mass_loss: 60.7694 - val_SFR_loss: 1.0659\n",
      "Epoch 13/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 61.4643 - Stellar_mass_loss: 60.4923 - SFR_loss: 0.9719 - val_loss: 60.9798 - val_Stellar_mass_loss: 59.8478 - val_SFR_loss: 1.1320\n",
      "Epoch 14/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 60.6099 - Stellar_mass_loss: 59.5735 - SFR_loss: 1.0364 - val_loss: 60.0996 - val_Stellar_mass_loss: 58.8953 - val_SFR_loss: 1.2043\n",
      "Epoch 15/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 59.7310 - Stellar_mass_loss: 58.6241 - SFR_loss: 1.1069 - val_loss: 59.1839 - val_Stellar_mass_loss: 57.8997 - val_SFR_loss: 1.2842\n",
      "Epoch 16/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 58.8165 - Stellar_mass_loss: 57.6315 - SFR_loss: 1.1850 - val_loss: 58.2249 - val_Stellar_mass_loss: 56.8517 - val_SFR_loss: 1.3732\n",
      "Epoch 17/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 57.8588 - Stellar_mass_loss: 56.5868 - SFR_loss: 1.2720 - val_loss: 57.2311 - val_Stellar_mass_loss: 55.7597 - val_SFR_loss: 1.4714\n",
      "Epoch 18/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 56.8662 - Stellar_mass_loss: 55.4981 - SFR_loss: 1.3681 - val_loss: 56.2067 - val_Stellar_mass_loss: 54.6274 - val_SFR_loss: 1.5793\n",
      "Epoch 19/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 55.8431 - Stellar_mass_loss: 54.3692 - SFR_loss: 1.4739 - val_loss: 55.1461 - val_Stellar_mass_loss: 53.4476 - val_SFR_loss: 1.6986\n",
      "Epoch 20/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 54.7838 - Stellar_mass_loss: 53.1930 - SFR_loss: 1.5908 - val_loss: 54.0450 - val_Stellar_mass_loss: 52.2142 - val_SFR_loss: 1.8309\n",
      "Epoch 21/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 53.6842 - Stellar_mass_loss: 51.9635 - SFR_loss: 1.7207 - val_loss: 52.9034 - val_Stellar_mass_loss: 50.9258 - val_SFR_loss: 1.9776\n",
      "Epoch 22/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 52.5440 - Stellar_mass_loss: 50.6791 - SFR_loss: 1.8649 - val_loss: 51.7221 - val_Stellar_mass_loss: 49.5816 - val_SFR_loss: 2.1405\n",
      "Epoch 23/100000\n",
      "10000/10000 [==============================] - 0s 4us/step - loss: 51.3642 - Stellar_mass_loss: 49.3392 - SFR_loss: 2.0250 - val_loss: 50.5029 - val_Stellar_mass_loss: 48.1820 - val_SFR_loss: 2.3209\n",
      "Epoch 24/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 50.1466 - Stellar_mass_loss: 47.9441 - SFR_loss: 2.2025 - val_loss: 49.2505 - val_Stellar_mass_loss: 46.7301 - val_SFR_loss: 2.5203\n",
      "Epoch 25/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 48.8958 - Stellar_mass_loss: 46.4969 - SFR_loss: 2.3989 - val_loss: 47.9665 - val_Stellar_mass_loss: 45.2259 - val_SFR_loss: 2.7407\n",
      "Epoch 26/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 47.6136 - Stellar_mass_loss: 44.9976 - SFR_loss: 2.6160 - val_loss: 46.6488 - val_Stellar_mass_loss: 43.6639 - val_SFR_loss: 2.9849\n",
      "Epoch 27/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 46.2976 - Stellar_mass_loss: 43.4408 - SFR_loss: 2.8568 - val_loss: 45.3015 - val_Stellar_mass_loss: 42.0462 - val_SFR_loss: 3.2553\n",
      "Epoch 28/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 44.9520 - Stellar_mass_loss: 41.8283 - SFR_loss: 3.1237 - val_loss: 43.9325 - val_Stellar_mass_loss: 40.3789 - val_SFR_loss: 3.5537\n",
      "Epoch 29/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 43.5847 - Stellar_mass_loss: 40.1666 - SFR_loss: 3.4182 - val_loss: 42.5459 - val_Stellar_mass_loss: 38.6632 - val_SFR_loss: 3.8827\n",
      "Epoch 30/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 42.1999 - Stellar_mass_loss: 38.4567 - SFR_loss: 3.7432 - val_loss: 41.1447 - val_Stellar_mass_loss: 36.8989 - val_SFR_loss: 4.2458\n",
      "Epoch 31/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 40.8005 - Stellar_mass_loss: 36.6985 - SFR_loss: 4.1020 - val_loss: 39.7366 - val_Stellar_mass_loss: 35.0908 - val_SFR_loss: 4.6459\n",
      "Epoch 32/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 39.3944 - Stellar_mass_loss: 34.8968 - SFR_loss: 4.4976 - val_loss: 38.3294 - val_Stellar_mass_loss: 33.2435 - val_SFR_loss: 5.0859\n",
      "Epoch 33/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 37.9892 - Stellar_mass_loss: 33.0563 - SFR_loss: 4.9329 - val_loss: 36.9307 - val_Stellar_mass_loss: 31.3610 - val_SFR_loss: 5.5697\n",
      "Epoch 34/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 36.5923 - Stellar_mass_loss: 31.1806 - SFR_loss: 5.4117 - val_loss: 35.5515 - val_Stellar_mass_loss: 29.4515 - val_SFR_loss: 6.1000\n",
      "Epoch 35/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 35.2148 - Stellar_mass_loss: 29.2780 - SFR_loss: 5.9368 - val_loss: 34.2034 - val_Stellar_mass_loss: 27.5236 - val_SFR_loss: 6.6798\n",
      "Epoch 36/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 33.8686 - Stellar_mass_loss: 27.3574 - SFR_loss: 6.5111 - val_loss: 32.8974 - val_Stellar_mass_loss: 25.5850 - val_SFR_loss: 7.3123\n",
      "Epoch 37/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 32.5644 - Stellar_mass_loss: 25.4264 - SFR_loss: 7.1379 - val_loss: 31.6469 - val_Stellar_mass_loss: 23.6468 - val_SFR_loss: 8.0001\n",
      "Epoch 38/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 31.3156 - Stellar_mass_loss: 23.4960 - SFR_loss: 7.8197 - val_loss: 30.4667 - val_Stellar_mass_loss: 21.7222 - val_SFR_loss: 8.7445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 30.1371 - Stellar_mass_loss: 21.5793 - SFR_loss: 8.5578 - val_loss: 29.3711 - val_Stellar_mass_loss: 19.8248 - val_SFR_loss: 9.5464\n",
      "Epoch 40/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 29.0431 - Stellar_mass_loss: 19.6899 - SFR_loss: 9.3531 - val_loss: 28.3742 - val_Stellar_mass_loss: 17.9689 - val_SFR_loss: 10.4053\n",
      "Epoch 41/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 28.0476 - Stellar_mass_loss: 17.8422 - SFR_loss: 10.2054 - val_loss: 27.4900 - val_Stellar_mass_loss: 16.1710 - val_SFR_loss: 11.3189\n",
      "Epoch 42/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 27.1646 - Stellar_mass_loss: 16.0525 - SFR_loss: 11.1122 - val_loss: 26.7306 - val_Stellar_mass_loss: 14.4486 - val_SFR_loss: 12.2820\n",
      "Epoch 43/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 26.4064 - Stellar_mass_loss: 14.3380 - SFR_loss: 12.0683 - val_loss: 26.1054 - val_Stellar_mass_loss: 12.8188 - val_SFR_loss: 13.2865\n",
      "Epoch 44/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 25.7822 - Stellar_mass_loss: 12.7163 - SFR_loss: 13.0659 - val_loss: 25.6197 - val_Stellar_mass_loss: 11.2987 - val_SFR_loss: 14.3209\n",
      "Epoch 45/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 25.2973 - Stellar_mass_loss: 11.2040 - SFR_loss: 14.0932 - val_loss: 25.2733 - val_Stellar_mass_loss: 9.9041 - val_SFR_loss: 15.3692\n",
      "Epoch 46/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.9516 - Stellar_mass_loss: 9.8170 - SFR_loss: 15.1346 - val_loss: 25.0596 - val_Stellar_mass_loss: 8.6488 - val_SFR_loss: 16.4108\n",
      "Epoch 47/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.7384 - Stellar_mass_loss: 8.5689 - SFR_loss: 16.1695 - val_loss: 24.9638 - val_Stellar_mass_loss: 7.5434 - val_SFR_loss: 17.4204\n",
      "Epoch 48/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.6430 - Stellar_mass_loss: 7.4703 - SFR_loss: 17.1728 - val_loss: 24.9638 - val_Stellar_mass_loss: 6.5945 - val_SFR_loss: 18.3693\n",
      "Epoch 49/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.6433 - Stellar_mass_loss: 6.5275 - SFR_loss: 18.1158 - val_loss: 25.0304 - val_Stellar_mass_loss: 5.8039 - val_SFR_loss: 19.2265\n",
      "Epoch 50/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.7101 - Stellar_mass_loss: 5.7422 - SFR_loss: 18.9679 - val_loss: 25.1300 - val_Stellar_mass_loss: 5.1687 - val_SFR_loss: 19.9613\n",
      "Epoch 51/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.8099 - Stellar_mass_loss: 5.1117 - SFR_loss: 19.6982 - val_loss: 25.2280 - val_Stellar_mass_loss: 4.6836 - val_SFR_loss: 20.5444\n",
      "Epoch 52/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.9080 - Stellar_mass_loss: 4.6301 - SFR_loss: 20.2779 - val_loss: 25.2925 - val_Stellar_mass_loss: 4.3412 - val_SFR_loss: 20.9512\n",
      "Epoch 53/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.9727 - Stellar_mass_loss: 4.2905 - SFR_loss: 20.6822 - val_loss: 25.3012 - val_Stellar_mass_loss: 4.1302 - val_SFR_loss: 21.1710\n",
      "Epoch 54/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.9819 - Stellar_mass_loss: 4.0812 - SFR_loss: 20.9006 - val_loss: 25.2423 - val_Stellar_mass_loss: 4.0378 - val_SFR_loss: 21.2045\n",
      "Epoch 55/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.9235 - Stellar_mass_loss: 3.9895 - SFR_loss: 20.9339 - val_loss: 25.1177 - val_Stellar_mass_loss: 4.0520 - val_SFR_loss: 21.0657\n",
      "Epoch 56/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.7994 - Stellar_mass_loss: 4.0036 - SFR_loss: 20.7959 - val_loss: 24.9388 - val_Stellar_mass_loss: 4.1596 - val_SFR_loss: 20.7793\n",
      "Epoch 57/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.6213 - Stellar_mass_loss: 4.1103 - SFR_loss: 20.5111 - val_loss: 24.7226 - val_Stellar_mass_loss: 4.3493 - val_SFR_loss: 20.3733\n",
      "Epoch 58/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 24.4059 - Stellar_mass_loss: 4.2984 - SFR_loss: 20.1074 - val_loss: 24.4874 - val_Stellar_mass_loss: 4.6109 - val_SFR_loss: 19.8765\n",
      "Epoch 59/100000\n",
      "10000/10000 [==============================] - 0s 4us/step - loss: 24.1715 - Stellar_mass_loss: 4.5579 - SFR_loss: 19.6136 - val_loss: 24.2511 - val_Stellar_mass_loss: 4.9334 - val_SFR_loss: 19.3177\n",
      "Epoch 60/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 23.9360 - Stellar_mass_loss: 4.8780 - SFR_loss: 19.0581 - val_loss: 24.0285 - val_Stellar_mass_loss: 5.3049 - val_SFR_loss: 18.7236\n",
      "Epoch 61/100000\n",
      "10000/10000 [==============================] - 0s 3us/step - loss: 23.7143 - Stellar_mass_loss: 5.2467 - SFR_loss: 18.4676 - val_loss: 23.8309 - val_Stellar_mass_loss: 5.7101 - val_SFR_loss: 18.1208\n",
      "Epoch 62/100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5f2b77dc8627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                     validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n\u001b[1;32m     12\u001b[0m                     training_data_dict['output_'+validation_data+'_dict']), \n\u001b[0;32m---> 13\u001b[0;31m                     epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1248\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1250\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_core.compile(optimizer = 'adam', loss = tunnel_loss)\n",
    "\n",
    "earlystop = EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, patience=early_stop_patience, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "history = model_core.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                    validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n",
    "                    training_data_dict['output_'+validation_data+'_dict']), \n",
    "                    epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[5].get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "#plt.title(title)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a batch run to see which input parameters gives the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_set_size = 300000\n",
    "train_size = 250000\n",
    "val_size = 25000\n",
    "test_size = 25000\n",
    "\n",
    "core_input_features = ['Halo_mass', 'Halo_mass_peak', 'Scale_peak_mass']\n",
    "tested_input_features = ['Concentration', 'Type', 'Scale_half_mass', \n",
    "                 'Scale_last_MajM', 'Environmental_density']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "nr_extra_params_list = [2]\n",
    "nr_runs_per_comb = 3\n",
    "\n",
    "nLayers = 10\n",
    "activationFunction = 'tanh'\n",
    "neuronsPerLayer = 30\n",
    "loss_function = 'mse' # 'mse', 'weighted_mse_1' 'mae'\n",
    "norm = 'zero_to_one' # 'none',   'zero_mean_unit',   'zero_to_one'\n",
    "\n",
    "nr_steps = 1e4\n",
    "batch_size = 4e4\n",
    "\n",
    "verb = 0 # prints progress to stdout\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "parameter_dictionary = {\n",
    "    'fixed_input_features': core_input_features,\n",
    "    'tested_input_features': tested_input_features,\n",
    "    'output_features': output_features,\n",
    "    'nr_extra_parameter_combinations': nr_extra_params_list,\n",
    "    'nr_steps': [nr_steps],\n",
    "    'batch_size': [batch_size],\n",
    "    'nr_epochs': [nr_epochs],\n",
    "    'nr_training_samples': [train_size],\n",
    "    'nr_validation_samples': [val_size],\n",
    "    'nr_test_samples': [test_size],\n",
    "    'data_normalization': norm,\n",
    "    'activation_function': activationFunction,\n",
    "    'neurons_per_layer': [neuronsPerLayer],\n",
    "    'nr_hidden_layers': [nLayers],\n",
    "    'output_activation_function': 'none',\n",
    "    'description': 'Each parameter setting is represented by one list containing three objects. The first one is ' + \\\n",
    "    'the input parameters. The second one is the mse test scores obtained for the different runs evaluated on the ' +\\\n",
    "    'normed units and the original units of the data set (original only if norm is \\'none\\'). The third one is the ' +\\\n",
    "    'loss histories for the different runs [training_loss, validation_loss] evaluated on the normalised data.'\n",
    "}\n",
    "results_list = [parameter_dictionary]\n",
    "nr_combs_total = 0\n",
    "for nr_extra_params in nr_extra_params_list:\n",
    "    nr_combs_total += comb(len(tested_input_features), nr_extra_params)\n",
    "comb_counter = 0 # to keep track of how many combinations I've gone through\n",
    "\n",
    "with open('model_comparisons/progress.txt', 'w+') as f:\n",
    "    \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on input parameters at ' + date_string_proper + '\\n\\n')\n",
    "    f.flush()\n",
    "    \n",
    "    # load the selected galaxyfile\n",
    "    galaxies, data_keys, unit_dict = load_galfile()\n",
    "    \n",
    "    for i_nr_extra_params, nr_extra_params in enumerate(nr_extra_params_list):\n",
    "        \n",
    "        extra_param_combs = list(combinations(tested_input_features, nr_extra_params))\n",
    "        \n",
    "        date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "        f.write(date_string_proper + '    Testing %d extra parameters. %d/%d extra parameter count tested. \\n\\n' %\n",
    "                (nr_extra_params, i_nr_extra_params+1, len(nr_extra_params_list)))\n",
    "        f.flush()\n",
    "    \n",
    "        for i_comb, param_comb in enumerate(extra_param_combs):\n",
    "            input_features = core_input_features.copy()\n",
    "            input_features.extend(param_comb)\n",
    "            \n",
    "            # prepare the training data\n",
    "            training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, \n",
    "                                                   total_set_size, train_size, val_size, test_size)\n",
    "            training_data_dict = normalise_data(training_data_dict, norm)\n",
    "            \n",
    "            date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "            comb_counter += 1\n",
    "            f.write(date_string_proper + '        Testing combination %d/%d. \\n\\n' % (comb_counter, nr_combs_total))\n",
    "            f.flush()\n",
    "            \n",
    "            original_scores = []\n",
    "            if not norm == 'none':\n",
    "                normed_scores = []\n",
    "            histories = []\n",
    "\n",
    "            for i_run in range(nr_runs_per_comb):\n",
    "\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(neuronsPerLayer, input_dim = len(input_features), activation = activationFunction))\n",
    "\n",
    "                for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "                    model.add(Dense(neuronsPerLayer, activation = activationFunction))\n",
    "\n",
    "                model.add(Dense(len(output_features)))\n",
    "\n",
    "                # Compile model\n",
    "                earlystop = EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=100, \\\n",
    "                          verbose=1, mode='auto')\n",
    "                callbacks_list = [earlystop]\n",
    "                model.compile(loss=loss_func_dict[loss_function], optimizer='adam')\n",
    "\n",
    "                # Fit the model\n",
    "                if norm == 'none':\n",
    "                    history = model.fit(training_data_dict['x_train'], training_data_dict['y_train'], \n",
    "                                        validation_data=(training_data_dict['x_val'], training_data_dict['y_val']), \n",
    "                                        epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list, \n",
    "                                        verbose=verb)\n",
    "                    orig_score = model.evaluate(x=training_data_dict['x_test'], y=training_data_dict['y_test'],\n",
    "                                               verbose=verb)\n",
    "                    original_scores.append(orig_score)\n",
    "                    \n",
    "                else:\n",
    "                    history = model.fit(training_data_dict['x_train_norm'] , training_data_dict['y_train_norm'], \n",
    "                                        validation_data=(training_data_dict['x_val_norm'], \n",
    "                                        training_data_dict['y_val_norm']), \n",
    "                                        epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list, \n",
    "                                        verbose=verb)\n",
    "                    norm_score = model.evaluate(x=training_data_dict['x_test_norm'], y=training_data_dict['y_test_norm'],\n",
    "                                               verbose=verb)\n",
    "                    normed_scores.append(norm_score)\n",
    "                    orig_score = get_test_score(model, training_data_dict, norm)\n",
    "                    original_scores.append(orig_score)\n",
    "                    \n",
    "                histories.append([history.history['loss'], history.history['val_loss']])\n",
    "                \n",
    "            if norm == 'none':\n",
    "                scores = original_scores\n",
    "            else:\n",
    "                scores = [normed_scores, original_scores]\n",
    "                \n",
    "            results_list.append([input_features, scores, histories])\n",
    "            \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark completed at ' + date_string_proper + '\\n')\n",
    "    f.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "custom_string = '5_total_inputs'\n",
    "with open('model_comparisons/' + custom_string + '.json', 'w+') as f:\n",
    "    json.dump(results_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(train_loss)\n",
    "print('Lowest train/val/test loss: %.2f, %.2f, %.2f' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a batch run result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a result\n",
    "loaded_list_string = '5_total_inputs'\n",
    "with open('model_comparisons/' + loaded_list_string + '.json', 'r') as f:\n",
    "    results_list = json.load(f)\n",
    "f.close()\n",
    "\n",
    "print(results_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_list[1][1]))\n",
    "print(results_list[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot the loss histories to make sure that the best performance was reached\n",
    "\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "\n",
    "    title = lst[0]\n",
    "    train_loss = lst[2][0][0]\n",
    "    val_loss = lst[2][0][1]\n",
    "    test_loss = lst[1][0]\n",
    "\n",
    "    #print('Lowest train/val/test loss: %.2e, %.2e, %.2e' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure(5, figsize=(8,8))\n",
    "    for i_run in range(len(test_loss)):\n",
    "        train_loss = lst[2][i_run][0]\n",
    "        val_loss = lst[2][i_run][1]\n",
    "        plt.plot(train_loss, 'b')\n",
    "        plt.plot(val_loss, 'r')\n",
    "    plt.yscale('log')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_list[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Get the X best performing input parameters\n",
    "get_normed_scores = 1\n",
    "top_nr_of_parameter_combs = 10\n",
    "column_widths = [0.2, 0.05]\n",
    "test_results = []\n",
    "val_results = []\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "    if get_normed_scores:\n",
    "        best_test = np.amin(lst[1][0])\n",
    "    else:\n",
    "        best_test = np.amin(lst[1][1])\n",
    "        \n",
    "#    val_losses = []\n",
    "#    for i_run in range(len(lst[1])):\n",
    "#        val_losses.append(lst[2][i_run][1])\n",
    "        \n",
    "    test_results.append(best_test)\n",
    "#    val_results.append(np.amin(val_losses))\n",
    "    \n",
    "best_test_indices = np.argsort(test_results)\n",
    "#best_val_indices = np.argsort(val_results)\n",
    "\n",
    "fig1362 = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "collabel=('Input parameters', 'best test mse\\n(normalised units)')\n",
    "table_vals = []\n",
    "for i in range(top_nr_of_parameter_combs):\n",
    "    \n",
    "    inputs = ', '.join(results_list[best_test_indices[i]+1][0][:3])\n",
    "    inputs = inputs + '\\n+\\n' + ', '.join(results_list[best_test_indices[i]+1][0][3:])\n",
    "    \n",
    "    table_vals.append([inputs, '%.3e' % \n",
    "                      (np.amin(results_list[best_test_indices[i]+1][1]))])\n",
    "\n",
    "the_table = ax.table(cellText=table_vals,colLabels=collabel,colWidths=column_widths,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 4)\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, output norm: False, 10 layers, 30 neurons per layer' % \n",
    "          (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization']) +\n",
    "           '\\nFixed input parameters: %s' % (', '.join(results_list[0]['fixed_input_features'])), \n",
    "          fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 1.15])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1362.savefig(fig_dir + '5_total_inputs' + '_param_comb_scores_test.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot best validation scores\n",
    "fig2 = plt.figure(figsize=(16,8))\n",
    "\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "collabel=('Input parameters', 'best val mse\\n(scaled units)')\n",
    "table_vals = []\n",
    "for i in range(top_nr_of_parameter_combs):\n",
    "    \n",
    "    inputs = ', '.join(results_list[best_val_indices[i]+1][0])\n",
    "    \n",
    "    table_vals.append([inputs, '%.3f' % \n",
    "                      (val_results[best_val_indices[i]])])\n",
    "\n",
    "the_table = ax.table(cellText=table_vals,colLabels=collabel,colWidths=column_widths,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 3)\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n",
    "                wspace=0.2, hspace=0.5)\n",
    "\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, relu on output: False, 10 layers, 10 neurons per layer' % \n",
    "          (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization'], \n",
    "          ), fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 1])\n",
    "\n",
    "#plt.tight_layout(h_pad=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2.savefig(fig_dir + '2018-05-12--13-36-02_param_comb_scores_val.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find out how a parameter affects the result\n",
    "collabel = ('Input parameter', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10')\n",
    "first_col_width = .1\n",
    "last_col_width = .02\n",
    "column_widths = [first_col_width]\n",
    "for i in range(10):\n",
    "    column_widths.append(last_col_width)\n",
    "tab_rows = []\n",
    "row_colours = []\n",
    "\n",
    "for param in results_list[0]['tested_input_features']:\n",
    "    \n",
    "    tab_row = [param]\n",
    "    row_colour = ['w']\n",
    "\n",
    "    for i, ind in enumerate(best_test_indices[:10]):\n",
    "        inputs = results_list[ind+1][0]\n",
    "        if param in inputs:\n",
    "            tab_row.append('Yes')\n",
    "            row_colour.append('g')\n",
    "        else:\n",
    "            tab_row.append('No')\n",
    "            row_colour.append('r')\n",
    "\n",
    "    tab_rows.append(tab_row)\n",
    "    row_colours.append(row_colour)\n",
    "        \n",
    "fig8845 = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "the_table = ax.table(cellText=tab_rows,colLabels=collabel,colWidths=column_widths,\n",
    "                     cellColours=row_colours,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 3)\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n",
    "                wspace=0.2, hspace=0.5)\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, output norm: False, 10 layers, 30 neurons per layer'\n",
    "          % (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization'])+\n",
    "           '\\nFixed input parameters: %s\\nOrdering based on test scores (normalised units)' % \n",
    "          (', '.join(results_list[0]['fixed_input_features'])), fontsize=20)\n",
    "#plt.title('Ordering based on validation scores', fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 0.8])\n",
    "\n",
    "#ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8845.savefig(fig_dir + '5_total_inputs' + '_param_importance_test.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(train_loss, 'b')\n",
    "plt.plot(val_loss, 'r')\n",
    "plt.yscale('log')\n",
    "plt.title(title)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On preprocessed data\n",
    "test_loss, test_mse = model.evaluate(x_test_norm, y_test_norm, verbose=0)\n",
    "print('MSE for the processed data: %.4f' % (test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict real value of points\n",
    "if norm == 'zero_mean_unit_std':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * training_data_dict['y_data_stds'] + training_data_dict['y_data_means']\n",
    "    \n",
    "if norm == 'zero_to_one':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * (training_data_dict['y_data_max'] - training_data_dict['y_data_min']) + \\\n",
    "                        training_data_dict['y_data_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get mse for the real predictions\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print('MSE for the unprocessed data: %.4f' % (total_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model if it is useful\n",
    "importlib.reload(model_management)\n",
    "model_dictionary = {\n",
    "    'training_method': 'backprop',\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'number_of_epochs': nEpochs,\n",
    "    'batch_size': batchSize,\n",
    "    'number_of_layers': nLayers,\n",
    "    'neurons_per_layer': neuronsPerLayer,\n",
    "    'activation_function': activationFunction,\n",
    "    'train_set_size': train_size,\n",
    "    'loss_function': loss_function,\n",
    "    'test_loss': test_loss,\n",
    "    'test_mse': test_mse,\n",
    "    'preprocess_data': preprocess_data\n",
    "}\n",
    "description = 'First network trained on preprocessed data.'\n",
    "model_management.SaveModel(model, model_dictionary, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#x1 = np.linspace(np.min(x_test[:,0]), np.max(x_test[:,0]), 30)\n",
    "#x2 = np.linspace(np.min(x_test[:,1]), np.max(x_test[:,1]), 30)\n",
    "#X1, X2 = np.meshgrid(x1, x2)\n",
    "#Z = np.zeros(X1.shape)\n",
    "#for i in range(30):\n",
    "#    for j in range(30):\n",
    "#        Z[i, j] = model.predict(np.array([X1[i,j], X2[i,j]])) TODO varför funkar inte det här??\n",
    "        \n",
    "#fig = plt.figure(4)\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "#ax.set_xlabel('x')\n",
    "#ax.set_ylabel('y')\n",
    "#ax.set_zlabel('z')\n",
    "        \n",
    "### Old visualisation way\n",
    "### Visualisation of prediction strength for when we have 2 input features\n",
    "if plot_threeD and len(input_features) == 2:\n",
    "    predictedY = model.predict(x_test_norm)\n",
    "    predictedY = predictedY * y_data_stds + y_data_means\n",
    "    fig = plt.figure(2, figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               y_test[:,0], s=3)\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               predictedY, s=3)\n",
    "    ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[0]))\n",
    "    ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[1]))\n",
    "    ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplots and boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from pylab import plot, show, savefig, xlim, figure, \\\n",
    "#                hold, ylim, legend, boxplot, setp, axes\n",
    "nBins = 8\n",
    "halo_min_mass = np.amin(galaxies[:,6])\n",
    "halo_max_mass = np.amax(galaxies[:,6])\n",
    "bin_edges = np.linspace(halo_min_mass, halo_max_mass, nBins+1)\n",
    "\n",
    "x_test = training_data_dict['x_test']\n",
    "y_test = training_data_dict['y_test']\n",
    "\n",
    "predictedY = predicted_points\n",
    "\n",
    "for i, feat in enumerate(output_features):\n",
    "    \n",
    "    ### Plot 1\n",
    "    fig1 = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.plot(y_test[:,i], y_test[:,i], 'k.')\n",
    "    plt.plot(predictedY[:,i], y_test[:,i], 'g.')\n",
    "    plt.ylabel('True %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('Predicted %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.legend(['Ideal result', 'predicted ' + feat], loc='upper center')\n",
    "    #plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "    #    batchSize, train_size, test_mse) + \n",
    "    #    '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "    #    len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "    #    test_size), y=1.03, fontsize=20)\n",
    "    plt.show\n",
    "    \n",
    "    ### Plot 2 - boxplot\n",
    "    \n",
    "    # bin_means contain (0: mean of the binned values, 1: bin edges, 2: numbers pointing each example to a bin)\n",
    "    bin_means_true = stats.binned_statistic(x_test[:,i], y_test[:,i], bins=bin_edges)\n",
    "    bin_means_pred = stats.binned_statistic(x_test[:,i], predictedY[:,i].flatten(), bins=bin_edges)\n",
    "    bin_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        bin_center = (bin_means_true[1][iBin] + bin_means_true[1][iBin+1]) / 2\n",
    "        bin_centers.append('%.2f' % (bin_center))\n",
    "    sorted_true_y_data = []\n",
    "    sorted_pred_y_data = []\n",
    "    for iBin in range(1,nBins+1):\n",
    "        sorted_true_y_data.append(y_test[bin_means_true[2] == iBin, i])\n",
    "        sorted_pred_y_data.append(predictedY[bin_means_pred[2] == iBin,i])\n",
    "        \n",
    "    fig2 = plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    bin_pos = np.array([-2,-1]) # (because this makes it work)\n",
    "    x_label_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        # Every boxplot adds 2 boxes, one from the true data and one from the predicted data\n",
    "        bin_pos += 3 \n",
    "        plt.boxplot([sorted_true_y_data[iBin], sorted_pred_y_data[iBin]] , positions = bin_pos, widths = 0.9)\n",
    "        x_label_centers.append(np.mean(bin_pos))\n",
    "    \n",
    "    plt.ylabel('%s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('True Halo mass log($M_{G}/M_{S}$)', fontsize=15)\n",
    "    ax.set_xlim(left=x_label_centers[0]-2, right=x_label_centers[-1]+2)\n",
    "    #xlim(0,bin_pos[1] + 1)\n",
    "    plt.xticks(x_label_centers, bin_centers)\n",
    "    #plt.text(12,7,'Left: true data. Right: predicted data.', fontsize=20)\n",
    "    \n",
    "    if feat == 'SFR':\n",
    "        ax.axhline(y=0, linestyle='--')\n",
    "    \n",
    "    #plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "    #    batchSize, train_size, test_mse) + \n",
    "    #    '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "    #    len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "    #    test_size), y=1.03, fontsize=20)\n",
    "    \n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.savefig(fig_dir+'bp_output_scatter_%d_plot_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')\n",
    "fig2.savefig(fig_dir+'bp_output_boxplot_%d_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplots and plots with errorbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict real value of points\n",
    "if norm == 'zero_mean_unit_std':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * training_data_dict['y_data_stds'] + training_data_dict['y_data_means']\n",
    "    \n",
    "if norm == 'zero_to_one':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * (training_data_dict['y_data_max'] - training_data_dict['y_data_min']) + \\\n",
    "                        training_data_dict['y_data_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from pylab import plot, show, savefig, xlim, figure, \\\n",
    "#                hold, ylim, legend, boxplot, setp, axes\n",
    "nBins = 8\n",
    "halo_min_mass = np.amin(galaxies[:,6])\n",
    "halo_max_mass = np.amax(galaxies[:,6])\n",
    "bin_edges = np.linspace(halo_min_mass, halo_max_mass, nBins+1)\n",
    "\n",
    "x_test = training_data_dict['x_test']\n",
    "y_test = training_data_dict['y_test']\n",
    "\n",
    "predictedY = predicted_points\n",
    "\n",
    "for i, feat in enumerate(output_features):\n",
    "    \n",
    "    \n",
    "    ### Plot 1\n",
    "    fig1 = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.plot(y_test[:,i], y_test[:,i], 'k.')\n",
    "    plt.plot(predictedY[:,i], y_test[:,i], 'g.')\n",
    "    plt.ylabel('True %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('Predicted %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.legend(['Ideal result', 'predicted ' + feat], loc='upper center')\n",
    "    #plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "    #    batchSize, train_size, test_mse) + 'loss function: %s\\n' % (loss_function) +\n",
    "    #    '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d data points (test) shown' % (\n",
    "    #    len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "    #    test_size), y=1.03, fontsize=20)\n",
    "    plt.show\n",
    "    \n",
    "    ### Plot 2 - Spread of the distribution\n",
    "    \n",
    "    # bin_means contain (0: mean of the binned values, 1: bin edges, 2: numbers pointing each example to a bin)\n",
    "    bin_means_true = stats.binned_statistic(x_test[:,0], y_test[:,i], bins=bin_edges)\n",
    "    bin_means_pred = stats.binned_statistic(x_test[:,0], predictedY[:,i].flatten(), bins=bin_edges)\n",
    "\n",
    "    bin_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        bin_center = (bin_means_true[1][iBin] + bin_means_true[1][iBin+1]) / 2\n",
    "        bin_centers.append('%.2f' % (bin_center))\n",
    "    sorted_true_y_data = []\n",
    "    sorted_pred_y_data = []\n",
    "    for iBin in range(1,nBins+1):\n",
    "        sorted_true_y_data.append(y_test[bin_means_true[2] == iBin, i])\n",
    "        sorted_pred_y_data.append(predictedY[bin_means_pred[2] == iBin,i])\n",
    "    \n",
    "    # get standard deviations of the binned values\n",
    "    stds_true = np.zeros((nBins))\n",
    "    stds_pred = np.zeros((nBins))\n",
    "    for iBin in range(nBins):\n",
    "        stds_true[iBin] = np.std(sorted_true_y_data[iBin])\n",
    "        stds_pred[iBin] = np.std(sorted_pred_y_data[iBin])\n",
    "        \n",
    "    fig2 = plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    bin_pos = np.array([-2,-1]) # (because this makes it work)\n",
    "    x_label_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        # Every plot adds 2 distributions, one from the true data and one from the predicted data\n",
    "        bin_pos += 3 \n",
    "        plt.errorbar(bin_pos[0], bin_means_true[0][iBin], yerr=stds_true[iBin], fmt = 'bo', capsize=5)\n",
    "        plt.errorbar(bin_pos[1], bin_means_pred[0][iBin], yerr=stds_pred[iBin], fmt = 'ro', capsize=5)\n",
    "        x_label_centers.append(np.mean(bin_pos))\n",
    "    \n",
    "    plt.ylabel('%s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('True Halo mass log($M_{G}/M_{S}$)', fontsize=15)\n",
    "    plt.legend(['True data $\\pm 1 \\sigma$', 'Predicted data $\\pm 1 \\sigma$'], loc='upper left', fontsize='xx-large')\n",
    "    ax.set_xlim(left=x_label_centers[0]-2, right=x_label_centers[-1]+2)\n",
    "    #xlim(0,bin_pos[1] + 1)\n",
    "    plt.xticks(x_label_centers, bin_centers)\n",
    "    \n",
    "    #plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "    #    batchSize, train_size, test_mse) + 'loss function: %s\\n' % (loss_function) +\n",
    "    #    '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d data points (test) shown' % (\n",
    "    #    len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "    #    test_size), y=1.03, fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.savefig(fig_dir+'bp_output_scatter_%d_plot_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')\n",
    "fig2.savefig(fig_dir+'bp_output_boxplot_%d_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(predictedY.flatten())\n",
    "#bin_means = stats.binned_statistic(predictedY.flatten(), y_test[:,0], bins=10)\n",
    "#bin_stds = stats.binned_statistic(predictedY.flatten(), y_test[:,0], bins=10, statistic=GetSTD)\n",
    "#print(bin_means[0])\n",
    "#print(bin_stds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SFR vs Stellar mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the figure\n",
    "fig.savefig(fig_dir+'bp_sfr_to_stellar_mass_inputs_' + '_and_'.join(input_features)+\n",
    "            '_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see how the MSE is calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_points = model.predict(x_test)\n",
    "print(np.shape(predicted_points))\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print(total_score)\n",
    "\n",
    "keras_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(keras_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING\n",
    "coordinates = galaxies[:1000, :3]\n",
    "halo_masses = np.power(10, galaxies[:1000, 6])\n",
    "nr_points = np.shape(coordinates)[0]\n",
    "\n",
    "nr_neighbours_wanted = 30\n",
    "box_sides = np.array([200, 200, 200])\n",
    "\n",
    "neigh_densities = get_density_periodic(coordinates, halo_masses, nr_neighbours_wanted, \n",
    "                                                                 box_sides, nr_points, verbatim=True)\n",
    "print(neigh_densities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
