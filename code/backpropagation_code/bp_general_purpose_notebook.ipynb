{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "module_path = home_dir + '/code/modules/'\n",
    "models_path = home_dir + '/models/'\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "fig_dir = 'figures/'\n",
    "import time\n",
    "import importlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, LeakyReLU, concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "#import model_management\n",
    "from scipy.special import comb\n",
    "import datetime\n",
    "import codecs, json\n",
    "import corner\n",
    "from itertools import combinations\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport data_processing\n",
    "%aimport plotting\n",
    "%aimport keras_objects\n",
    "from data_processing import *\n",
    "from plotting import *\n",
    "from keras_objects import *\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run on CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameter string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set name ending with parameters for figures to be saved\n",
    "param_string = 'nLayers_%d_nNeurons_%d_actFun_%s_lossFunc_%s_nTrainSamples_%d_nEpochs_%d_batchSize_%d' % (\n",
    "    nLayers, neuronsPerLayer, activationFunction, loss_function, train_size, nEpochs, batchSize)\n",
    "print(param_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y_test_norm, 0))\n",
    "print(np.std(y_test_norm, 0))\n",
    "print(np.min(x_test_norm, 0))\n",
    "print(np.max(x_test_norm, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a feel for the data\n",
    "for i in range(len(input_features)):\n",
    "    print(input_features[i],': min: %.2e, max: %.2e.' % (np.min(x_train[:,i]), np.max(x_train[:,i])))\n",
    "for i in range(len(output_features)):\n",
    "    print(output_features[i],': min: %.2e, max: %.2e.' % (np.min(y_train[:,i]), np.max(y_train[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualisation for when we have 2 input features\n",
    "%matplotlib notebook\n",
    "input_feat_1 = 0\n",
    "input_feat_2 = 1\n",
    "output_feat = 1\n",
    "\n",
    "fig = plt.figure(1, figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train_norm[:500,input_feat_1], x_train_norm[:500,input_feat_2], \n",
    "           y_train_norm[:500,output_feat])\n",
    "ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_1]))\n",
    "ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_2]))\n",
    "ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[output_feat]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search for the model that you want\n",
    "importlib.reload(model_management)\n",
    "search_dict = {\n",
    "    'training_method': 'backprop'\n",
    "}\n",
    "[model_dicts, description_dicts] = model_management.SearchModel(search_dict, get_hits=True)\n",
    "print(description_dicts)\n",
    "print('\\n')\n",
    "for key in model_dicts:\n",
    "    print(key)\n",
    "    print(model_dicts[key])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model_management)\n",
    "model, model_dict, description = model_management.LoadModel(search_dict, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the standard pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test' # 'train', 'val, 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scores = model.evaluate(x=training_data_dict['input_test_dict'], y=training_data_dict['output_test_dict'],\n",
    "                                               verbose=1)\n",
    "tot_score = norm_scores[0]\n",
    "predicted_points = predict_points(model, training_data_dict, data_type = mode)\n",
    "title = 'Inputs: %s\\n%.1e train points, test mse %.3e, %s data' % (', '.join(input_features), train_size, tot_score, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Standard plots\n",
    "fig1 = get_pred_vs_real_scatterplot(model, training_data_dict, unit_dict, data_keys, 'SFR', title=title, data_type=mode,\n",
    "                                   predicted_points = predicted_points)\n",
    "fig2 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, predicted_feat = 'Stellar_mass', \n",
    "                                binning_feat = 'Halo_mass', title=title, data_type=mode,\n",
    "                                predicted_points = predicted_points)\n",
    "fig3 = get_scatter_comparison_plots(model, training_data_dict, unit_dict, x_axis_feature = 'Halo_mass', \n",
    "                                    y_axis_feature = 'Stellar_mass', title=title, y_max = None, y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type=mode, predicted_points = predicted_points)\n",
    "fig4 = get_real_vs_pred_boxplot(model, training_data_dict, unit_dict, data_keys, predicted_feat = 'SFR', \n",
    "                                binning_feat = 'Stellar_mass', title=title, data_type=mode,\n",
    "                                predicted_points = predicted_points)\n",
    "fig5 = get_scatter_comparison_plots(model, training_data_dict, unit_dict, x_axis_feature = 'Stellar_mass', \n",
    "                                    y_axis_feature = 'SFR', title=title, y_max = 5, y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type=mode, predicted_points = predicted_points)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig6 = get_real_vs_pred_same_fig(model, training_data_dict, unit_dict, x_axis_feature='Halo_mass', \n",
    "                                 y_axis_feature = 'Stellar_mass', title=title, data_type=mode, marker_size=5,\n",
    "                                 y_min=None, y_max=None, x_min=None, x_max=None)\n",
    "fig7 = get_real_vs_pred_same_fig(model, training_data_dict, unit_dict, x_axis_feature='Stellar_mass', \n",
    "                                 y_axis_feature = 'SFR', title=title, data_type=mode, marker_size=2,\n",
    "                                 y_min=None, y_max=None, x_min=None, x_max=None)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig8, fig9 = get_sfr_stellar_mass_contour(model, training_data_dict, unit_dict, title=None, data_type='test',\n",
    "                                 y_min=None, y_max=None, x_min=None, x_max=None, predicted_points=predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the figures\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "fig1.savefig(fig_dir + date_string + '_' + '_'.join(input_features) + '_to_' + '_'.join(output_features) +'_true_pred_sfr_scatter.png', bbox_inches = 'tight')\n",
    "fig2.savefig(fig_dir + date_string + '_' + '_'.join(input_features) + '_to_' + '_'.join(output_features) + '_boxplot_stellar_mass.png', bbox_inches = 'tight')\n",
    "fig3.savefig(fig_dir + date_string + '_' + '_'.join(input_features) + '_to_' + '_'.join(output_features) +'_scatter_comp_halo_vs_stellar_mass.png', bbox_inches = 'tight')\n",
    "fig4.savefig(fig_dir + date_string + '_' + '_'.join(input_features) + '_to_' + '_'.join(output_features) +'_boxplot_sfr.png', bbox_inches = 'tight')\n",
    "fig5.savefig(fig_dir + date_string + '_' + '_'.join(input_features) + '_to_' + '_'.join(output_features) +'_scatter_comp_stellar_mass_vs_sfr.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "nr_steps = 5e3\n",
    "nr_repetitions = 20\n",
    "batch_size = 5e4\n",
    "total_set_size = 3e5 # how many examples will be used for training+validation+testing\n",
    "train_size = 2.5e5\n",
    "val_size = .25e5\n",
    "test_size = .25e5\n",
    "norm = {'input': 'zero_mean_unit_std',\n",
    "        'output': 'none'} # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "input_features = ['Halo_mass', 'Halo_mass_peak', 'Scale_half_mass', 'Scale_peak_mass']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "\n",
    "outputs_to_weigh = ['Stellar_mass']\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "\n",
    "early_stop_patience = int(nr_epochs / 10)\n",
    "early_stop_monitor = 'val_loss'\n",
    "early_stop_min_delta = 1e-16\n",
    "\n",
    "validation_data = 'val' #'val' is normally used, use 'train' to check overfitting potential\n",
    "\n",
    "### Network parameters\n",
    "nLayers = 10\n",
    "activation_function = 'tanh'\n",
    "output_activation = {'SFR': None, 'Stellar_mass': None}\n",
    "neuronsPerLayer = 10\n",
    "reg_strength = 0#1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the selected galaxyfile\n",
    "galaxies, data_keys, unit_dict = load_galfile()\n",
    "    \n",
    "# prepare the training data\n",
    "training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, \n",
    "                                       int(total_set_size), int(train_size), int(val_size), int(test_size))\n",
    "#galaxies = None\n",
    "training_data_dict = normalise_data(training_data_dict, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "main_input = Input(shape=(len(input_features),), name = 'main_input')\n",
    "\n",
    "x1 = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "          activation=activation_function)(main_input)\n",
    "x2 = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "          activation=activation_function)(main_input)\n",
    "for i in range(0, nLayers-2): # -2 because one layer is added automatically with the input layer and one before the loop\n",
    "    x1 = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "              activation=activation_function)(x1)\n",
    "    x2 = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "              activation=activation_function)(x2)\n",
    "    #x = LeakyReLU(alpha = 0.1)(x)\n",
    "        \n",
    "output_layers = []\n",
    "output_layers.append(Dense(1, kernel_regularizer=regularizers.l2(reg_strength), name = 'Stellar_mass',\n",
    "                          activation = output_activation['Stellar_mass'])(x1))\n",
    "output_layers.append(Dense(1, kernel_regularizer=regularizers.l2(reg_strength), name = 'SFR',\n",
    "                          activation = output_activation['SFR'])(x2))\n",
    "\n",
    "model = Model(main_input, output_layers)\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, patience=early_stop_patience, \\\n",
    "                          verbose=1, mode='auto')\n",
    "file_path = models_path + \"model_checkpoint.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True, mode='min', period=10)\n",
    "#callbacks_list = [earlystop, checkpoint]\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "# set data point weights\n",
    "train_weights_tmp = training_data_dict['original_data'][training_data_dict['train_indices'], \n",
    "                                              training_data_dict['original_data_keys']['Halo_mass']]\n",
    "train_weights_tmp = np.power(10, train_weights_tmp)\n",
    "train_weights_tmp = train_weights_tmp / np.sum(train_weights_tmp)\n",
    "val_weights_tmp = training_data_dict['original_data'][training_data_dict['val_indices'], \n",
    "                                              training_data_dict['original_data_keys']['Halo_mass']]\n",
    "val_weights_tmp = np.power(10, val_weights_tmp)\n",
    "val_weights_tmp = val_weights_tmp / np.sum(val_weights_tmp)\n",
    "train_weights = {}\n",
    "val_weights = {}\n",
    "    \n",
    "for output in output_features:\n",
    "    if output in outputs_to_weigh:\n",
    "        train_weights[output] = train_weights_tmp\n",
    "        val_weights[output] = val_weights_tmp\n",
    "    else:\n",
    "        train_weights[output] = np.ones(int(train_size))\n",
    "        val_weights[output] = np.ones(int(val_size))\n",
    "\n",
    "for i_rep in range(1, nr_repetitions+1):\n",
    "    \n",
    "    \n",
    "    history = model.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                        validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n",
    "                        training_data_dict['output_'+validation_data+'_dict'], val_weights), \n",
    "                        epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list,\n",
    "                        sample_weight=train_weights, verbose=0)\n",
    "    fig = get_scatter_comparison_plots(model, training_data_dict, unit_dict, x_axis_feature = 'Stellar_mass', \n",
    "                                    y_axis_feature = 'SFR', title='rep %d/%d'%(i_rep,nr_repetitions), y_max = None, \n",
    "                                    y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type='val')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(main_input, output_layers)\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "#plt.title(title)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a batch run to see which input parameters gives the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_set_size = 300000\n",
    "train_size = 250000\n",
    "val_size = 25000\n",
    "test_size = 25000\n",
    "\n",
    "core_input_features = ['Halo_mass', 'Halo_mass_peak', 'Scale_peak_mass']\n",
    "tested_input_features = ['Concentration', 'Type', 'Scale_half_mass', \n",
    "                 'Scale_last_MajM', 'Environmental_density']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "nr_extra_params_list = [2]\n",
    "nr_runs_per_comb = 3\n",
    "\n",
    "nLayers = 10\n",
    "activationFunction = 'tanh'\n",
    "neuronsPerLayer = 30\n",
    "loss_function = 'mse' # 'mse', 'weighted_mse_1' 'mae'\n",
    "norm = 'zero_to_one' # 'none',   'zero_mean_unit',   'zero_to_one'\n",
    "\n",
    "nr_steps = 1e4\n",
    "batch_size = 4e4\n",
    "\n",
    "verb = 0 # prints progress to stdout\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "parameter_dictionary = {\n",
    "    'fixed_input_features': core_input_features,\n",
    "    'tested_input_features': tested_input_features,\n",
    "    'output_features': output_features,\n",
    "    'nr_extra_parameter_combinations': nr_extra_params_list,\n",
    "    'nr_steps': [nr_steps],\n",
    "    'batch_size': [batch_size],\n",
    "    'nr_epochs': [nr_epochs],\n",
    "    'nr_training_samples': [train_size],\n",
    "    'nr_validation_samples': [val_size],\n",
    "    'nr_test_samples': [test_size],\n",
    "    'data_normalization': norm,\n",
    "    'activation_function': activationFunction,\n",
    "    'neurons_per_layer': [neuronsPerLayer],\n",
    "    'nr_hidden_layers': [nLayers],\n",
    "    'output_activation_function': 'none',\n",
    "    'description': 'Each parameter setting is represented by one list containing three objects. The first one is ' + \\\n",
    "    'the input parameters. The second one is the mse test scores obtained for the different runs evaluated on the ' +\\\n",
    "    'normed units and the original units of the data set (original only if norm is \\'none\\'). The third one is the ' +\\\n",
    "    'loss histories for the different runs [training_loss, validation_loss] evaluated on the normalised data.'\n",
    "}\n",
    "results_list = [parameter_dictionary]\n",
    "nr_combs_total = 0\n",
    "for nr_extra_params in nr_extra_params_list:\n",
    "    nr_combs_total += comb(len(tested_input_features), nr_extra_params)\n",
    "comb_counter = 0 # to keep track of how many combinations I've gone through\n",
    "\n",
    "with open('model_comparisons/progress.txt', 'w+') as f:\n",
    "    \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on input parameters at ' + date_string_proper + '\\n\\n')\n",
    "    f.flush()\n",
    "    \n",
    "    # load the selected galaxyfile\n",
    "    galaxies, data_keys, unit_dict = load_galfile()\n",
    "    \n",
    "    for i_nr_extra_params, nr_extra_params in enumerate(nr_extra_params_list):\n",
    "        \n",
    "        extra_param_combs = list(combinations(tested_input_features, nr_extra_params))\n",
    "        \n",
    "        date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "        f.write(date_string_proper + '    Testing %d extra parameters. %d/%d extra parameter count tested. \\n\\n' %\n",
    "                (nr_extra_params, i_nr_extra_params+1, len(nr_extra_params_list)))\n",
    "        f.flush()\n",
    "    \n",
    "        for i_comb, param_comb in enumerate(extra_param_combs):\n",
    "            input_features = core_input_features.copy()\n",
    "            input_features.extend(param_comb)\n",
    "            \n",
    "            # prepare the training data\n",
    "            training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, \n",
    "                                                   total_set_size, train_size, val_size, test_size)\n",
    "            training_data_dict = normalise_data(training_data_dict, norm)\n",
    "            \n",
    "            date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "            comb_counter += 1\n",
    "            f.write(date_string_proper + '        Testing combination %d/%d. \\n\\n' % (comb_counter, nr_combs_total))\n",
    "            f.flush()\n",
    "            \n",
    "            original_scores = []\n",
    "            if not norm == 'none':\n",
    "                normed_scores = []\n",
    "            histories = []\n",
    "\n",
    "            for i_run in range(nr_runs_per_comb):\n",
    "\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(neuronsPerLayer, input_dim = len(input_features), activation = activationFunction))\n",
    "\n",
    "                for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "                    model.add(Dense(neuronsPerLayer, activation = activationFunction))\n",
    "\n",
    "                model.add(Dense(len(output_features)))\n",
    "\n",
    "                # Compile model\n",
    "                earlystop = EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=100, \\\n",
    "                          verbose=1, mode='auto')\n",
    "                callbacks_list = [earlystop]\n",
    "                model.compile(loss=loss_func_dict[loss_function], optimizer='adam')\n",
    "\n",
    "                # Fit the model\n",
    "                if norm == 'none':\n",
    "                    history = model.fit(training_data_dict['x_train'], training_data_dict['y_train'], \n",
    "                                        validation_data=(training_data_dict['x_val'], training_data_dict['y_val']), \n",
    "                                        epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list, \n",
    "                                        verbose=verb)\n",
    "                    orig_score = model.evaluate(x=training_data_dict['x_test'], y=training_data_dict['y_test'],\n",
    "                                               verbose=verb)\n",
    "                    original_scores.append(orig_score)\n",
    "                    \n",
    "                else:\n",
    "                    history = model.fit(training_data_dict['x_train_norm'] , training_data_dict['y_train_norm'], \n",
    "                                        validation_data=(training_data_dict['x_val_norm'], \n",
    "                                        training_data_dict['y_val_norm']), \n",
    "                                        epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list, \n",
    "                                        verbose=verb)\n",
    "                    norm_score = model.evaluate(x=training_data_dict['x_test_norm'], y=training_data_dict['y_test_norm'],\n",
    "                                               verbose=verb)\n",
    "                    normed_scores.append(norm_score)\n",
    "                    orig_score = get_test_score(model, training_data_dict, norm)\n",
    "                    original_scores.append(orig_score)\n",
    "                    \n",
    "                histories.append([history.history['loss'], history.history['val_loss']])\n",
    "                \n",
    "            if norm == 'none':\n",
    "                scores = original_scores\n",
    "            else:\n",
    "                scores = [normed_scores, original_scores]\n",
    "                \n",
    "            results_list.append([input_features, scores, histories])\n",
    "            \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark completed at ' + date_string_proper + '\\n')\n",
    "    f.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "custom_string = '5_total_inputs'\n",
    "with open('model_comparisons/' + custom_string + '.json', 'w+') as f:\n",
    "    json.dump(results_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(train_loss)\n",
    "print('Lowest train/val/test loss: %.2f, %.2f, %.2f' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a batch run result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a result\n",
    "loaded_list_string = '5_total_inputs'\n",
    "with open('model_comparisons/' + loaded_list_string + '.json', 'r') as f:\n",
    "    results_list = json.load(f)\n",
    "f.close()\n",
    "\n",
    "print(results_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_list[1][1]))\n",
    "print(results_list[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot the loss histories to make sure that the best performance was reached\n",
    "\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "\n",
    "    title = lst[0]\n",
    "    train_loss = lst[2][0][0]\n",
    "    val_loss = lst[2][0][1]\n",
    "    test_loss = lst[1][0]\n",
    "\n",
    "    #print('Lowest train/val/test loss: %.2e, %.2e, %.2e' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure(5, figsize=(8,8))\n",
    "    for i_run in range(len(test_loss)):\n",
    "        train_loss = lst[2][i_run][0]\n",
    "        val_loss = lst[2][i_run][1]\n",
    "        plt.plot(train_loss, 'b')\n",
    "        plt.plot(val_loss, 'r')\n",
    "    plt.yscale('log')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_list[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Get the X best performing input parameters\n",
    "get_normed_scores = 1\n",
    "top_nr_of_parameter_combs = 10\n",
    "column_widths = [0.2, 0.05]\n",
    "test_results = []\n",
    "val_results = []\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "    if get_normed_scores:\n",
    "        best_test = np.amin(lst[1][0])\n",
    "    else:\n",
    "        best_test = np.amin(lst[1][1])\n",
    "        \n",
    "#    val_losses = []\n",
    "#    for i_run in range(len(lst[1])):\n",
    "#        val_losses.append(lst[2][i_run][1])\n",
    "        \n",
    "    test_results.append(best_test)\n",
    "#    val_results.append(np.amin(val_losses))\n",
    "    \n",
    "best_test_indices = np.argsort(test_results)\n",
    "#best_val_indices = np.argsort(val_results)\n",
    "\n",
    "fig1362 = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "collabel=('Input parameters', 'best test mse\\n(normalised units)')\n",
    "table_vals = []\n",
    "for i in range(top_nr_of_parameter_combs):\n",
    "    \n",
    "    inputs = ', '.join(results_list[best_test_indices[i]+1][0][:3])\n",
    "    inputs = inputs + '\\n+\\n' + ', '.join(results_list[best_test_indices[i]+1][0][3:])\n",
    "    \n",
    "    table_vals.append([inputs, '%.3e' % \n",
    "                      (np.amin(results_list[best_test_indices[i]+1][1]))])\n",
    "\n",
    "the_table = ax.table(cellText=table_vals,colLabels=collabel,colWidths=column_widths,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 4)\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, output norm: False, 10 layers, 30 neurons per layer' % \n",
    "          (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization']) +\n",
    "           '\\nFixed input parameters: %s' % (', '.join(results_list[0]['fixed_input_features'])), \n",
    "          fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 1.15])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1362.savefig(fig_dir + '5_total_inputs' + '_param_comb_scores_test.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot best validation scores\n",
    "fig2 = plt.figure(figsize=(16,8))\n",
    "\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "collabel=('Input parameters', 'best val mse\\n(scaled units)')\n",
    "table_vals = []\n",
    "for i in range(top_nr_of_parameter_combs):\n",
    "    \n",
    "    inputs = ', '.join(results_list[best_val_indices[i]+1][0])\n",
    "    \n",
    "    table_vals.append([inputs, '%.3f' % \n",
    "                      (val_results[best_val_indices[i]])])\n",
    "\n",
    "the_table = ax.table(cellText=table_vals,colLabels=collabel,colWidths=column_widths,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 3)\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n",
    "                wspace=0.2, hspace=0.5)\n",
    "\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, relu on output: False, 10 layers, 10 neurons per layer' % \n",
    "          (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization'], \n",
    "          ), fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 1])\n",
    "\n",
    "#plt.tight_layout(h_pad=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2.savefig(fig_dir + '2018-05-12--13-36-02_param_comb_scores_val.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find out how a parameter affects the result\n",
    "collabel = ('Input parameter', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10')\n",
    "first_col_width = .1\n",
    "last_col_width = .02\n",
    "column_widths = [first_col_width]\n",
    "for i in range(10):\n",
    "    column_widths.append(last_col_width)\n",
    "tab_rows = []\n",
    "row_colours = []\n",
    "\n",
    "for param in results_list[0]['tested_input_features']:\n",
    "    \n",
    "    tab_row = [param]\n",
    "    row_colour = ['w']\n",
    "\n",
    "    for i, ind in enumerate(best_test_indices[:10]):\n",
    "        inputs = results_list[ind+1][0]\n",
    "        if param in inputs:\n",
    "            tab_row.append('Yes')\n",
    "            row_colour.append('g')\n",
    "        else:\n",
    "            tab_row.append('No')\n",
    "            row_colour.append('r')\n",
    "\n",
    "    tab_rows.append(tab_row)\n",
    "    row_colours.append(row_colour)\n",
    "        \n",
    "fig8845 = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "the_table = ax.table(cellText=tab_rows,colLabels=collabel,colWidths=column_widths,\n",
    "                     cellColours=row_colours,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 3)\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n",
    "                wspace=0.2, hspace=0.5)\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, output norm: False, 10 layers, 30 neurons per layer'\n",
    "          % (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization'])+\n",
    "           '\\nFixed input parameters: %s\\nOrdering based on test scores (normalised units)' % \n",
    "          (', '.join(results_list[0]['fixed_input_features'])), fontsize=20)\n",
    "#plt.title('Ordering based on validation scores', fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 0.8])\n",
    "\n",
    "#ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8845.savefig(fig_dir + '5_total_inputs' + '_param_importance_test.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-09c8b6ceee50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m                                                     callbacks=callbacks_list, sample_weight=train_weights, verbose=0)\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                                 \u001b[0mtrain_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                                 \u001b[0mval_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                                 \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_loss'"
     ]
    }
   ],
   "source": [
    "total_set_size = int(3e4)\n",
    "\n",
    "input_features = ['Halo_mass', 'Halo_mass_peak', 'Scale_peak_mass', 'Scale_half_mass', 'Scale_last_MajM']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "outputs_to_weigh = ['Stellar_mass']\n",
    "\n",
    "nr_layers = [3, 8, 20]\n",
    "neurons_per_layer = [3, 10, 30]\n",
    "activation_functions = ['leaky_relu', 'tanh']\n",
    "nr_folds = 3\n",
    "\n",
    "loss_function = 'mse' # 'mse', 'weighted_mse_1' 'mae'\n",
    "input_norms = ['zero_mean_unit_std', 'none', 'zero_to_one']\n",
    "output_activation = {'SFR': None, 'Stellar_mass': None}\n",
    "reg_strengths = [0, 1e-10, 1e-5, 1e-2]\n",
    "\n",
    "nr_steps = [5e4]\n",
    "batch_sizes = np.array([5e2, 5e3, 1e4])\n",
    "nr_epochs = nr_steps * batch_sizes / (total_set_size * (nr_folds-1) / nr_folds)\n",
    "early_stop_patience = nr_epochs / 50\n",
    "early_stop_min_delta = 1e-20\n",
    "\n",
    "verb = 0 # prints progress to stdout\n",
    "\n",
    "parameter_dictionary = {\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'outputs_to_weigh': outputs_to_weigh,\n",
    "    'loss_function': loss_function,\n",
    "    'nr_steps': nr_steps,\n",
    "    'batch_sizes': batch_sizes,\n",
    "    'nr_epochs': nr_epochs,\n",
    "    'total_set_size': [total_set_size],\n",
    "    'reg_strengths': reg_strengths,\n",
    "    'nr_folds': nr_folds,\n",
    "    'input_norms': input_norms,\n",
    "    'activation_functions': activation_functions,\n",
    "    'neurons_per_layer': neurons_per_layer,\n",
    "    'nr_hidden_layers': nr_layers,\n",
    "    'output_activation_function': 'none',\n",
    "    'description': 'Each parameter setting is represented by one list containing four objects. The first one is ' + \\\n",
    "    'the parameters of the model. The second one is the mean best val score obtained based on the k folds. ' +\\\n",
    "    'The third one is the training' +\\\n",
    "    'loss histories for the different folds and the fourth one is the validation loss histories on the different folds.'\n",
    "}\n",
    "results_list = [parameter_dictionary]\n",
    "tot_nr_combs = len(nr_layers) * len(neurons_per_layer) * len(activation_functions) * len(input_norms) * \\\n",
    "                len(batch_sizes) * len(reg_strengths)\n",
    "comb_counter = 0 # to keep track of how many combinations I've gone through\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=100, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "with open('hyperparameter_searches/progress.txt', 'w+') as f:\n",
    "    \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on input parameters at ' + date_string_proper + '\\n\\n')\n",
    "    f.flush()\n",
    "    \n",
    "    # load the selected galaxyfile, be sure that the autoloaded galaxyfile is the one that you want to use!!\n",
    "    galaxies, data_keys, unit_dict = load_galfile()\n",
    "    \n",
    "    for i_inp_norm, inp_norm in enumerate(input_norms):\n",
    "        norm = {'input': inp_norm,\n",
    "                'output': 'none'}\n",
    "        for i_nr_lay, nr_lay in enumerate(nr_layers):\n",
    "            for i_neur_per_lay, neur_per_lay in enumerate(neurons_per_layer):\n",
    "                for i_act_fun, act_fun in enumerate(activation_functions):\n",
    "                    for i_batch_size, batch_size in enumerate(batch_sizes):\n",
    "                        for i_reg_strength, reg_strength in enumerate(reg_strengths):\n",
    "\n",
    "                            date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "                            comb_counter += 1\n",
    "                            f.write(date_string_proper + '        Testing combination %d/%d. \\n\\n' % (comb_counter, \n",
    "                                                                                                      tot_nr_combs))\n",
    "                            f.flush()\n",
    "\n",
    "                            train_histories = []\n",
    "                            val_histories = []\n",
    "                            scores = []\n",
    "                            \n",
    "                            for fold in range(nr_folds):\n",
    "\n",
    "                                # prepare the training data\n",
    "                                training_data_dict = divide_train_data(galaxies, data_keys, input_features, \n",
    "                                                                       output_features, total_set_size, k_fold_cv=True, \n",
    "                                                                       tot_cv_folds=nr_folds, cv_fold_nr=fold)\n",
    "                                training_data_dict = normalise_data(training_data_dict, norm)\n",
    "                                \n",
    "                                # set data point weights\n",
    "                                train_weights, val_weights = get_weights(training_data_dict, output_features, \n",
    "                                                                         outputs_to_weigh)\n",
    "                                \n",
    "                                # Create model\n",
    "                                main_input = Input(shape=(len(input_features),), name = 'main_input')\n",
    "                                \n",
    "                                if act_fun == 'leaky_relu':\n",
    "                                    x1 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength))(main_input)\n",
    "                                    x2 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength))(main_input)\n",
    "                                    x1 = LeakyReLU(alpha = 0.1)(x1)\n",
    "                                    x2 = LeakyReLU(alpha = 0.1)(x2)\n",
    "                                else:\n",
    "                                    x1 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "                                              activation=act_fun)(main_input)\n",
    "                                    x2 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "                                              activation=act_fun)(main_input)\n",
    "                                    \n",
    "                                for i in range(0, nr_lay-1): # -1 because one is added automatically\n",
    "                                    if act_fun == 'leaky_relu':\n",
    "                                        x1 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength))(x1)\n",
    "                                        x2 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength))(x2)\n",
    "                                        x1 = LeakyReLU(alpha = 0.1)(x1)\n",
    "                                        x2 = LeakyReLU(alpha = 0.1)(x2)\n",
    "                                        \n",
    "                                    else:\n",
    "                                        x1 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "                                                  activation=act_fun)(x1)\n",
    "                                        x2 = Dense(neur_per_lay, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "                                                  activation=act_fun)(x2)\n",
    "\n",
    "                                output_layers = []\n",
    "                                output_layers.append(Dense(1, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "                                                           name = 'Stellar_mass',\n",
    "                                                           activation = output_activation['Stellar_mass'])(x1))\n",
    "                                output_layers.append(Dense(1, kernel_regularizer=regularizers.l2(reg_strength), \n",
    "                                                           name = 'SFR',\n",
    "                                                           activation = output_activation['SFR'])(x2))\n",
    "\n",
    "                                model = Model(main_input, output_layers)\n",
    "                                model.compile(optimizer = 'adam', loss = loss_function)\n",
    "\n",
    "                                # Fit the model\n",
    "                                history = model.fit(x = training_data_dict['input_train_dict'], \n",
    "                                                    y = training_data_dict['output_train_dict'], \n",
    "                                                    validation_data = (training_data_dict['input_val_dict'], \n",
    "                                                    training_data_dict['output_val_dict'], val_weights), \n",
    "                                                    epochs=int(nr_epochs[i_batch_size]), batch_size=int(batch_size), \n",
    "                                                    callbacks=callbacks_list, sample_weight=train_weights, verbose=0)\n",
    "\n",
    "                                train_histories.append(history.history['train_loss'])\n",
    "                                val_histories.append(history.history['val_loss'])\n",
    "                                scores.append(np.amin(history.history['val_loss']))\n",
    "                                \n",
    "                            mean_score = np.mean(scores)\n",
    "                            parameters = {'inp_norm': inp_norm, 'nr_lay': nr_lay, 'neur_per_lay': neur_per_lay, \n",
    "                                         'act_fun': act_fun, 'batch_size': batch_size, 'reg_strength': reg_strength}\n",
    "                            results_list.append([parameters, mean_score, train_histories, val_histories])\n",
    "            \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark completed at ' + date_string_proper + '\\n')\n",
    "    f.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_list[1])\n",
    "print(len(results_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "custom_string = '5_total_inputs'\n",
    "with open('hyperparameter_searches/' + date_string + '.json', 'w+') as f:\n",
    "    json.dump(results_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On preprocessed data\n",
    "test_loss, test_mse = model.evaluate(x_test_norm, y_test_norm, verbose=0)\n",
    "print('MSE for the processed data: %.4f' % (test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict real value of points\n",
    "if norm == 'zero_mean_unit_std':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * training_data_dict['y_data_stds'] + training_data_dict['y_data_means']\n",
    "    \n",
    "if norm == 'zero_to_one':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * (training_data_dict['y_data_max'] - training_data_dict['y_data_min']) + \\\n",
    "                        training_data_dict['y_data_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get mse for the real predictions\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print('MSE for the unprocessed data: %.4f' % (total_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model if it is useful\n",
    "importlib.reload(model_management)\n",
    "model_dictionary = {\n",
    "    'training_method': 'backprop',\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'number_of_epochs': nEpochs,\n",
    "    'batch_size': batchSize,\n",
    "    'number_of_layers': nLayers,\n",
    "    'neurons_per_layer': neuronsPerLayer,\n",
    "    'activation_function': activationFunction,\n",
    "    'train_set_size': train_size,\n",
    "    'loss_function': loss_function,\n",
    "    'test_loss': test_loss,\n",
    "    'test_mse': test_mse,\n",
    "    'preprocess_data': preprocess_data\n",
    "}\n",
    "description = 'First network trained on preprocessed data.'\n",
    "model_management.SaveModel(model, model_dictionary, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#x1 = np.linspace(np.min(x_test[:,0]), np.max(x_test[:,0]), 30)\n",
    "#x2 = np.linspace(np.min(x_test[:,1]), np.max(x_test[:,1]), 30)\n",
    "#X1, X2 = np.meshgrid(x1, x2)\n",
    "#Z = np.zeros(X1.shape)\n",
    "#for i in range(30):\n",
    "#    for j in range(30):\n",
    "#        Z[i, j] = model.predict(np.array([X1[i,j], X2[i,j]])) TODO varför funkar inte det här??\n",
    "        \n",
    "#fig = plt.figure(4)\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "#ax.set_xlabel('x')\n",
    "#ax.set_ylabel('y')\n",
    "#ax.set_zlabel('z')\n",
    "        \n",
    "### Old visualisation way\n",
    "### Visualisation of prediction strength for when we have 2 input features\n",
    "if plot_threeD and len(input_features) == 2:\n",
    "    predictedY = model.predict(x_test_norm)\n",
    "    predictedY = predictedY * y_data_stds + y_data_means\n",
    "    fig = plt.figure(2, figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               y_test[:,0], s=3)\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               predictedY, s=3)\n",
    "    ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[0]))\n",
    "    ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[1]))\n",
    "    ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see how the MSE is calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_points = model.predict(x_test)\n",
    "print(np.shape(predicted_points))\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print(total_score)\n",
    "\n",
    "keras_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(keras_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model TESTING\n",
    "inputs = []\n",
    "\n",
    "main_input = Input(shape=(len(input_features),), name = 'main_input')\n",
    "halo_mass_input = Input(shape=(1,), name = 'Halo_mass')\n",
    "sfr_input = Input(shape=(1,), name = 'SFR')\n",
    "stellar_mass_input = Input(shape=(1,), name = 'Stellar_mass')\n",
    "inputs.append(main_input)\n",
    "inputs.append(halo_mass_input)\n",
    "#inputs.append(sfr_input)\n",
    "#inputs.append(stellar_mass_input)\n",
    "#for i_feat, feat in enumerate(weighted_output_features):\n",
    "#    inputs.append(Input(shape=(1,), name = feat))\n",
    "    \n",
    "for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "    if i == 0:\n",
    "        #x = concatenate([halo_mass_input, others_input])\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(main_input)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(main_input)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "    else:\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(x)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "        \n",
    "outputs = []\n",
    "\n",
    "sfr_output = Dense(1, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "sfr_weigh_loss = Nonweighted_loss_layer()([sfr_input, sfr_output])\n",
    "\n",
    "stellar_mass_output = Dense(1, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "stellar_mass_weigh_loss = Weighted_loss_layer()([halo_mass_input, stellar_mass_input, \n",
    "                                                                    stellar_mass_output])\n",
    "\n",
    "outputs.append(sfr_weigh_loss)\n",
    "outputs.append(stellar_mass_weigh_loss)\n",
    "\n",
    "#out = Weighted_loss_layer()([halo_mass_input, ])\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_core.compile(optimizer = 'adam', loss = tunnel_loss)\n",
    "\n",
    "earlystop = EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, patience=early_stop_patience, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "history = model_core.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                    validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n",
    "                    training_data_dict['output_'+validation_data+'_dict']), \n",
    "                    epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[5].get_weights()\n",
    "print(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
