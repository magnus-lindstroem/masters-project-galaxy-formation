{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "module_path = home_dir + '/modules/'\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "import time\n",
    "import datetime\n",
    "import importlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "import model_management\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "run_on_cpu = True\n",
    "total_set_size = 30000 # how many examples will be used for training+validation+testing\n",
    "train_size = 10000\n",
    "val_size = 10000\n",
    "test_size = 10000\n",
    "input_features = ['Halo_mass', 'Halo_mass_peak']\n",
    "output_features = ['Stellar_mass']\n",
    "nr_iters_before_restart_check = 60 # start making sure that the network did not converge to a local minimum\n",
    "min_std_tol = 0.01 # minimum allowed std for any parameter\n",
    "plot_threeD = 0\n",
    "save_figs = 0\n",
    "fig_dir = 'figures/'\n",
    "\n",
    "### Network parameters\n",
    "nr_hidden_layers = 10 \n",
    "activationFunction = 'tanh'\n",
    "nr_neurons_per_layer = 5\n",
    "\n",
    "### PSO parameters\n",
    "nIterations = 1000\n",
    "nParticles = 40\n",
    "xMin = -10\n",
    "xMax = 10\n",
    "alpha = 1\n",
    "deltaT = 1\n",
    "c1 = 2\n",
    "c2 = 2\n",
    "inertiaWeightStart = 1.4\n",
    "inertiaWeightMin = 0.3\n",
    "explorationFraction = 0.8\n",
    "\n",
    "data_dict = {'X_pos': 0, 'Y_pos': 1, 'Z_pos': 2, 'X_vel': 3, 'Y_vel': 4, \n",
    "             'Z_vel': 5, 'Halo_mass': 6, 'Stellar_mass': 7, 'SFR': 8, \n",
    "             'Intra_cluster_mass': 9, 'Halo_mass_peak': 10, \n",
    "             'Stellar_mass_obs': 11, 'SFR_obs': 12, 'Halo_radius': 13, \n",
    "             'Concentration': 14, 'Halo_spin': 15, 'Type': 16}\n",
    "unit_dict = {'X_pos': '', 'Y_pos': '', 'Z_pos': '', 'X_vel': '', 'Y_vel': '', \n",
    "             'Z_vel': '', 'Halo_mass': 'log($M_{G}/M_{S}$)', 'Stellar_mass': 'log($M_{G}/M_{S}$)', 'SFR': '', \n",
    "             'Intra_cluster_mass': '', 'Halo_mass_peak': 'log($M_{G}/M_{S}$)', \n",
    "             'Stellar_mass_obs': '', 'SFR_obs': '', 'Halo_radius': '', \n",
    "             'Concentration': '', 'Halo_spin': '', 'Type': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_on_cpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nLayers_10_nNeurons_10_actFun_tanh_nTrainSamples_1000_nIterations_1000_\n"
     ]
    }
   ],
   "source": [
    "### Set name ending with parameters for figures to be saved\n",
    "param_string = 'nLayers_%d_nNeurons_%d_actFun_%s_nTrainSamples_%d_nIterations_%d_' % (\n",
    "    nr_hidden_layers, nr_neurons_per_layer, activationFunction, train_size, nIterations)\n",
    "print(param_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(634742, 17)\n",
      "(305382, 17)\n",
      "[11.06128  10.74806  10.517851 11.935673 11.820144 11.564987 10.759135\n",
      " 10.828308 10.803138 10.55098 ]\n"
     ]
    }
   ],
   "source": [
    "galfile = pd.read_hdf('/scratch/data/galcats/P200/galaxies.Z01.h5')\n",
    "galaxies = galfile.as_matrix()\n",
    "gal_header = galfile.keys().tolist()\n",
    "\n",
    "### Remove data points with halo mass below 10.5\n",
    "print(np.shape(galaxies))\n",
    "galaxies = galaxies[galaxies[:,6] > 10.5, :]\n",
    "print(np.shape(galaxies))\n",
    "print(galaxies[:10,6])\n",
    "\n",
    "halo_min_mass = np.min(galaxies[:, 6])\n",
    "halo_max_mass = np.max(galaxies[:, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the different data sets that will be used\n",
    "n_data_points = galaxies.shape[0]\n",
    "subset_indices = np.random.choice(n_data_points, total_set_size, replace=False)\n",
    "train_indices = subset_indices[: train_size]\n",
    "val_indices = subset_indices[train_size : train_size+val_size]\n",
    "test_indices = subset_indices[train_size+val_size :]\n",
    "\n",
    "x_train = np.zeros((len(train_indices), len(input_features)))\n",
    "x_val = np.zeros((len(val_indices), len(input_features)))\n",
    "x_test = np.zeros((len(test_indices), len(input_features)))\n",
    "y_train = np.zeros((len(train_indices), len(output_features)))\n",
    "y_val = np.zeros((len(val_indices), len(output_features)))\n",
    "y_test = np.zeros((len(test_indices), len(output_features)))\n",
    "\n",
    "for i in range(len(input_features)):\n",
    "    x_train[:,i] = galaxies[train_indices, data_dict[input_features[i]]]\n",
    "    x_val[:,i] = galaxies[val_indices, data_dict[input_features[i]]]\n",
    "    x_test[:,i] = galaxies[test_indices, data_dict[input_features[i]]]\n",
    "    \n",
    "for i in range(len(output_features)):\n",
    "    y_train[:,i] = galaxies[train_indices, data_dict[output_features[i]]]\n",
    "    y_val[:,i] = galaxies[val_indices, data_dict[output_features[i]]]\n",
    "    y_test[:,i] = galaxies[test_indices, data_dict[output_features[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you want to preprocess the data\n",
    "\n",
    "for i in range(np.size(x_train, 1)):\n",
    "    x_data_means = np.mean(x_train, 0)\n",
    "    x_data_stds = np.std(x_train, 0)\n",
    "\n",
    "    x_train_norm = (x_train - x_data_means) / x_data_stds\n",
    "    x_val_norm = (x_val - x_data_means) / x_data_stds\n",
    "    x_test_norm = (x_test - x_data_means) / x_data_stds\n",
    "\n",
    "for i in range(np.size(y_train, 1)):\n",
    "    y_data_means = np.mean(y_train, 0)\n",
    "    y_data_stds = np.std(y_train, 0)\n",
    "\n",
    "    y_train_norm = (y_train - y_data_means) / y_data_stds\n",
    "    y_val_norm = (y_val - y_data_means) / y_data_stds\n",
    "    y_test_norm = (y_test - y_data_means) / y_data_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01384457]\n",
      "[1.01360266]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_test_norm, 0))\n",
    "print(np.std(y_test_norm, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halo_mass : min: 1.05e+01, max: 1.46e+01.\n",
      "Halo_mass_peak : min: 1.05e+01, max: 1.46e+01.\n",
      "Stellar_mass : min: 7.00e+00, max: 1.16e+01.\n"
     ]
    }
   ],
   "source": [
    "### Get a feel for the data\n",
    "for i in range(len(input_features)):\n",
    "    print(input_features[i],': min: %.2e, max: %.2e.' % (np.min(x_train[:,i]), np.max(x_train[:,i])))\n",
    "for i in range(len(output_features)):\n",
    "    print(output_features[i],': min: %.2e, max: %.2e.' % (np.min(y_train[:,i]), np.max(y_train[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualisation for when we have 2 input features\n",
    "%matplotlib notebook\n",
    "input_feat_1 = 0\n",
    "input_feat_2 = 1\n",
    "output_feat = 1\n",
    "\n",
    "fig = plt.figure(1, figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train_norm[:500,input_feat_1], x_train_norm[:500,input_feat_2], \n",
    "           y_train_norm[:500,output_feat])\n",
    "ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_1]))\n",
    "ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_2]))\n",
    "ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[output_feat]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, particle 0, new swarm best. Train: 100.610, Val: 100.349\n",
      "Iteration 0, particle 1, new swarm best. Train: 40.689, Val: 40.877\n",
      "Iteration 0, particle 6, new swarm best. Train: 17.866, Val: 18.025\n",
      "Iteration 0, particle 26, new swarm best. Train: 10.176, Val: 10.312\n",
      "Iteration 0, particle 27, new swarm best. Train: 5.202, Val: 5.099\n",
      "Iteration 1, particle 12, new swarm best. Train: 4.111, Val: 4.073\n",
      "Iteration 2, particle 6, new swarm best. Train: 1.537, Val: 1.522\n",
      "Iteration 4, particle 10, new swarm best. Train: 1.400, Val: 1.420\n",
      "Iteration 5, particle 36, new swarm best. Train: 1.014, Val: 1.015\n",
      "Iteration 7, particle 7, new swarm best. Train: 0.477, Val: 0.469\n",
      "Iteration 10\n",
      "Iteration 10, particle 4, new swarm best. Train: 0.366, Val: 0.365\n",
      "Iteration 17, particle 29, new swarm best. Train: 0.356, Val: 0.349\n",
      "Iteration 20\n",
      "Iteration 30\n",
      "Iteration 40\n",
      "Iteration 50\n",
      "Iteration 50, particle 14, new swarm best. Train: 0.327, Val: 0.318\n",
      "Iteration 52, particle 1, new swarm best. Train: 0.308, Val: 0.302\n",
      "Iteration 60\n",
      "Iteration 70\n",
      "Iteration 78, particle 14, new swarm best. Train: 0.303, Val: 0.291\n",
      "Iteration 80\n",
      "Iteration 90\n",
      "Iteration 100\n",
      "Iteration 110\n",
      "Iteration 115, particle 16, new swarm best. Train: 0.299, Val: 0.290\n",
      "Iteration 120\n",
      "Iteration 130\n",
      "Iteration 140\n",
      "Iteration 150\n",
      "Iteration 160\n",
      "Iteration 170\n",
      "standard deviations of predicted parameters:  [1.0548444]\n",
      "Iteration 180\n",
      "standard deviations of predicted parameters:  [1.0548444]\n",
      "Iteration 190\n",
      "standard deviations of predicted parameters:  [1.0548444]\n",
      "Iteration 200\n",
      "standard deviations of predicted parameters:  [1.0548444]\n",
      "Iteration 210\n",
      "standard deviations of predicted parameters:  [1.0548444]\n",
      "Iteration 220\n",
      "standard deviations of predicted parameters:  [1.0548444]\n",
      "Iteration 230\n",
      "standard deviations of predicted parameters:  [1.0548444]\n",
      "Iteration 240\n",
      "Iteration 240, particle 16, new swarm best. Train: 0.295, Val: 0.284\n",
      "Iteration 250\n",
      "Iteration 260\n",
      "Iteration 270\n",
      "Iteration 280\n",
      "Iteration 290\n",
      "Iteration 300\n",
      "standard deviations of predicted parameters:  [1.037218]\n",
      "Iteration 310\n",
      "Iteration 313, particle 10, new swarm best. Train: 0.266, Val: 0.250\n",
      "Iteration 320\n",
      "Iteration 327, particle 15, new swarm best. Train: 0.262, Val: 0.254\n",
      "Iteration 330\n",
      "Iteration 340\n",
      "Iteration 350\n",
      "Iteration 360\n",
      "Iteration 368, particle 32, new swarm best. Train: 0.261, Val: 0.251\n",
      "Iteration 370\n",
      "Iteration 380\n",
      "Iteration 390\n",
      "Iteration 400\n",
      "Iteration 402, particle 33, new swarm best. Train: 0.254, Val: 0.244\n",
      "Iteration 404, particle 19, new swarm best. Train: 0.251, Val: 0.243\n",
      "Iteration 410\n",
      "Iteration 414, particle 19, new swarm best. Train: 0.251, Val: 0.242\n",
      "Iteration 420\n",
      "Iteration 430\n",
      "Iteration 435, particle 11, new swarm best. Train: 0.249, Val: 0.239\n",
      "Iteration 440\n",
      "Iteration 450\n",
      "Iteration 460\n",
      "Iteration 463, particle 20, new swarm best. Train: 0.247, Val: 0.241\n",
      "Iteration 470\n",
      "Iteration 470, particle 20, new swarm best. Train: 0.244, Val: 0.234\n",
      "Iteration 480\n",
      "Iteration 490\n",
      "Iteration 495, particle 30, new swarm best. Train: 0.237, Val: 0.224\n",
      "Iteration 500\n",
      "Iteration 510\n",
      "Iteration 520\n",
      "Iteration 530\n",
      "Iteration 540\n",
      "Iteration 550\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 560\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 570\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 580\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 590\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 600\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 610\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 620\n",
      "standard deviations of predicted parameters:  [0.90734214]\n",
      "Iteration 630\n",
      "Iteration 633, particle 30, new swarm best. Train: 0.237, Val: 0.224\n",
      "Iteration 640\n",
      "Iteration 650\n",
      "Iteration 660\n",
      "Iteration 670\n",
      "Iteration 680\n",
      "Iteration 690\n",
      "Iteration 698, particle 16, new swarm best. Train: 0.236, Val: 0.231\n",
      "Iteration 700\n",
      "Iteration 710\n",
      "Iteration 720\n",
      "Iteration 728, particle 34, new swarm best. Train: 0.235, Val: 0.229\n",
      "Iteration 730\n",
      "Iteration 740\n",
      "Iteration 750\n",
      "Iteration 760\n",
      "Iteration 770\n",
      "Iteration 780\n",
      "Iteration 783, particle 16, new swarm best. Train: 0.234, Val: 0.226\n",
      "Iteration 790\n",
      "Iteration 800\n",
      "SWITCH TO EPLOIT! Iteration 800/1000.\n",
      "Iteration 810\n",
      "Iteration 820\n",
      "Iteration 830\n",
      "Iteration 840\n",
      "standard deviations of predicted parameters:  [0.90771294]\n",
      "Iteration 850\n",
      "Iteration 856, particle 22, new swarm best. Train: 0.234, Val: 0.226\n",
      "Iteration 860\n",
      "Iteration 863, particle 31, new swarm best. Train: 0.228, Val: 0.219\n",
      "Iteration 870\n",
      "Iteration 880\n",
      "Iteration 890\n",
      "Iteration 900\n",
      "Iteration 910\n",
      "Iteration 920\n",
      "standard deviations of predicted parameters:  [0.8749069]\n",
      "Iteration 930\n",
      "standard deviations of predicted parameters:  [0.8749069]\n",
      "Iteration 940\n",
      "standard deviations of predicted parameters:  [0.8749069]\n",
      "Iteration 950\n",
      "standard deviations of predicted parameters:  [0.8749069]\n",
      "Iteration 960\n",
      "standard deviations of predicted parameters:  [0.8749069]\n",
      "Iteration 970\n",
      "standard deviations of predicted parameters:  [0.8749069]\n",
      "Iteration 980\n",
      "standard deviations of predicted parameters:  [0.8749069]\n",
      "Iteration 990\n"
     ]
    }
   ],
   "source": [
    "network = Feed_Forward_Neural_Network(nr_hidden_layers, nr_neurons_per_layer, input_features, output_features, \n",
    "                                      activationFunction)\n",
    "network.pso_setup({'inertiaWeightMin': 0.1})\n",
    "network.pso_train(nIterations, x_train_norm, y_train_norm, x_val_norm, y_val_norm, speed_check=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualisation of prediction strength for when we have 2 input features\n",
    "if plot_threeD and len(input_features) == 2:\n",
    "    predictedY = PredictFunc(bestWeightList, bestBiasList, model, 'test')\n",
    "    fig = plt.figure(2, figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               y_test[:,0], s=3)\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               predictedY, s=3)\n",
    "    ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[0]))\n",
    "    ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[1]))\n",
    "    ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nBins = 8\n",
    "bin_edges = np.linspace(halo_min_mass, halo_max_mass, nBins+1)\n",
    "\n",
    "predictedY = model.predict(x_test)\n",
    "\n",
    "for i, feat in enumerate(output_features):\n",
    "    \n",
    "    \n",
    "    ### Plot 1\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax = plt.subplot(211)\n",
    "    plt.plot(y_test[:,i], y_test[:,i], 'k.')\n",
    "    plt.plot(predictedY[:,i], y_test[:,i], 'g.')\n",
    "    plt.ylabel('True %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('Predicted %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.legend(['Ideal result', 'predicted ' + feat], loc='upper center')\n",
    "    plt.title('nIterations: %d, training set size: %d, test mse score: %.2e\\n' % (nIterations, \n",
    "        train_size, testScore) + \n",
    "        '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "        len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "        test_size), y=1.03, fontsize=20)\n",
    "    plt.show\n",
    "        \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'pso_output_scatter_%d_plot_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')\n",
    "    \n",
    "    ### Plot 2 - boxplot\n",
    "    \n",
    "    # bin_means contain (0: mean of the binned values, 1: bin edges, 2: numbers pointing each example to a bin)\n",
    "    bin_means_true = stats.binned_statistic(x_test[:,i], y_test[:,i], bins=bin_edges)\n",
    "    bin_means_pred = stats.binned_statistic(x_test[:,i], predictedY[:,i].flatten(), bins=bin_edges)\n",
    "    bin_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        bin_centers.append((bin_means_true[1][iBin] + bin_means_true[1][iBin+1]) / 2)\n",
    "    sorted_true_y_data = []\n",
    "    sorted_pred_y_data = []\n",
    "    for iBin in range(1,nBins+1):\n",
    "        sorted_true_y_data.append(y_test[bin_means_true[2] == iBin, i])\n",
    "        sorted_pred_y_data.append(predictedY[bin_means_pred[2] == iBin,i])\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(212)\n",
    "\n",
    "    bin_pos = np.array([-2,-1]) # (because this makes it work)\n",
    "    x_label_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        # Every boxplot adds 2 boxes, one from the true data and one from the predicted data\n",
    "        bin_pos += 3 \n",
    "        plt.boxplot([sorted_true_y_data[iBin], sorted_pred_y_data[iBin]] , positions = bin_pos, widths = 0.9)\n",
    "        x_label_centers.append(np.mean(bin_pos))\n",
    "    \n",
    "    plt.ylabel('%s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('True Halo mass log($M_{G}/M_{S}$)', fontsize=15)\n",
    "    ax.set_xlim(left=x_label_centers[0]-2, right=x_label_centers[-1]+2)\n",
    "    #xlim(0,bin_pos[1] + 1)\n",
    "    plt.xticks(x_label_centers, bin_centers) TODO fixa siffrorna\n",
    "    plt.text(12,7,'Left: true data. Right: predicted data.', fontsize=20)\n",
    "    \n",
    "    if feat == 'SFR':\n",
    "        ax.axhline(y=0, linestyle='--')\n",
    "    \n",
    "    #plt.title('nIterations: %d, training set size: %d, test mse score: %.2e\\n' % (nIterations, \n",
    "    #    train_size, testScore) + \n",
    "    #    '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "    #    len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "    #    test_size), y=1.03, fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'pso_output_boxplot_%d_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(trainingScoreHistory, 'b')\n",
    "plt.plot(validationScoreHistory, 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_Neural_Network():\n",
    "    \n",
    "    def __init__(self, nr_hidden_layers, nr_neurons_per_lay, input_features, output_features, \n",
    "                 activation_function):\n",
    "        \n",
    "        self.nr_hidden_layers = nr_hidden_layers\n",
    "        self.nr_neurons_per_lay = nr_neurons_per_lay\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "    def pso_setup(self, pso_param_dict={}):\n",
    "        \n",
    "        self.pso_swarm = PSO_Swarm(self, self.nr_hidden_layers, self.nr_neurons_per_lay, self.input_features, \n",
    "                                   self.output_features, self.activation_function, pso_param_dict=pso_param_dict)\n",
    "        \n",
    "    def pso_train(self, nr_iterations, x_train, y_train, x_val, y_val, speed_check=False):\n",
    "        \n",
    "        self.pso_swarm.train_network(nr_iterations, x_train, y_train, x_val, y_val, speed_check)\n",
    "\n",
    "\n",
    "class PSO_Swarm(Feed_Forward_Neural_Network):\n",
    "    \n",
    "    def __init__(self, parent, nr_hidden_layers, nr_neurons_per_lay, input_features, output_features, \n",
    "                 activation_function, loss_function='mse', metric='mse', pso_param_dict=None):\n",
    "        self.pso_param_dict = {\n",
    "            'nr_particles': 40,\n",
    "            'xMin': -10,\n",
    "            'xMax': 10,\n",
    "            'alpha': 1,\n",
    "            'deltaT': 1,\n",
    "            'c1': 2,\n",
    "            'c2': 2,\n",
    "            'inertiaWeightStart': 1.4,\n",
    "            'inertiaWeightMin': 0.3,\n",
    "            'explorationFraction': 0.8,\n",
    "            'min_std_tol': 0.01\n",
    "        }\n",
    "    \n",
    "        if pso_param_dict is not None:\n",
    "            for key in pso_param_dict:\n",
    "                if key in self.pso_param_dict:\n",
    "                    self.pso_param_dict[key] = pso_param_dict[key]\n",
    "                else:\n",
    "                    print('\\'%s\\ is not a valid key. Choose between:' % (key), self.pso_param_dict.keys())\n",
    "                    break\n",
    "        \n",
    "        self.parent = parent\n",
    "        self.nr_variables = (nr_hidden_layers-1)*nr_neurons_per_lay**2 + \\\n",
    "            (len(input_features)+len(output_features)+nr_hidden_layers)*nr_neurons_per_lay + len(output_features)\n",
    "        self.nr_hidden_layers = nr_hidden_layers\n",
    "        self.nr_neurons_per_lay = nr_neurons_per_lay\n",
    "        self.activation_function = activation_function\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        self.best_weights = None\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.inertia_weight = self.pso_param_dict['inertiaWeightStart']\n",
    "        self.vMax = (self.pso_param_dict['xMax']-self.pso_param_dict['xMin']) / self.pso_param_dict['deltaT']\n",
    "        \n",
    "        self.set_up_model()\n",
    "        \n",
    "        self.initialise_swarm()\n",
    "        \n",
    "    def set_up_model(self):\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(self.nr_neurons_per_lay, input_dim = len(self.input_features), \n",
    "                             activation = self.activation_function))\n",
    "    \n",
    "        for i in range(0, self.nr_hidden_layers-1):\n",
    "            self.model.add(Dense(self.nr_neurons_per_lay, activation = self.activation_function))\n",
    "\n",
    "        self.model.add(Dense(len(self.output_features), activation = None))\n",
    "                \n",
    "        self.model.compile(loss=self.loss_function, metrics=[self.metric], optimizer='adam')\n",
    "        \n",
    "    def train_network(self, nr_iterations, x_train, y_train, x_val, y_val, speed_check):\n",
    "\n",
    "        self.nr_iterations_trained = nr_iterations\n",
    "        self.nr_train_points_used = np.size(x_train, 0)\n",
    "        self.nr_val_points_used = np.size(x_val, 0)\n",
    "        \n",
    "        with open('progress.txt', 'w+') as f:\n",
    "\n",
    "            # make sure the output isn't just the same\n",
    "            shouldStartFresh = 1\n",
    "            while shouldStartFresh:\n",
    "                shouldStartFresh = 0\n",
    "\n",
    "                inertia_weight_reduction = np.exp(np.log(self.pso_param_dict['inertiaWeightMin'] / \n",
    "                                            self.pso_param_dict['inertiaWeightStart']) / \n",
    "                                            self.pso_param_dict['explorationFraction'] / nr_iterations)\n",
    "                inertia_weight = self.pso_param_dict['inertiaWeightStart']\n",
    "\n",
    "                self.validationScoreHistory = []\n",
    "                self.trainingScoreHistory = []\n",
    "                \n",
    "                self.avg_speed_before_history = []\n",
    "                self.avg_speed_after_history = []\n",
    "                    \n",
    "                lastTimeSwarmBest = 0\n",
    "                \n",
    "                self.initialise_swarm()\n",
    "\n",
    "                glob_start = time.time()\n",
    "                for iteration in range(nr_iterations):\n",
    "\n",
    "                    if (int(iteration/10) == iteration/10) and (iteration > 0):\n",
    "                        # see if network has run into a local minima                        \n",
    "                        if (iteration - lastTimeSwarmBest) > nr_iters_before_restart_check:\n",
    "                            self.set_weights(self.best_weights)\n",
    "                            y_pred = self.predict_output(x_val_norm)\n",
    "\n",
    "                            stds = np.std(y_pred, axis=0)\n",
    "                            print('standard deviations of predicted parameters: ', stds)\n",
    "                            shouldStartFresh = np.any(stds < self.pso_param_dict['min_std_tol'])\n",
    "                            if shouldStartFresh:\n",
    "                                break\n",
    "\n",
    "                        progress_end = time.time()\n",
    "                        elapsed_so_far = (progress_end - glob_start) / 60\n",
    "                        time_remaining = elapsed_so_far / iteration * (self.nr_iterations_trained - iteration)\n",
    "\n",
    "                        print('Iteration %d' % (iteration))\n",
    "                        f.write('%s      ' % (datetime.datetime.now().strftime(\"%H:%M:%S\")))\n",
    "                        f.write('Iterations tried: %d/%d     ' % (iteration, self.nr_iterations_trained))\n",
    "                        f.write('Elapsed time: %dmin     ' % (elapsed_so_far))\n",
    "                        f.write('Time remaining: %dmin.\\n' % (time_remaining))\n",
    "                        f.flush()\n",
    "\n",
    "                    for iParticle, particle in enumerate(self.particle_list):\n",
    "                        \n",
    "                        train_score = particle.evaluate_particle(x_train, y_train)\n",
    "\n",
    "                        is_swarm_best_train = (train_score < self.swarm_best_train)\n",
    "                        \n",
    "                        if is_swarm_best_train:\n",
    "                            \n",
    "                        \n",
    "                            lastTimeSwarmBest = iteration\n",
    "                            self.swarm_best_train = train_score\n",
    "                            self.swarm_best_position = particle.position\n",
    "                            \n",
    "                            val_score = particle.evaluate_particle(x_val, y_val)\n",
    "                            is_swarm_best_val = (val_score < self.swarm_best_val)\n",
    "                            if is_swarm_best_val: # only update best weights after val highscore\n",
    "                                self.best_weights = particle.get_weights()\n",
    "                            \n",
    "                            \n",
    "                            self.validationScoreHistory.append(val_score)\n",
    "                            self.trainingScoreHistory.append(train_score)\n",
    "\n",
    "                            print('Iteration %d, particle %d, new swarm best. Train: %.3f, Val: %.3f' % (iteration, \n",
    "                                                            iParticle, train_score, val_score))\n",
    "                            f.write('Iteration %d, particle %d, new swarm best. Train: %.3f, Val: %.3f\\n' % (iteration, \n",
    "                                                            iParticle, train_score, val_score))\n",
    "                            f.flush()\n",
    "\n",
    "\n",
    "                    self.update_swarm(speed_check, f)\n",
    "                    \n",
    "                    inertia_weight = self.update_inertia_weight(inertia_weight, inertia_weight_reduction, \n",
    "                                                                iteration, f)\n",
    "                    \n",
    "                    \n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "    def update_inertia_weight(self, inertia_weight, inertia_weight_reduction, iteration, f):\n",
    "        \n",
    "        isExploring = (inertia_weight > self.pso_param_dict['inertiaWeightMin'])\n",
    "        if isExploring:\n",
    "            inertia_weight = inertia_weight * inertia_weight_reduction\n",
    "            isExploring = (inertia_weight > self.pso_param_dict['inertiaWeightMin'])\n",
    "            if not isExploring:\n",
    "                print('SWITCH TO EPLOIT! Iteration %d/%d.' % (iteration, self.nr_iterations_trained))\n",
    "                f.write('SWITCH TO EPLOIT! Iteration %d/%d.\\n' % (iteration, self.nr_iterations_trained))\n",
    "                f.flush()\n",
    "        return inertia_weight\n",
    "        \n",
    "    def predict_output(self, x_data):\n",
    "    \n",
    "        y_pred = self.model.predict(x_data)  # always contains the best model so far\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def initialise_swarm(self):\n",
    "        \n",
    "        self.particle_list = []\n",
    "        \n",
    "        for i in range(self.pso_param_dict['nr_particles']):\n",
    "            \n",
    "            particle = PSO_Particle(self)\n",
    "            self.particle_list.append(particle)\n",
    "            \n",
    "        self.swarm_best_train = 1e20\n",
    "        self.swarm_best_val = 1e20\n",
    "        self.swarm_best_position = self.particle_list[0].best_position  # arbitrarily take the first position\n",
    "        self.best_particle_nr = 0\n",
    "        \n",
    "    def update_swarm(self, speed_check, f):\n",
    "        \n",
    "        self.speeds_before = []\n",
    "        self.speeds_after = []\n",
    "        #self.term_one = []\n",
    "        #self.term_two = []\n",
    "        #self.too_fast_count = 0\n",
    "        #self.mean_particle_best_difference = []\n",
    "        #self.mean_swarm_best_difference = []\n",
    "        \n",
    "        #q = np.random.uniform(size = self.nr_variables)\n",
    "        #r = np.random.uniform(size = self.nr_variables)\n",
    "        \n",
    "        for particle in self.particle_list:\n",
    "            particle.update_particle()\n",
    "            \n",
    "        #print('term 1: ', np.mean(self.term_one))\n",
    "        #print('term 2:', np.mean(self.term_two)) \n",
    "        #print('%d/%d particles were too fast.' % (self.too_fast_count, self.pso_param_dict['nr_particles']))\n",
    "        #print('mean particle best diff: %.2f'% (np.mean(self.mean_particle_best_difference)))\n",
    "        #print('mean swarm best diff: %.2f'% (np.mean(self.mean_swarm_best_difference)))\n",
    "        #print('q: ', np.mean(q))\n",
    "        #print('r: ', np.mean(r))\n",
    "        avg_speed_before = np.mean(self.speeds_before)\n",
    "        avg_speed_after = np.mean(self.speeds_after)\n",
    "        self.avg_speed_before_history.append(avg_speed_before)\n",
    "        self.avg_speed_after_history.append(avg_speed_after)\n",
    "        \n",
    "        if speed_check:\n",
    "            print('Average speed of the particles before normalization is: ', avg_speed_before)\n",
    "            print('Average speed of the particles after normalization is: ', avg_speed_after)\n",
    "            f.write('Average speed of the particles before normalization is: %.2f' % (avg_speed_before))\n",
    "            f.write('Average speed of the particles after normalization is: %.2f' % (avg_speed_after))\n",
    "            f.flush()\n",
    "            \n",
    "            \n",
    "    def set_weights(self, weightList):\n",
    "        \n",
    "        weightMatrixList = weightList[0]\n",
    "        biasList = weightList[1]\n",
    "        for i in range(len(weightMatrixList)):\n",
    "            self.model.layers[i].set_weights([weightMatrixList[i], biasList[i]])\n",
    "            \n",
    "        \n",
    "class PSO_Particle(PSO_Swarm):\n",
    "        \n",
    "    def __init__(self, parent):\n",
    "        \n",
    "        self.parent = parent\n",
    "            \n",
    "        r1 = np.random.uniform(size=(self.parent.nr_variables))\n",
    "        r2 = np.random.uniform(size=(self.parent.nr_variables))\n",
    "\n",
    "        self.position = self.parent.pso_param_dict['xMin'] + r1 * (self.parent.pso_param_dict['xMax'] - \n",
    "                                self.parent.pso_param_dict['xMin'])\n",
    "        self.velocity = self.parent.pso_param_dict['alpha']/self.parent.pso_param_dict['deltaT'] * \\\n",
    "                        ((self.parent.pso_param_dict['xMin'] - self.parent.pso_param_dict['xMax'])/2 + r2 * \n",
    "                         (self.parent.pso_param_dict['xMax'] - self.parent.pso_param_dict['xMin']))\n",
    "        \n",
    "        self.best_score = 1e20\n",
    "        self.best_position = self.position\n",
    "        \n",
    "        \n",
    "        \n",
    "    def evaluate_particle(self, x_data, y_data):\n",
    "        \n",
    "        weightList = self.get_weights()\n",
    "        self.parent.set_weights(weightList)\n",
    "        \n",
    "        score = self.parent.model.evaluate(x_data, y_data, verbose=0)\n",
    "        if score[0] < self.best_score:\n",
    "            self.best_score = score[0]\n",
    "            self.best_position = self.position\n",
    "            \n",
    "        return score[0]\n",
    "        \n",
    "    def get_weights(self): # sets the weights from the current pos in parameter space\n",
    "        \n",
    "        weightMatrixList = [] # will contain a list of all the weight matrices \n",
    "        biasList = []   # will contain a list of all the biases\n",
    "\n",
    "        weightCounter = 0 # to help assign weights and biases to their correct matrix\n",
    "\n",
    "        ### Extract weight matrices\n",
    "        input_dim = len(self.parent.input_features)\n",
    "        output_dim = len(self.parent.output_features)\n",
    "        weightMatrix = np.zeros((input_dim, self.parent.nr_neurons_per_lay)) \n",
    "        for i in range(input_dim):  \n",
    "            weightMatrix[i,:] = self.position[weightCounter:weightCounter+self.parent.nr_neurons_per_lay]\n",
    "            weightCounter += self.parent.nr_neurons_per_lay\n",
    "        weightMatrixList.append(weightMatrix)\n",
    "\n",
    "        \n",
    "        for iLayer in range(self.parent.nr_hidden_layers-1):\n",
    "            weightMatrix = np.zeros((self.parent.nr_neurons_per_lay, self.parent.nr_neurons_per_lay))\n",
    "            for iNeuron in range(self.parent.nr_neurons_per_lay):\n",
    "\n",
    "                weightMatrix[iNeuron,:] = self.position[weightCounter:weightCounter+self.parent.nr_neurons_per_lay]\n",
    "                weightCounter += self.parent.nr_neurons_per_lay\n",
    "\n",
    "            weightMatrixList.append(weightMatrix)\n",
    "\n",
    "        weightMatrix = np.zeros((self.parent.nr_neurons_per_lay, output_dim))\n",
    "        for i in range(self.parent.nr_neurons_per_lay):  \n",
    "            weightMatrix[i,:] = self.position[weightCounter:weightCounter+output_dim]\n",
    "            weightCounter += output_dim\n",
    "\n",
    "        weightMatrixList.append(weightMatrix)\n",
    "\n",
    "        ### Extract bias vectors\n",
    "        for iLayer in range(self.parent.nr_hidden_layers):\n",
    "\n",
    "            biasVector = self.position[weightCounter:weightCounter+self.parent.nr_neurons_per_lay]\n",
    "            weightCounter += self.parent.nr_neurons_per_lay\n",
    "\n",
    "            biasList.append(biasVector)\n",
    "\n",
    "        biasVector = np.zeros(output_dim)\n",
    "        biasVector = self.position[weightCounter:weightCounter+output_dim] # for the output layer\n",
    "        biasList.append(biasVector)\n",
    "\n",
    "        weightCounter += output_dim\n",
    "        \n",
    "        weightList = [weightMatrixList, biasList]\n",
    "\n",
    "        #print(weightCounter == len(self.position))  # a check if the number of variables is correct\n",
    "        \n",
    "        return weightList\n",
    "\n",
    "    def update_particle(self):\n",
    "\n",
    "        q = np.random.uniform()#size = self.parent.nr_variables)\n",
    "        r = np.random.uniform()#size = self.parent.nr_variables)\n",
    "        #print(q)\n",
    "        #print(r)\n",
    "        particle_best_difference = self.best_position - self.position\n",
    "        swarm_best_difference = self.parent.swarm_best_position - self.position\n",
    "        \n",
    "        #self.parent.mean_particle_best_difference.append(np.mean(np.abs(particle_best_difference)))\n",
    "        #self.parent.mean_swarm_best_difference.append(np.mean(np.abs(swarm_best_difference)))\n",
    "\n",
    "        self.velocity = self.parent.inertia_weight * self.velocity + self.parent.pso_param_dict['c1'] * q * \\\n",
    "                        particle_best_difference / self.parent.pso_param_dict['deltaT'] + \\\n",
    "                        self.parent.pso_param_dict['c2'] * r * swarm_best_difference / \\\n",
    "                        self.parent.pso_param_dict['deltaT']\n",
    "                    \n",
    "        #self.parent.term_one.append(np.mean(np.abs(self.parent.pso_param_dict['c1'] * q * \\\n",
    "        #                particle_best_difference / self.parent.pso_param_dict['deltaT'])))\n",
    "        #self.parent.term_two.append(np.mean(np.abs(self.parent.pso_param_dict['c2'] * r * swarm_best_difference / \\\n",
    "        #                self.parent.pso_param_dict['deltaT'])))\n",
    "\n",
    "        # now limit velocity to vMax\n",
    "        absolute_velocity_before_normalization = np.sqrt(np.sum(np.power(self.velocity, 2)))\n",
    "        is_too_fast = absolute_velocity_before_normalization > self.parent.vMax\n",
    "        if is_too_fast:\n",
    "            #self.parent.too_fast_count += 1\n",
    "            self.velocity = self.velocity * self.parent.vMax / absolute_velocity_before_normalization\n",
    "            \n",
    "        absolute_velocity_after_normalization = np.sqrt(np.sum(np.power(self.velocity, 2)))\n",
    "\n",
    "        self.parent.speeds_before.append(absolute_velocity_before_normalization)\n",
    "        self.parent.speeds_after.append(absolute_velocity_after_normalization)\n",
    "            \n",
    "        self.position = self.position + self.velocity * self.parent.pso_param_dict['deltaT']\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
