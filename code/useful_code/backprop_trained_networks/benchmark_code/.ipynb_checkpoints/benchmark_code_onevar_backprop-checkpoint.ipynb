{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### General parameters\n",
    "run_on_cpu = False\n",
    "nEpochs = [1e2]\n",
    "batch_size = [1e4] # when this is not the tested variable\n",
    "train_set_size = [3e5] # how many examples will be used for training+validation+testing\n",
    "input_features = ['Halo_mass', 'Halo_mass_peak','Concentration', 'Halo_spin']\n",
    "output_features = ['Stellar_mass']\n",
    "\n",
    "tested_param = 'neurons_per_layer'     #batch_size, nLayers, actFun or neurons_per_layer\n",
    "tested_values = [10, 50, 100, 500, 1000]\n",
    "\n",
    "### Network parameters\n",
    "nLayers = [10] # when this is not the tested variable\n",
    "activation_func = 'tanh' # when this is not the tested variable\n",
    "neurons_per_layer = [1000] # when this is not the tested variable\n",
    "\n",
    "data_dict = {'X_pos': 0, 'Y_pos': 1, 'Z_pos': 2, 'X_vel': 3, 'Y_vel': 4, 'Z_vel': 5, 'Halo_mass': 6, \n",
    "             'Stellar_mass': 7, 'SFR': 8, 'Intra_cluster_mass': 9, 'Halo_mass_peak': 10, 'Stellar_mass_obs': 11, \n",
    "             'SFR_obs': 12, 'Halo_radius': 13, 'Concentration': 14, 'Halo_spin': 15, 'Scale_peak_mass': 16, \n",
    "             'Scale_half_mass': 17, 'Scale_last_MajM': 18, 'Type': 19}\n",
    "unit_dict = {'X_pos': '', 'Y_pos': '', 'Z_pos': '', 'X_vel': '', 'Y_vel': '', \n",
    "             'Z_vel': '', 'Halo_mass': 'log($M_{G}/M_{S}$)', 'Stellar_mass': 'log($M_{G}/M_{S}$)', 'SFR': '', \n",
    "             'Intra_cluster_mass': '', 'Halo_mass_peak': 'log($M_{G}/M_{S}$)', \n",
    "             'Stellar_mass_obs': '', 'SFR_obs': '', 'Halo_radius': '', \n",
    "             'Concentration': '', 'Halo_spin': '', 'Scale_peak_mass': 'a', \n",
    "             'Scale_half_mass': 'a', 'Scale_last_MajM': 'a', 'Type': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if run_on_cpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "result_dir = 'results/'\n",
    "import datetime\n",
    "import codecs, json\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16\n"
     ]
    }
   ],
   "source": [
    "keras_info = json.load(open(home_dir + '/.keras/keras.json'))\n",
    "float_prec = keras_info['floatx']\n",
    "print(float_prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_info_dict = {\n",
    "    'On_CPU_only': run_on_cpu,\n",
    "    'number_of_epochs': nEpochs,\n",
    "    'training_set_size': train_set_size,\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'tested_parameter': tested_param,\n",
    "    'tested_values': tested_values,\n",
    "    'batch_size': batch_size,                                                                       \n",
    "    'nr_of_layers': nLayers,\n",
    "    'activation_function': activation_func,\n",
    "    'neurons_per_layer': neurons_per_layer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "galfile = pd.read_hdf('/scratch/data/galcats/P200/galaxies.Z01.h5')\n",
    "galaxies = galfile.as_matrix()\n",
    "gal_header = galfile.keys().tolist()\n",
    "\n",
    "### Remove data points with halo mass below 10.5\n",
    "galaxies = galaxies[galaxies[:,6] > 10.5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = galaxies.shape[0]\n",
    "train_indices = np.random.choice(n_data_points, int(train_set_size[0]), replace=False)\n",
    "\n",
    "x_train = np.zeros((len(train_indices), len(input_features)))\n",
    "\n",
    "y_train = np.zeros((len(train_indices), len(output_features)))\n",
    "\n",
    "\n",
    "for i in range(len(input_features)):\n",
    "    x_train[:,i] = galaxies[train_indices, data_dict[input_features[i]]]\n",
    "    \n",
    "for i in range(len(output_features)):\n",
    "    y_train[:,i] = galaxies[train_indices, data_dict[output_features[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bSize = batch_size[0]\n",
    "nLay = nLayers[0]\n",
    "neurPerLay = neurons_per_layer[0]\n",
    "actFun = activation_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_on_cpu:\n",
    "    pu_string = 'CPU'\n",
    "else:\n",
    "    pu_string = 'GPU'\n",
    "\n",
    "timing_array = np.zeros(len(tested_values))\n",
    "tot_nr_comb = np.size(timing_array)\n",
    "comb_tried = 0\n",
    "glob_start = time.time()\n",
    "date_string = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "with open(result_dir + pu_string + '_' + tested_param + '_' + date_string + '.txt', 'w+') as f:\n",
    "    \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on ' + pu_string + ' at ' + date_string_proper + '\\n')\n",
    "    f.write('Parameter checked is %s\\n\\n' % (tested_param))\n",
    "    f.flush()\n",
    "    \n",
    "    for i_value, value in enumerate(tested_values):\n",
    "        \n",
    "        if tested_param == 'batch_size': bSize = value\n",
    "        elif tested_param == 'nLayers': nLay = value\n",
    "        elif tested_param == 'neurons_per_layer': neurPerLay = value\n",
    "        elif tested_param == 'actfun': actFun = value\n",
    "        else: \n",
    "            print('error')\n",
    "\n",
    "        comb_tried += 1\n",
    "\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(neurPerLay, input_dim = len(input_features), activation = actFun))\n",
    "\n",
    "        for i in range(0, nLay):\n",
    "            model.add(Dense(neurPerLay, activation = actFun))\n",
    "\n",
    "        model.add(Dense(len(output_features), activation = None))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "        start = time.time()\n",
    "        # Fit the model\n",
    "        history = model.fit(x_train , y_train, epochs=int(nEpochs[0]), \n",
    "                batch_size=int(bSize), verbose=0)\n",
    "        end = time.time()\n",
    "\n",
    "        timing_array[i_value] = (end - start) / 60\n",
    "\n",
    "        progress_end = time.time()\n",
    "        elapsed_so_far = (progress_end - glob_start) / 60\n",
    "        time_remaining = elapsed_so_far / comb_tried * (tot_nr_comb - comb_tried)\n",
    "\n",
    "        f.write('%s      ' % (datetime.datetime.now().strftime(\"%H:%M:%S\")))\n",
    "        f.write('Combinations tried: %d/%d     ' % (comb_tried, tot_nr_comb))\n",
    "        f.write('Elapsed time: %dmin     ' % (elapsed_so_far))\n",
    "        f.write('Time remaining: %dmin.\\n' % (time_remaining))\n",
    "        f.flush()\n",
    "\n",
    "                    \n",
    "f.close()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = timing_array.tolist()\n",
    "json_data = [json_data]\n",
    "json_data.append(json_info_dict)\n",
    "\n",
    "with open(result_dir + pu_string + '_' + tested_param + '_' + float_prec + '_' + date_string + '.json', 'w+') as f:\n",
    "    json.dump(json_data, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
