{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "module_path = home_dir + '/modules/'\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "import time\n",
    "import importlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "import model_management\n",
    "from scipy.special import comb\n",
    "import datetime\n",
    "import codecs, json\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport environmental_density\n",
    "from environmental_density import get_density_periodic\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = 1\n",
    "fig_dir = 'figures/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_on_cpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f2d43936bfca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mrun_on_cpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_on_cpu' is not defined"
     ]
    }
   ],
   "source": [
    "if run_on_cpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_galfile(galfile_directory='/home/magnus/code/useful_code/special_functions/test_galcat_w_log_densities_3e5.h5'):\n",
    "    # '/scratch/data/galcats/P200/galaxies.Z01.h5'\n",
    "    galfile = pd.read_hdf(galfile_directory)\n",
    "    galaxies = galfile.as_matrix()\n",
    "    gal_header = galfile.keys().tolist()\n",
    "\n",
    "    ### Remove data points with halo mass below 10.5\n",
    "    galaxies = galaxies[galaxies[:,6] > 10.5, :]\n",
    "    \n",
    "    data_keys = {'X_pos': 0, 'Y_pos': 1, 'Z_pos': 2, 'X_vel': 3, 'Y_vel': 4, 'Z_vel': 5, 'Halo_mass': 6, \n",
    "             'Stellar_mass': 7, 'SFR': 8, 'Intra_cluster_mass': 9, 'Halo_mass_peak': 10, 'Stellar_mass_obs': 11, \n",
    "             'SFR_obs': 12, 'Halo_radius': 13, 'Concentration': 14, 'Halo_spin': 15, 'Scale_peak_mass': 16, \n",
    "             'Scale_half_mass': 17, 'Scale_last_MajM': 18, 'Type': 19, 'Environmental_density': 20}\n",
    "    unit_dict = {'X_pos': '', 'Y_pos': '', 'Z_pos': '', 'X_vel': '', 'Y_vel': '', \n",
    "             'Z_vel': '', 'Halo_mass': 'log($M_{G}/M_{S}$)', 'Stellar_mass': 'log($M_{G}/M_{S}$)', 'SFR': '', \n",
    "             'Intra_cluster_mass': '', 'Halo_mass_peak': 'log($M_{G}/M_{S}$)', \n",
    "             'Stellar_mass_obs': '', 'SFR_obs': '', 'Halo_radius': '', \n",
    "             'Concentration': '', 'Halo_spin': '', 'Scale_peak_mass': 'a', \n",
    "             'Scale_half_mass': 'a', 'Scale_last_MajM': 'a', 'Type': '', \n",
    "             'Environmental_density': 'log($M_{G}/M_{S}/Mpc^3$)'}\n",
    "    \n",
    "    return galaxies, data_keys, unit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_train_data(galaxies, data_keys, input_features, output_features, total_set_size, train_size, test_size):\n",
    "    \n",
    "    n_data_points = galaxies.shape[0]\n",
    "    subset_indices = np.random.choice(n_data_points, total_set_size, replace=False)\n",
    "    train_indices = subset_indices[: int(train_size)]\n",
    "    val_indices = subset_indices[int(train_size) : int(train_size+val_size)]\n",
    "    test_indices = subset_indices[int(train_size+val_size) :]\n",
    "\n",
    "    x_train = np.zeros((len(train_indices), len(input_features)))\n",
    "    x_val = np.zeros((len(val_indices), len(input_features)))\n",
    "    x_test = np.zeros((len(test_indices), len(input_features)))\n",
    "    y_train = np.zeros((len(train_indices), len(output_features)))\n",
    "    y_val = np.zeros((len(val_indices), len(output_features)))\n",
    "    y_test = np.zeros((len(test_indices), len(output_features)))\n",
    "\n",
    "    for i in range(len(input_features)):\n",
    "        x_train[:,i] = galaxies[train_indices, data_keys[input_features[i]]]\n",
    "        x_val[:,i] = galaxies[val_indices, data_keys[input_features[i]]]\n",
    "        x_test[:,i] = galaxies[test_indices, data_keys[input_features[i]]]\n",
    "\n",
    "    for i in range(len(output_features)):\n",
    "        y_train[:,i] = galaxies[train_indices, data_keys[output_features[i]]]\n",
    "        y_val[:,i] = galaxies[val_indices, data_keys[output_features[i]]]\n",
    "        y_test[:,i] = galaxies[test_indices, data_keys[output_features[i]]]\n",
    "        \n",
    "    training_data_dict = {\n",
    "        'x_train': x_train,\n",
    "        'x_val': x_val,\n",
    "        'x_test': x_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test        \n",
    "    }\n",
    "    \n",
    "    return training_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(training_data_dict, norm):\n",
    "    \n",
    "    x_train = training_data_dict['x_train']\n",
    "    x_val = training_data_dict['x_val']\n",
    "    x_test = training_data_dict['x_test']\n",
    "    y_train = training_data_dict['y_train']\n",
    "    y_val = training_data_dict['y_val']\n",
    "    y_test = training_data_dict['y_test']\n",
    "    \n",
    "    if norm == 'none':\n",
    "        \n",
    "        training_data_dict['norm'] = norm\n",
    "\n",
    "    elif norm == 'zero_mean_unit_std':\n",
    "\n",
    "        for i in range(np.size(x_train, 1)):\n",
    "            x_data_means = np.mean(x_train, 0)\n",
    "            x_data_stds = np.std(x_train, 0)\n",
    "\n",
    "            x_train_norm = (x_train - x_data_means) / x_data_stds\n",
    "            x_val_norm = (x_val - x_data_means) / x_data_stds\n",
    "            x_test_norm = (x_test - x_data_means) / x_data_stds\n",
    "\n",
    "        for i in range(np.size(y_train, 1)):\n",
    "            y_data_means = np.mean(y_train, 0)\n",
    "            y_data_stds = np.std(y_train, 0)\n",
    "\n",
    "            y_train_norm = (y_train - y_data_means) / y_data_stds\n",
    "            y_val_norm = (y_val - y_data_means) / y_data_stds\n",
    "            y_test_norm = (y_test - y_data_means) / y_data_stds\n",
    "            \n",
    "        training_data_dict['norm'] = norm\n",
    "            \n",
    "        training_data_dict['x_data_means'] = x_data_means\n",
    "        training_data_dict['x_data_stds'] = x_data_stds\n",
    "        training_data_dict['y_data_means'] = y_data_means\n",
    "        training_data_dict['y_data_stds'] = y_data_stds\n",
    "        \n",
    "        training_data_dict['x_train_norm'] = x_train_norm\n",
    "        training_data_dict['x_val_norm'] = x_val_norm\n",
    "        training_data_dict['x_test_norm'] = x_test_norm\n",
    "        training_data_dict['y_train_norm'] = y_train_norm\n",
    "        training_data_dict['y_val_norm'] = y_val_norm\n",
    "        training_data_dict['y_test_norm'] = y_test_norm\n",
    "        \n",
    "\n",
    "\n",
    "    elif norm == 'zero_to_one':\n",
    "\n",
    "        for i in range(np.size(x_train, 1)):\n",
    "            x_data_max = np.max(x_train, 0)\n",
    "            x_data_min = np.min(x_train, 0)\n",
    "\n",
    "            x_train_norm = (x_train - x_data_min) / (x_data_max - x_data_min)\n",
    "            x_val_norm = (x_val - x_data_min) / (x_data_max - x_data_min)\n",
    "            x_test_norm = (x_test - x_data_min) / (x_data_max - x_data_min)\n",
    "\n",
    "        for i in range(np.size(y_train, 1)):\n",
    "            y_data_max = np.max(y_train, 0)\n",
    "            y_data_min = np.min(y_train, 0)\n",
    "\n",
    "            y_train_norm = (y_train - y_data_min) / (y_data_max - y_data_min)\n",
    "            y_val_norm = (y_val - y_data_min) / (y_data_max - y_data_min)\n",
    "            y_test_norm = (y_test - y_data_min) / (y_data_max - y_data_min)\n",
    "            \n",
    "        training_data_dict['norm'] = norm\n",
    "            \n",
    "        training_data_dict['x_data_max'] = x_data_max\n",
    "        training_data_dict['x_data_min'] = x_data_min\n",
    "        training_data_dict['y_data_max'] = y_data_max\n",
    "        training_data_dict['y_data_min'] = y_data_min\n",
    "        \n",
    "        training_data_dict['x_train_norm'] = x_train_norm\n",
    "        training_data_dict['x_val_norm'] = x_val_norm\n",
    "        training_data_dict['x_test_norm'] = x_test_norm\n",
    "        training_data_dict['y_train_norm'] = y_train_norm\n",
    "        training_data_dict['y_val_norm'] = y_val_norm\n",
    "        training_data_dict['y_test_norm'] = y_test_norm\n",
    "       \n",
    "    else:\n",
    "        print('Incorrect norm provided: ', norm)    \n",
    "        \n",
    "    \n",
    "    \n",
    "    return training_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameter string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nLayers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-445094939672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Set name ending with parameters for figures to be saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m param_string = 'nLayers_%d_nNeurons_%d_actFun_%s_lossFunc_%s_nTrainSamples_%d_nEpochs_%d_batchSize_%d' % (\n\u001b[0;32m----> 3\u001b[0;31m     nLayers, neuronsPerLayer, activationFunction, loss_function, train_size, nEpochs, batchSize)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nLayers' is not defined"
     ]
    }
   ],
   "source": [
    "### Set name ending with parameters for figures to be saved\n",
    "param_string = 'nLayers_%d_nNeurons_%d_actFun_%s_lossFunc_%s_nTrainSamples_%d_nEpochs_%d_batchSize_%d' % (\n",
    "    nLayers, neuronsPerLayer, activationFunction, loss_function, train_size, nEpochs, batchSize)\n",
    "print(param_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y_test_norm, 0))\n",
    "print(np.std(y_test_norm, 0))\n",
    "print(np.min(x_test_norm, 0))\n",
    "print(np.max(x_test_norm, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a feel for the data\n",
    "for i in range(len(input_features)):\n",
    "    print(input_features[i],': min: %.2e, max: %.2e.' % (np.min(x_train[:,i]), np.max(x_train[:,i])))\n",
    "for i in range(len(output_features)):\n",
    "    print(output_features[i],': min: %.2e, max: %.2e.' % (np.min(y_train[:,i]), np.max(y_train[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def weighted_mse_1(y_true, y_pred):\n",
    "    \n",
    "    return K.mean(K.log(y_true+1.5) + K.square(y_pred - y_true), axis=-1)\n",
    "loss_func_dict = {\n",
    "    'mse': 'mse',\n",
    "    'mae': 'mae',\n",
    "    'weighted_mse_1': weighted_mse_1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualisation for when we have 2 input features\n",
    "%matplotlib notebook\n",
    "input_feat_1 = 0\n",
    "input_feat_2 = 1\n",
    "output_feat = 1\n",
    "\n",
    "fig = plt.figure(1, figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train_norm[:500,input_feat_1], x_train_norm[:500,input_feat_2], \n",
    "           y_train_norm[:500,output_feat])\n",
    "ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_1]))\n",
    "ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_2]))\n",
    "ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[output_feat]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search for the model that you want\n",
    "importlib.reload(model_management)\n",
    "search_dict = {\n",
    "    'training_method': 'backprop'\n",
    "}\n",
    "[model_dicts, description_dicts] = model_management.SearchModel(search_dict, get_hits=True)\n",
    "print(description_dicts)\n",
    "print('\\n')\n",
    "for key in model_dicts:\n",
    "    print(key)\n",
    "    print(model_dicts[key])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model_management)\n",
    "model, model_dict, description = model_management.LoadModel(search_dict, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "nr_steps = 1e4\n",
    "batch_size = 4e4\n",
    "total_set_size = 300000 # how many examples will be used for training+validation+testing\n",
    "train_size = 280000\n",
    "val_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "norm = 'zero_mean_unit_std' # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "\n",
    "input_features = ['Halo_mass', 'Scale_half_mass']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "\n",
    "### Network parameters\n",
    "nLayers = 10\n",
    "activationFunction = 'tanh'\n",
    "neuronsPerLayer = 10\n",
    "loss_function = 'mse' # 'mse', 'weighted_mse_1' 'mae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the selected galaxyfile\n",
    "galaxies, data_keys, unit_dict = load_galfile()\n",
    "    \n",
    "# prepare the training data\n",
    "training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, \n",
    "                                       total_set_size, train_size, test_size)\n",
    "training_data_dict = normalise_data(training_data_dict, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280000 samples, validate on 10000 samples\n",
      "Epoch 1/1428\n",
      "280000/280000 [==============================] - 4s 14us/step - loss: 1.2373 - mean_squared_error: 1.2373 - val_loss: 1.2305 - val_mean_squared_error: 1.2305\n",
      "Epoch 2/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 1.0682 - mean_squared_error: 1.0682 - val_loss: 1.0361 - val_mean_squared_error: 1.0361\n",
      "Epoch 3/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.8945 - mean_squared_error: 0.8945 - val_loss: 0.9082 - val_mean_squared_error: 0.9082\n",
      "Epoch 4/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.8041 - mean_squared_error: 0.8041 - val_loss: 0.8398 - val_mean_squared_error: 0.8398\n",
      "Epoch 5/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.7349 - mean_squared_error: 0.7349 - val_loss: 0.7930 - val_mean_squared_error: 0.7930\n",
      "Epoch 6/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6954 - mean_squared_error: 0.6954 - val_loss: 0.7534 - val_mean_squared_error: 0.7534\n",
      "Epoch 7/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6625 - mean_squared_error: 0.6625 - val_loss: 0.7373 - val_mean_squared_error: 0.7373\n",
      "Epoch 8/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6491 - mean_squared_error: 0.6491 - val_loss: 0.7261 - val_mean_squared_error: 0.7261\n",
      "Epoch 9/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6396 - mean_squared_error: 0.6396 - val_loss: 0.7202 - val_mean_squared_error: 0.7202\n",
      "Epoch 10/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6348 - mean_squared_error: 0.6348 - val_loss: 0.7163 - val_mean_squared_error: 0.7163\n",
      "Epoch 11/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6304 - mean_squared_error: 0.6304 - val_loss: 0.7129 - val_mean_squared_error: 0.7129\n",
      "Epoch 12/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6272 - mean_squared_error: 0.6272 - val_loss: 0.7090 - val_mean_squared_error: 0.7090\n",
      "Epoch 13/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6238 - mean_squared_error: 0.6238 - val_loss: 0.7080 - val_mean_squared_error: 0.7080\n",
      "Epoch 14/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6217 - mean_squared_error: 0.6217 - val_loss: 0.7046 - val_mean_squared_error: 0.7046\n",
      "Epoch 15/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6190 - mean_squared_error: 0.6190 - val_loss: 0.7026 - val_mean_squared_error: 0.7026\n",
      "Epoch 16/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6159 - mean_squared_error: 0.6159 - val_loss: 0.6992 - val_mean_squared_error: 0.6992\n",
      "Epoch 17/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.6151 - mean_squared_error: 0.6151 - val_loss: 0.7061 - val_mean_squared_error: 0.7061\n",
      "Epoch 18/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5997 - mean_squared_error: 0.5997 - val_loss: 0.6743 - val_mean_squared_error: 0.6743\n",
      "Epoch 19/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5861 - mean_squared_error: 0.5861 - val_loss: 0.6646 - val_mean_squared_error: 0.6646\n",
      "Epoch 20/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5765 - mean_squared_error: 0.5765 - val_loss: 0.6567 - val_mean_squared_error: 0.6567\n",
      "Epoch 21/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5684 - mean_squared_error: 0.5684 - val_loss: 0.6528 - val_mean_squared_error: 0.6528\n",
      "Epoch 22/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5641 - mean_squared_error: 0.5641 - val_loss: 0.6504 - val_mean_squared_error: 0.6504\n",
      "Epoch 23/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5609 - mean_squared_error: 0.5609 - val_loss: 0.6479 - val_mean_squared_error: 0.6479\n",
      "Epoch 24/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5582 - mean_squared_error: 0.5582 - val_loss: 0.6460 - val_mean_squared_error: 0.6460\n",
      "Epoch 25/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5559 - mean_squared_error: 0.5559 - val_loss: 0.6440 - val_mean_squared_error: 0.6440\n",
      "Epoch 26/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5536 - mean_squared_error: 0.5536 - val_loss: 0.6421 - val_mean_squared_error: 0.6421\n",
      "Epoch 27/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 0.6401 - val_mean_squared_error: 0.6401\n",
      "Epoch 28/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5511 - mean_squared_error: 0.5511 - val_loss: 0.6382 - val_mean_squared_error: 0.6382\n",
      "Epoch 29/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5492 - mean_squared_error: 0.5492 - val_loss: 0.6392 - val_mean_squared_error: 0.6392\n",
      "Epoch 30/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5483 - mean_squared_error: 0.5483 - val_loss: 0.6372 - val_mean_squared_error: 0.6372\n",
      "Epoch 31/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5465 - mean_squared_error: 0.5465 - val_loss: 0.6348 - val_mean_squared_error: 0.6348\n",
      "Epoch 32/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5444 - mean_squared_error: 0.5444 - val_loss: 0.6328 - val_mean_squared_error: 0.6328\n",
      "Epoch 33/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5436 - mean_squared_error: 0.5436 - val_loss: 0.6343 - val_mean_squared_error: 0.6343\n",
      "Epoch 34/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5434 - mean_squared_error: 0.5434 - val_loss: 0.6313 - val_mean_squared_error: 0.6313\n",
      "Epoch 35/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5406 - mean_squared_error: 0.5406 - val_loss: 0.6299 - val_mean_squared_error: 0.6299\n",
      "Epoch 36/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5400 - mean_squared_error: 0.5400 - val_loss: 0.6294 - val_mean_squared_error: 0.6294\n",
      "Epoch 37/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5391 - mean_squared_error: 0.5391 - val_loss: 0.6284 - val_mean_squared_error: 0.6284\n",
      "Epoch 38/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5381 - mean_squared_error: 0.5381 - val_loss: 0.6274 - val_mean_squared_error: 0.6274\n",
      "Epoch 39/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5375 - mean_squared_error: 0.5375 - val_loss: 0.6270 - val_mean_squared_error: 0.6270\n",
      "Epoch 40/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5372 - mean_squared_error: 0.5372 - val_loss: 0.6265 - val_mean_squared_error: 0.6265\n",
      "Epoch 41/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5362 - mean_squared_error: 0.5362 - val_loss: 0.6250 - val_mean_squared_error: 0.6250\n",
      "Epoch 42/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5353 - mean_squared_error: 0.5353 - val_loss: 0.6250 - val_mean_squared_error: 0.6250\n",
      "Epoch 43/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5349 - mean_squared_error: 0.5349 - val_loss: 0.6240 - val_mean_squared_error: 0.6240\n",
      "Epoch 44/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5343 - mean_squared_error: 0.5343 - val_loss: 0.6235 - val_mean_squared_error: 0.6235\n",
      "Epoch 45/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5338 - mean_squared_error: 0.5338 - val_loss: 0.6230 - val_mean_squared_error: 0.6230\n",
      "Epoch 46/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5334 - mean_squared_error: 0.5334 - val_loss: 0.6226 - val_mean_squared_error: 0.6226\n",
      "Epoch 47/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5329 - mean_squared_error: 0.5329 - val_loss: 0.6226 - val_mean_squared_error: 0.6226\n",
      "Epoch 48/1428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5327 - mean_squared_error: 0.5327 - val_loss: 0.6226 - val_mean_squared_error: 0.6226\n",
      "Epoch 49/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5324 - mean_squared_error: 0.5324 - val_loss: 0.6230 - val_mean_squared_error: 0.6230\n",
      "Epoch 50/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5322 - mean_squared_error: 0.5322 - val_loss: 0.6221 - val_mean_squared_error: 0.6221\n",
      "Epoch 51/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5316 - mean_squared_error: 0.5316 - val_loss: 0.6221 - val_mean_squared_error: 0.6221\n",
      "Epoch 52/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5315 - mean_squared_error: 0.5315 - val_loss: 0.6216 - val_mean_squared_error: 0.6216\n",
      "Epoch 53/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5314 - mean_squared_error: 0.5314 - val_loss: 0.6216 - val_mean_squared_error: 0.6216\n",
      "Epoch 54/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5313 - mean_squared_error: 0.5313 - val_loss: 0.6221 - val_mean_squared_error: 0.6221\n",
      "Epoch 55/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5311 - mean_squared_error: 0.5311 - val_loss: 0.6206 - val_mean_squared_error: 0.6206\n",
      "Epoch 56/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5307 - mean_squared_error: 0.5307 - val_loss: 0.6206 - val_mean_squared_error: 0.6206\n",
      "Epoch 57/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5302 - mean_squared_error: 0.5302 - val_loss: 0.6206 - val_mean_squared_error: 0.6206\n",
      "Epoch 58/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5302 - mean_squared_error: 0.5302 - val_loss: 0.6206 - val_mean_squared_error: 0.6206\n",
      "Epoch 59/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5299 - mean_squared_error: 0.5299 - val_loss: 0.6201 - val_mean_squared_error: 0.6201\n",
      "Epoch 60/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5296 - mean_squared_error: 0.5296 - val_loss: 0.6211 - val_mean_squared_error: 0.6211\n",
      "Epoch 61/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5299 - mean_squared_error: 0.5299 - val_loss: 0.6216 - val_mean_squared_error: 0.6216\n",
      "Epoch 62/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5299 - mean_squared_error: 0.5299 - val_loss: 0.6206 - val_mean_squared_error: 0.6206\n",
      "Epoch 63/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5293 - mean_squared_error: 0.5293 - val_loss: 0.6196 - val_mean_squared_error: 0.6196\n",
      "Epoch 64/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5293 - mean_squared_error: 0.5293 - val_loss: 0.6196 - val_mean_squared_error: 0.6196\n",
      "Epoch 65/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5294 - mean_squared_error: 0.5294 - val_loss: 0.6201 - val_mean_squared_error: 0.6201\n",
      "Epoch 66/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5291 - mean_squared_error: 0.5291 - val_loss: 0.6196 - val_mean_squared_error: 0.6196\n",
      "Epoch 67/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5286 - mean_squared_error: 0.5286 - val_loss: 0.6211 - val_mean_squared_error: 0.6211\n",
      "Epoch 68/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5293 - mean_squared_error: 0.5293 - val_loss: 0.6201 - val_mean_squared_error: 0.6201\n",
      "Epoch 69/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5284 - mean_squared_error: 0.5284 - val_loss: 0.6187 - val_mean_squared_error: 0.6187\n",
      "Epoch 70/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5281 - mean_squared_error: 0.5281 - val_loss: 0.6196 - val_mean_squared_error: 0.6196\n",
      "Epoch 71/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5287 - mean_squared_error: 0.5287 - val_loss: 0.6187 - val_mean_squared_error: 0.6187\n",
      "Epoch 72/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5283 - mean_squared_error: 0.5283 - val_loss: 0.6196 - val_mean_squared_error: 0.6196\n",
      "Epoch 73/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5284 - mean_squared_error: 0.5284 - val_loss: 0.6187 - val_mean_squared_error: 0.6187\n",
      "Epoch 74/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5275 - mean_squared_error: 0.5275 - val_loss: 0.6191 - val_mean_squared_error: 0.6191\n",
      "Epoch 75/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5278 - mean_squared_error: 0.5278 - val_loss: 0.6187 - val_mean_squared_error: 0.6187\n",
      "Epoch 76/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5271 - mean_squared_error: 0.5271 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 77/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 78/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6187 - val_mean_squared_error: 0.6187\n",
      "Epoch 79/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 80/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 81/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 82/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6182 - val_mean_squared_error: 0.6182\n",
      "Epoch 83/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 84/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 85/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 86/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6187 - val_mean_squared_error: 0.6187\n",
      "Epoch 87/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5266 - mean_squared_error: 0.5266 - val_loss: 0.6177 - val_mean_squared_error: 0.6177\n",
      "Epoch 88/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5268 - mean_squared_error: 0.5268 - val_loss: 0.6187 - val_mean_squared_error: 0.6187\n",
      "Epoch 89/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6172 - val_mean_squared_error: 0.6172\n",
      "Epoch 90/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6172 - val_mean_squared_error: 0.6172\n",
      "Epoch 91/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6167 - val_mean_squared_error: 0.6167\n",
      "Epoch 92/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5249 - mean_squared_error: 0.5249 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 93/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5247 - mean_squared_error: 0.5247 - val_loss: 0.6167 - val_mean_squared_error: 0.6167\n",
      "Epoch 94/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5248 - mean_squared_error: 0.5248 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 95/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5245 - mean_squared_error: 0.5245 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5244 - mean_squared_error: 0.5244 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 97/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5249 - mean_squared_error: 0.5249 - val_loss: 0.6172 - val_mean_squared_error: 0.6172\n",
      "Epoch 98/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5247 - mean_squared_error: 0.5247 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 99/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5243 - mean_squared_error: 0.5243 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 100/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5245 - mean_squared_error: 0.5245 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 101/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5247 - mean_squared_error: 0.5247 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 102/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5243 - mean_squared_error: 0.5243 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 103/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5243 - mean_squared_error: 0.5243 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 104/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5245 - mean_squared_error: 0.5245 - val_loss: 0.6167 - val_mean_squared_error: 0.6167\n",
      "Epoch 105/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5248 - mean_squared_error: 0.5248 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 106/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5243 - mean_squared_error: 0.5243 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 107/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5241 - mean_squared_error: 0.5241 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 108/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5240 - mean_squared_error: 0.5240 - val_loss: 0.6162 - val_mean_squared_error: 0.6162\n",
      "Epoch 109/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5236 - mean_squared_error: 0.5236 - val_loss: 0.6157 - val_mean_squared_error: 0.6157\n",
      "Epoch 110/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5234 - mean_squared_error: 0.5234 - val_loss: 0.6157 - val_mean_squared_error: 0.6157\n",
      "Epoch 111/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5238 - mean_squared_error: 0.5238 - val_loss: 0.6157 - val_mean_squared_error: 0.6157\n",
      "Epoch 112/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5236 - mean_squared_error: 0.5236 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 113/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5232 - mean_squared_error: 0.5232 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 114/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5237 - mean_squared_error: 0.5237 - val_loss: 0.6157 - val_mean_squared_error: 0.6157\n",
      "Epoch 115/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5237 - mean_squared_error: 0.5237 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 116/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5236 - mean_squared_error: 0.5236 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 117/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5233 - mean_squared_error: 0.5233 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 118/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5229 - mean_squared_error: 0.5229 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 119/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5231 - mean_squared_error: 0.5231 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 120/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5235 - mean_squared_error: 0.5235 - val_loss: 0.6157 - val_mean_squared_error: 0.6157\n",
      "Epoch 121/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5231 - mean_squared_error: 0.5231 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 122/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5228 - mean_squared_error: 0.5228 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 123/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5233 - mean_squared_error: 0.5233 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 124/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5235 - mean_squared_error: 0.5235 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 125/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5227 - mean_squared_error: 0.5227 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 126/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5229 - mean_squared_error: 0.5229 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 127/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5227 - mean_squared_error: 0.5227 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 128/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5228 - mean_squared_error: 0.5228 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 129/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5230 - mean_squared_error: 0.5230 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 130/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5229 - mean_squared_error: 0.5229 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 131/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5230 - mean_squared_error: 0.5230 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 132/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5228 - mean_squared_error: 0.5228 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 133/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5224 - mean_squared_error: 0.5224 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 134/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5224 - mean_squared_error: 0.5224 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 135/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5222 - mean_squared_error: 0.5222 - val_loss: 0.6143 - val_mean_squared_error: 0.6143\n",
      "Epoch 136/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5222 - mean_squared_error: 0.5222 - val_loss: 0.6143 - val_mean_squared_error: 0.6143\n",
      "Epoch 137/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5222 - mean_squared_error: 0.5222 - val_loss: 0.6143 - val_mean_squared_error: 0.6143\n",
      "Epoch 138/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5221 - mean_squared_error: 0.5221 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 139/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5226 - mean_squared_error: 0.5226 - val_loss: 0.6143 - val_mean_squared_error: 0.6143\n",
      "Epoch 140/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5226 - mean_squared_error: 0.5226 - val_loss: 0.6143 - val_mean_squared_error: 0.6143\n",
      "Epoch 141/1428\n",
      "280000/280000 [==============================] - 0s 1us/step - loss: 0.5225 - mean_squared_error: 0.5225 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
      "Epoch 142/1428\n",
      "200000/280000 [====================>.........] - ETA: 0s - loss: 0.5086 - mean_squared_error: 0.5086"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c8d3c167ca65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     history = model.fit(training_data_dict['x_train_norm'] , training_data_dict['y_train_norm'], \n\u001b[1;32m     20\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_val_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_val_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                         epochs=int(nr_epochs), batch_size=int(batch_size))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                         raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(neuronsPerLayer, input_dim = len(input_features), activation = activationFunction))\n",
    "\n",
    "for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "    model.add(Dense(neuronsPerLayer, activation = activationFunction))\n",
    "\n",
    "model.add(Dense(len(output_features), activation = 'tanh'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=loss_func_dict[loss_function], metrics=['mse'], optimizer='adam')#, metrics=[loss_function])\n",
    "\n",
    "# Fit the model\n",
    "if training_data_dict['norm'] == 'none':\n",
    "    history = model.fit(training_data_dict['x_train'], training_data_dict['y_train'], \n",
    "                        validation_data=(training_data_dict['x_val'], training_data_dict['y_val']), \n",
    "                        epochs=int(nr_epochs), batch_size=int(batch_size))\n",
    "else:\n",
    "    history = model.fit(training_data_dict['x_train_norm'] , training_data_dict['y_train_norm'], \n",
    "                        validation_data=(training_data_dict['x_val_norm'], training_data_dict['y_val_norm']), \n",
    "                        epochs=int(nr_epochs), batch_size=int(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do a batch run to see which input parameters gives the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_input_features = ['Halo_mass']\n",
    "tested_input_features = ['Halo_mass_peak', 'Concentration', 'Type', 'Scale_peak_mass', 'Scale_half_mass', \n",
    "                 'Scale_last_MajM', 'Environmental_density']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "nr_extra_params_list = [1, 2, 3]\n",
    "nr_runs_per_comb = 10\n",
    "\n",
    "nr_steps = 1e4\n",
    "batch_size = 4e4\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "parameter_dictionary = {\n",
    "    'fixed_input_features': core_input_features,\n",
    "    'tested_input_features': tested_input_features,\n",
    "    'output_features': output_features,\n",
    "    'nr_extra_parameter_combinations': nr_extra_params_list,\n",
    "    'nr_steps': [nr_steps],\n",
    "    'batch_size': [batch_size],\n",
    "    'nr_epochs': [nr_epochs],\n",
    "    'nr_training_samples': [train_size],\n",
    "    'nr_validation_samples': [val_size],\n",
    "    'nr_test_samples': [test_size],\n",
    "    'data_normalization': norm,\n",
    "    'activation_function': activationFunction,\n",
    "    'neurons_per_layer': [neuronsPerLayer],\n",
    "    'nr_hidden_layers': [nLayers],\n",
    "    'description': 'Each parameter setting is represented by one list containing three objects. The first one is ' + \\\n",
    "    'the input parameters. The second one is the mse test scores obtained for the different runs evaluated on the ' +\\\n",
    "    'original units of the data set. The third one is the loss histories for the different runs [training_loss, ' +\\\n",
    "    'validation_loss].'\n",
    "}\n",
    "results_list = [parameter_dictionary]\n",
    "nr_combs_total = 0\n",
    "for nr_extra_params in nr_extra_params_list:\n",
    "    nr_combs_total += comb(len(tested_input_features), nr_extra_params)\n",
    "\n",
    "with open('model_comparisons/progress.txt', 'w+') as f:\n",
    "    \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on input parameters at ' + date_string_proper + '\\n\\n')\n",
    "    f.flush()\n",
    "    \n",
    "    # load the selected galaxyfile\n",
    "    galaxies, data_keys, unit_dict = load_galfile()\n",
    "    \n",
    "    for i_nr_extra_params, nr_extra_params in enumerate(nr_extra_params_list):\n",
    "        \n",
    "        extra_param_combs = list(combinations(tested_input_features, nr_extra_params))\n",
    "        \n",
    "        date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "        f.write(date_string_proper + '    Testing %d extra parameters. %d/%d extra parameter count tested. \\n\\n' %\n",
    "                (nr_extra_params, i_nr_extra_params+1, len(nr_extra_params_list)))\n",
    "        f.flush()\n",
    "    \n",
    "        for i_comb, param_comb in enumerate(extra_param_combs):\n",
    "            input_features = core_input_features.copy()\n",
    "            input_features.extend(param_comb)\n",
    "            \n",
    "            # prepare the training data\n",
    "            training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, \n",
    "                                                   total_set_size, train_size, test_size)\n",
    "            training_data_dict = normalise_data(training_data_dict, norm)\n",
    "            \n",
    "            date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "            f.write(date_string_proper + '        Testing combination %d/%d. \\n\\n' % (i_comb+1, nr_combs_total))\n",
    "            f.flush()\n",
    "            \n",
    "            scores = []\n",
    "            histories = []\n",
    "\n",
    "            for i_run in range(nr_runs_per_comb):\n",
    "\n",
    "                # create model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(neuronsPerLayer, input_dim = len(input_features), activation = activationFunction))\n",
    "\n",
    "                for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "                    model.add(Dense(neuronsPerLayer, activation = activationFunction))\n",
    "\n",
    "                model.add(Dense(len(output_features), activation = activationFunction))\n",
    "\n",
    "                # Compile model\n",
    "                model.compile(loss=loss_func_dict[loss_function], metrics=['mse'], optimizer='adam')\n",
    "\n",
    "                # Fit the model\n",
    "                if norm == 'none':\n",
    "                    history = model.fit(x_train_norm , y_train_norm, validation_data=(x_val_norm, y_val_norm), \n",
    "                                        epochs=int(nr_epochs), batch_size=int(batch_size), verbose=0)\n",
    "                else:\n",
    "                    history = model.fit(x_train , y_train, validation_data=(x_val, y_val), \n",
    "                                        epochs=int(nr_epochs), batch_size=int(batch_size), verbose=0)\n",
    "\n",
    "                # Evaluate the model on test data\n",
    "                if norm == 'zero_mean_unit_std_norm':\n",
    "                    predicted_norm_points = model.predict(x_test_norm)\n",
    "                    predicted_points = predicted_norm_points * y_data_stds + y_data_means\n",
    "\n",
    "                elif norm == 'zero_to_one_norm':\n",
    "                    predicted_norm_points = model.predict(x_test_norm)\n",
    "                    predicted_points = predicted_norm_points * (y_data_max - y_data_min) + y_data_min\n",
    "                    \n",
    "                elif norm == 'none':\n",
    "                    predicted_points = model.predict(x_test)\n",
    "                    \n",
    "                else:\n",
    "                    print('Incorrect normalisation provided: ' + norm)\n",
    "                    \n",
    "                ### Get mse for the real predictions\n",
    "                n_points, n_outputs = np.shape(predicted_points)\n",
    "                x_minus_y = predicted_points - y_test\n",
    "\n",
    "                feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "                total_score = np.sum(feature_scores) / n_outputs\n",
    "                \n",
    "                histories.append([history.history['loss'], history.history['val_loss']])\n",
    "                scores.append(total_score)\n",
    "                \n",
    "            results_list.append([input_features, scores, histories])\n",
    "            \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark completed at ' + date_string_proper + '\\n')\n",
    "    f.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_list[1][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_nr = 5\n",
    "title = results_list[comb_nr][0]\n",
    "train_loss = results_list[comb_nr][2][0][0]\n",
    "val_loss = results_list[comb_nr][2][0][1]\n",
    "test_loss = results_list[comb_nr][1]\n",
    "#print(train_loss)\n",
    "print('Lowest train/val/test loss: %.2f, %.2f, %.2f' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "custom_string = 'aborted_long_run_may_10'\n",
    "with open('model_comparisons/' + custom_string + '.json', 'w+') as f:\n",
    "    json.dump(results_list, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a result\n",
    "loaded_list_string = '2018-05-09--17-15-18'\n",
    "with open('model_comparisons/' + loaded_list_string + '.json', 'r') as f:\n",
    "    loaded_list = json.load(f)\n",
    "f.close()\n",
    "print(loaded_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(train_loss, 'b')\n",
    "plt.plot(val_loss, 'r')\n",
    "plt.yscale('log')\n",
    "plt.title(title)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On preprocessed data\n",
    "test_loss, test_mse = model.evaluate(x_test_norm, y_test_norm, verbose=0)\n",
    "print('MSE for the processed data: %.4f' % (test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict real value of points\n",
    "if norm == 'zero_mean_unit_std_norm':\n",
    "    predicted_norm_points = model.predict(x_test_norm)\n",
    "    predicted_points = predicted_norm_points * y_data_stds + y_data_means\n",
    "    \n",
    "if norm == 'zero_to_one_norm':\n",
    "    predicted_norm_points = model.predict(x_test_norm)\n",
    "    predicted_points = predicted_norm_points * (y_data_max - y_data_min) + y_data_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get mse for the real predictions\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print('MSE for the unprocessed data: %.4f' % (total_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model if it is useful\n",
    "importlib.reload(model_management)\n",
    "model_dictionary = {\n",
    "    'training_method': 'backprop',\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'number_of_epochs': nEpochs,\n",
    "    'batch_size': batchSize,\n",
    "    'number_of_layers': nLayers,\n",
    "    'neurons_per_layer': neuronsPerLayer,\n",
    "    'activation_function': activationFunction,\n",
    "    'train_set_size': train_size,\n",
    "    'loss_function': loss_function,\n",
    "    'test_loss': test_loss,\n",
    "    'test_mse': test_mse,\n",
    "    'preprocess_data': preprocess_data\n",
    "}\n",
    "description = 'First network trained on preprocessed data.'\n",
    "model_management.SaveModel(model, model_dictionary, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#x1 = np.linspace(np.min(x_test[:,0]), np.max(x_test[:,0]), 30)\n",
    "#x2 = np.linspace(np.min(x_test[:,1]), np.max(x_test[:,1]), 30)\n",
    "#X1, X2 = np.meshgrid(x1, x2)\n",
    "#Z = np.zeros(X1.shape)\n",
    "#for i in range(30):\n",
    "#    for j in range(30):\n",
    "#        Z[i, j] = model.predict(np.array([X1[i,j], X2[i,j]])) TODO varför funkar inte det här??\n",
    "        \n",
    "#fig = plt.figure(4)\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "#ax.set_xlabel('x')\n",
    "#ax.set_ylabel('y')\n",
    "#ax.set_zlabel('z')\n",
    "        \n",
    "### Old visualisation way\n",
    "### Visualisation of prediction strength for when we have 2 input features\n",
    "if plot_threeD and len(input_features) == 2:\n",
    "    predictedY = model.predict(x_test_norm)\n",
    "    predictedY = predictedY * y_data_stds + y_data_means\n",
    "    fig = plt.figure(2, figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               y_test[:,0], s=3)\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               predictedY, s=3)\n",
    "    ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[0]))\n",
    "    ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[1]))\n",
    "    ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatterplots and boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from pylab import plot, show, savefig, xlim, figure, \\\n",
    "#                hold, ylim, legend, boxplot, setp, axes\n",
    "nBins = 8\n",
    "bin_edges = np.linspace(halo_min_mass, halo_max_mass, nBins+1)\n",
    "\n",
    "predictedY = predicted_points\n",
    "\n",
    "for i, feat in enumerate(output_features):\n",
    "    \n",
    "    \n",
    "    ### Plot 1\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.plot(y_test[:,i], y_test[:,i], 'k.')\n",
    "    plt.plot(predictedY[:,i], y_test[:,i], 'g.')\n",
    "    plt.ylabel('True %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('Predicted %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.legend(['Ideal result', 'predicted ' + feat], loc='upper center')\n",
    "    plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "        batchSize, train_size, test_mse) + \n",
    "        '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "        len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "        test_size), y=1.03, fontsize=20)\n",
    "    plt.show\n",
    "        \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'bp_output_scatter_%d_plot_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')\n",
    "    \n",
    "    ### Plot 2 - boxplot\n",
    "    \n",
    "    # bin_means contain (0: mean of the binned values, 1: bin edges, 2: numbers pointing each example to a bin)\n",
    "    bin_means_true = stats.binned_statistic(x_test[:,i], y_test[:,i], bins=bin_edges)\n",
    "    bin_means_pred = stats.binned_statistic(x_test[:,i], predictedY[:,i].flatten(), bins=bin_edges)\n",
    "    bin_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        bin_center = (bin_means_true[1][iBin] + bin_means_true[1][iBin+1]) / 2\n",
    "        bin_centers.append('%.2f' % (bin_center))\n",
    "    sorted_true_y_data = []\n",
    "    sorted_pred_y_data = []\n",
    "    for iBin in range(1,nBins+1):\n",
    "        sorted_true_y_data.append(y_test[bin_means_true[2] == iBin, i])\n",
    "        sorted_pred_y_data.append(predictedY[bin_means_pred[2] == iBin,i])\n",
    "        \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    bin_pos = np.array([-2,-1]) # (because this makes it work)\n",
    "    x_label_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        # Every boxplot adds 2 boxes, one from the true data and one from the predicted data\n",
    "        bin_pos += 3 \n",
    "        plt.boxplot([sorted_true_y_data[iBin], sorted_pred_y_data[iBin]] , positions = bin_pos, widths = 0.9)\n",
    "        x_label_centers.append(np.mean(bin_pos))\n",
    "    \n",
    "    plt.ylabel('%s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('True Halo mass log($M_{G}/M_{S}$)', fontsize=15)\n",
    "    ax.set_xlim(left=x_label_centers[0]-2, right=x_label_centers[-1]+2)\n",
    "    #xlim(0,bin_pos[1] + 1)\n",
    "    plt.xticks(x_label_centers, bin_centers)\n",
    "    plt.text(12,7,'Left: true data. Right: predicted data.', fontsize=20)\n",
    "    \n",
    "    if feat == 'SFR':\n",
    "        ax.axhline(y=0, linestyle='--')\n",
    "    \n",
    "    plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "        batchSize, train_size, test_mse) + \n",
    "        '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "        len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "        test_size), y=1.03, fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'bp_output_boxplot_%d_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatterplots and plots with errorbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from pylab import plot, show, savefig, xlim, figure, \\\n",
    "#                hold, ylim, legend, boxplot, setp, axes\n",
    "nBins = 8\n",
    "bin_edges = np.linspace(halo_min_mass, halo_max_mass, nBins+1)\n",
    "\n",
    "predictedY = predicted_points\n",
    "\n",
    "for i, feat in enumerate(output_features):\n",
    "    \n",
    "    \n",
    "    ### Plot 1\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.plot(y_test[:,i], y_test[:,i], 'k.')\n",
    "    plt.plot(predictedY[:,i], y_test[:,i], 'g.')\n",
    "    plt.ylabel('True %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('Predicted %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.legend(['Ideal result', 'predicted ' + feat], loc='upper center')\n",
    "    plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "        batchSize, train_size, test_mse) + 'loss function: %s\\n' % (loss_function) +\n",
    "        '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d data points (test) shown' % (\n",
    "        len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "        test_size), y=1.03, fontsize=20)\n",
    "    plt.show\n",
    "        \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'bp_output_scatter_%d_plot_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')\n",
    "    \n",
    "    ### Plot 2 - boxplot\n",
    "    \n",
    "    # bin_means contain (0: mean of the binned values, 1: bin edges, 2: numbers pointing each example to a bin)\n",
    "    bin_means_true = stats.binned_statistic(x_test[:,i], y_test[:,i], bins=bin_edges)\n",
    "    bin_means_pred = stats.binned_statistic(x_test[:,i], predictedY[:,i].flatten(), bins=bin_edges)\n",
    "    bin_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        bin_center = (bin_means_true[1][iBin] + bin_means_true[1][iBin+1]) / 2\n",
    "        bin_centers.append('%.2f' % (bin_center))\n",
    "    sorted_true_y_data = []\n",
    "    sorted_pred_y_data = []\n",
    "    for iBin in range(1,nBins+1):\n",
    "        sorted_true_y_data.append(y_test[bin_means_true[2] == iBin, i])\n",
    "        sorted_pred_y_data.append(predictedY[bin_means_pred[2] == iBin,i])\n",
    "    \n",
    "    # get standard deviations of the binned values\n",
    "    stds_true = np.zeros((nBins))\n",
    "    stds_pred = np.zeros((nBins))\n",
    "    for iBin in range(nBins):\n",
    "        stds_true[iBin] = np.std(sorted_true_y_data[iBin])\n",
    "        stds_pred[iBin] = np.std(sorted_pred_y_data[iBin])\n",
    "        \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    bin_pos = np.array([-2,-1]) # (because this makes it work)\n",
    "    x_label_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        # Every plot adds 2 distributions, one from the true data and one from the predicted data\n",
    "        bin_pos += 3 \n",
    "        plt.errorbar(bin_pos[0], bin_means_true[0][iBin], yerr=stds_true[iBin], fmt = 'bo', capsize=5)\n",
    "        plt.errorbar(bin_pos[1], bin_means_pred[0][iBin], yerr=stds_pred[iBin], fmt = 'ro', capsize=5)\n",
    "        x_label_centers.append(np.mean(bin_pos))\n",
    "    \n",
    "    plt.ylabel('%s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('True Halo mass log($M_{G}/M_{S}$)', fontsize=15)\n",
    "    plt.legend(['True data $\\pm 1 \\sigma$', 'Predicted data $\\pm 1 \\sigma$'], loc='upper left', fontsize='xx-large')\n",
    "    ax.set_xlim(left=x_label_centers[0]-2, right=x_label_centers[-1]+2)\n",
    "    #xlim(0,bin_pos[1] + 1)\n",
    "    plt.xticks(x_label_centers, bin_centers)\n",
    "    \n",
    "    plt.title('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "        batchSize, train_size, test_mse) + 'loss function: %s\\n' % (loss_function) +\n",
    "        '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d data points (test) shown' % (\n",
    "        len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "        test_size), y=1.03, fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'bp_output_boxplot_%d_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(predictedY.flatten())\n",
    "#bin_means = stats.binned_statistic(predictedY.flatten(), y_test[:,0], bins=10)\n",
    "#bin_stds = stats.binned_statistic(predictedY.flatten(), y_test[:,0], bins=10, statistic=GetSTD)\n",
    "#print(bin_means[0])\n",
    "#print(bin_stds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot SFR vs Stellar mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedY = model.predict(x_test_norm)\n",
    "predictedY = predictedY * y_data_stds + y_data_means\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = plt.subplot(121)\n",
    "\n",
    "plt.plot(predictedY[:,0], predictedY[:,1], 'b.', markersize=2)\n",
    "plt.ylabel('Predicted SFR %s' % (unit_dict['SFR']), fontsize=15)\n",
    "plt.xlabel('Predicted Stellar Mass %s' % (unit_dict['Stellar_mass']), fontsize=15)\n",
    "ymin, ymax = ax.get_ylim()\n",
    "ax.set_ylim(bottom=ymin, top=ymax)\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.plot(y_test[:,0], y_test[:,1], 'k.', markersize=2)\n",
    "plt.ylabel('True SFR %s' % (unit_dict['SFR']), fontsize=15)\n",
    "plt.xlabel('True Stellar Mass %s' % (unit_dict['Stellar_mass']), fontsize=15)\n",
    "ax.set_ylim(bottom=ymin, top=ymax)\n",
    "plt.suptitle('nEpochs: %d, batch size: %d, training set size: %d, test mse score: %.2e\\n' % (nEpochs, \n",
    "    batchSize, train_size, total_score) + 'loss function: %s\\n' % (loss_function) +\n",
    "    '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d data points (test) shown' % (\n",
    "    len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "    test_size), y=1.17, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the figure\n",
    "fig.savefig(fig_dir+'bp_sfr_to_stellar_mass_inputs_' + '_and_'.join(input_features)+\n",
    "            '_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check to see how the MSE is calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_points = model.predict(x_test)\n",
    "print(np.shape(predicted_points))\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print(total_score)\n",
    "\n",
    "keras_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(keras_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING\n",
    "coordinates = galaxies[:1000, :3]\n",
    "halo_masses = np.power(10, galaxies[:1000, 6])\n",
    "nr_points = np.shape(coordinates)[0]\n",
    "\n",
    "nr_neighbours_wanted = 30\n",
    "box_sides = np.array([200, 200, 200])\n",
    "\n",
    "neigh_densities = get_density_periodic(coordinates, halo_masses, nr_neighbours_wanted, \n",
    "                                                                 box_sides, nr_points, verbatim=True)\n",
    "print(neigh_densities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
