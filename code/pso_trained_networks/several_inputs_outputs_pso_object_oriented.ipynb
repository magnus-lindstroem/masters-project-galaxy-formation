{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "module_path = home_dir + '/modules/'\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "import time\n",
    "import datetime\n",
    "import importlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "import model_management\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "run_on_cpu = False\n",
    "total_set_size = 3000 # how many examples will be used for training+validation+testing\n",
    "train_size = 1000\n",
    "val_size = 1000\n",
    "test_size = 1000\n",
    "input_features = ['Halo_mass', 'Halo_mass_peak', 'Type', 'Concentration', ]\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "nr_iters_before_restart_check = 60 # start making sure that the network did not converge to a local minimum\n",
    "min_std_tol = 0.01 # minimum allowed std for any parameter\n",
    "plot_threeD = 0\n",
    "save_figs = 0\n",
    "fig_dir = 'figures/'\n",
    "\n",
    "### Network parameters\n",
    "nr_hidden_layers = 10\n",
    "activationFunction = 'tanh'\n",
    "nr_neurons_per_layer = 10\n",
    "\n",
    "### PSO parameters\n",
    "nIterations = 1000\n",
    "nParticles = 40\n",
    "xMin = -10\n",
    "xMax = 10\n",
    "alpha = 1\n",
    "deltaT = 1\n",
    "c1 = 2\n",
    "c2 = 2\n",
    "inertiaWeightStart = 1.4\n",
    "inertiaWeightMin = 0.3\n",
    "explorationFraction = 0.8\n",
    "\n",
    "data_dict = {'X_pos': 0, 'Y_pos': 1, 'Z_pos': 2, 'X_vel': 3, 'Y_vel': 4, 'Z_vel': 5, 'Halo_mass': 6, \n",
    "             'Stellar_mass': 7, 'SFR': 8, 'Intra_cluster_mass': 9, 'Halo_mass_peak': 10, 'Stellar_mass_obs': 11, \n",
    "             'SFR_obs': 12, 'Halo_radius': 13, 'Concentration': 14, 'Halo_spin': 15, 'Scale_peak_mass': 16, \n",
    "             'Scale_half_mass': 17, 'Scale_last_MajM': 18, 'Type': 19}\n",
    "unit_dict = {'X_pos': '', 'Y_pos': '', 'Z_pos': '', 'X_vel': '', 'Y_vel': '', \n",
    "             'Z_vel': '', 'Halo_mass': 'log($M_{G}/M_{S}$)', 'Stellar_mass': 'log($M_{G}/M_{S}$)', 'SFR': '', \n",
    "             'Intra_cluster_mass': '', 'Halo_mass_peak': 'log($M_{G}/M_{S}$)', \n",
    "             'Stellar_mass_obs': '', 'SFR_obs': '', 'Halo_radius': '', \n",
    "             'Concentration': '', 'Halo_spin': '', 'Scale_peak_mass': 'a', \n",
    "             'Scale_half_mass': 'a', 'Scale_last_MajM': 'a', 'Type': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_on_cpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nLayers_10_nNeurons_10_actFun_tanh_nTrainSamples_1000_nIterations_1000_\n"
     ]
    }
   ],
   "source": [
    "### Set name ending with parameters for figures to be saved\n",
    "param_string = 'nLayers_%d_nNeurons_%d_actFun_%s_nTrainSamples_%d_nIterations_%d_' % (\n",
    "    nr_hidden_layers, nr_neurons_per_layer, activationFunction, train_size, nIterations)\n",
    "print(param_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_pos', 'Y_pos', 'Z_pos', 'X_vel', 'Y_vel', 'Z_vel', 'Halo_mass', 'Stellar_mass', 'SFR', 'Intra_cluster_mass', 'Halo_mass_peak', 'Stellar_mass_obs', 'SFR_obs', 'Halo_radius', 'Concentration', 'Halo_spin', 'Scale_peak_mass', 'Scale_half_mass', 'Scale_last_MajM', 'Type']\n",
      "(594946, 20)\n",
      "(306925, 20)\n",
      "[11.06128  10.74806  10.517851 11.935673 11.820144 11.564987 10.759135\n",
      " 10.828308 10.803138 10.55098 ]\n"
     ]
    }
   ],
   "source": [
    "galfile = pd.read_hdf('/scratch/data/galcats/P200/galaxies.Z01.h5')\n",
    "galaxies = galfile.as_matrix()\n",
    "gal_header = galfile.keys().tolist()\n",
    "print(gal_header)\n",
    "\n",
    "### Remove data points with halo mass below 10.5\n",
    "print(np.shape(galaxies))\n",
    "galaxies = galaxies[galaxies[:,6] > 10.5, :]\n",
    "print(np.shape(galaxies))\n",
    "print(galaxies[:10,6])\n",
    "\n",
    "halo_min_mass = np.min(galaxies[:, 6])\n",
    "halo_max_mass = np.max(galaxies[:, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the different data sets that will be used\n",
    "n_data_points = galaxies.shape[0]\n",
    "subset_indices = np.random.choice(n_data_points, total_set_size, replace=False)\n",
    "train_indices = subset_indices[: train_size]\n",
    "val_indices = subset_indices[train_size : train_size+val_size]\n",
    "test_indices = subset_indices[train_size+val_size :]\n",
    "\n",
    "x_train = np.zeros((len(train_indices), len(input_features)))\n",
    "x_val = np.zeros((len(val_indices), len(input_features)))\n",
    "x_test = np.zeros((len(test_indices), len(input_features)))\n",
    "y_train = np.zeros((len(train_indices), len(output_features)))\n",
    "y_val = np.zeros((len(val_indices), len(output_features)))\n",
    "y_test = np.zeros((len(test_indices), len(output_features)))\n",
    "\n",
    "for i in range(len(input_features)):\n",
    "    x_train[:,i] = galaxies[train_indices, data_dict[input_features[i]]]\n",
    "    x_val[:,i] = galaxies[val_indices, data_dict[input_features[i]]]\n",
    "    x_test[:,i] = galaxies[test_indices, data_dict[input_features[i]]]\n",
    "    \n",
    "for i in range(len(output_features)):\n",
    "    y_train[:,i] = galaxies[train_indices, data_dict[output_features[i]]]\n",
    "    y_val[:,i] = galaxies[val_indices, data_dict[output_features[i]]]\n",
    "    y_test[:,i] = galaxies[test_indices, data_dict[output_features[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you want to preprocess the data\n",
    "\n",
    "for i in range(np.size(x_train, 1)):\n",
    "    x_data_means = np.mean(x_train, 0)\n",
    "    x_data_stds = np.std(x_train, 0)\n",
    "\n",
    "    x_train_norm = (x_train - x_data_means) / x_data_stds\n",
    "    x_val_norm = (x_val - x_data_means) / x_data_stds\n",
    "    x_test_norm = (x_test - x_data_means) / x_data_stds\n",
    "\n",
    "for i in range(np.size(y_train, 1)):\n",
    "    y_data_means = np.mean(y_train, 0)\n",
    "    y_data_stds = np.std(y_train, 0)\n",
    "\n",
    "    y_train_norm = (y_train - y_data_means) / y_data_stds\n",
    "    y_val_norm = (y_val - y_data_means) / y_data_stds\n",
    "    y_test_norm = (y_test - y_data_means) / y_data_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00493404 -0.02780973]\n",
      "[1.03360238 0.88024908]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_test_norm, 0))\n",
    "print(np.std(y_test_norm, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halo_mass : min: 1.05e+01, max: 1.41e+01.\n",
      "Halo_mass_peak : min: 1.05e+01, max: 1.41e+01.\n",
      "Type : min: 0.00e+00, max: 2.00e+00.\n",
      "Concentration : min: 1.01e+00, max: 8.28e+02.\n",
      "Stellar_mass : min: 7.00e+00, max: 1.16e+01.\n",
      "SFR : min: 0.00e+00, max: 8.82e+00.\n"
     ]
    }
   ],
   "source": [
    "### Get a feel for the data\n",
    "for i in range(len(input_features)):\n",
    "    print(input_features[i],': min: %.2e, max: %.2e.' % (np.min(x_train[:,i]), np.max(x_train[:,i])))\n",
    "for i in range(len(output_features)):\n",
    "    print(output_features[i],': min: %.2e, max: %.2e.' % (np.min(y_train[:,i]), np.max(y_train[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualisation for when we have 2 input features\n",
    "%matplotlib notebook\n",
    "input_feat_1 = 0\n",
    "input_feat_2 = 1\n",
    "output_feat = 1\n",
    "\n",
    "fig = plt.figure(1, figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train_norm[:500,input_feat_1], x_train_norm[:500,input_feat_2], \n",
    "           y_train_norm[:500,output_feat])\n",
    "ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_1]))\n",
    "ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_2]))\n",
    "ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[output_feat]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, particle 0, new swarm best. Train: 655.916, Val: 655.682\n",
      "Iteration 0, particle 1, new swarm best. Train: 86.213, Val: 81.308\n",
      "Iteration 0, particle 10, new swarm best. Train: 81.350, Val: 87.774\n",
      "Iteration 2, particle 11, new swarm best. Train: 74.987, Val: 75.742\n",
      "Iteration 4, particle 13, new swarm best. Train: 72.318, Val: 69.549\n",
      "Iteration 5, particle 5, new swarm best. Train: 56.699, Val: 57.044\n",
      "Iteration 6, particle 4, new swarm best. Train: 44.404, Val: 47.545\n",
      "Iteration 6, particle 29, new swarm best. Train: 29.236, Val: 28.395\n",
      "Iteration 6, particle 33, new swarm best. Train: 23.205, Val: 27.441\n",
      "Iteration 8, particle 2, new swarm best. Train: 22.878, Val: 24.127\n",
      "Iteration 8, particle 31, new swarm best. Train: 16.787, Val: 17.835\n",
      "Iteration 10\n",
      "Iteration 11, particle 16, new swarm best. Train: 16.203, Val: 17.017\n",
      "Iteration 11, particle 32, new swarm best. Train: 13.609, Val: 13.140\n",
      "Iteration 12, particle 20, new swarm best. Train: 12.914, Val: 12.869\n",
      "Iteration 13, particle 4, new swarm best. Train: 8.869, Val: 8.452\n",
      "Iteration 15, particle 15, new swarm best. Train: 7.951, Val: 7.326\n",
      "Iteration 16, particle 38, new swarm best. Train: 6.942, Val: 5.869\n",
      "Iteration 17, particle 22, new swarm best. Train: 6.370, Val: 6.456\n",
      "Iteration 17, particle 39, new swarm best. Train: 6.237, Val: 5.395\n",
      "Iteration 18, particle 25, new swarm best. Train: 6.189, Val: 6.883\n",
      "Iteration 19, particle 24, new swarm best. Train: 4.512, Val: 4.585\n",
      "Iteration 20\n",
      "Iteration 25, particle 0, new swarm best. Train: 4.324, Val: 4.550\n",
      "Iteration 30\n",
      "Iteration 31, particle 28, new swarm best. Train: 4.227, Val: 4.116\n",
      "Iteration 34, particle 27, new swarm best. Train: 3.777, Val: 3.852\n",
      "Iteration 38, particle 11, new swarm best. Train: 3.506, Val: 3.327\n",
      "Iteration 40\n",
      "Iteration 43, particle 33, new swarm best. Train: 2.998, Val: 3.183\n",
      "Iteration 44, particle 7, new swarm best. Train: 2.252, Val: 2.134\n",
      "Iteration 50\n",
      "Iteration 54, particle 33, new swarm best. Train: 1.923, Val: 2.385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e7a41ebae7a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                       activationFunction)\n\u001b[1;32m      3\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpso_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'inertiaWeightMin'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpso_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-5b917433c528>\u001b[0m in \u001b[0;36mpso_train\u001b[0;34m(self, nr_iterations, x_train, y_train, x_val, y_val, speed_check)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpso_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpso_swarm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnr_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5b917433c528>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, nr_iterations, x_train, y_train, x_val, y_val, speed_check)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0miParticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticle_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                         \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                         \u001b[0mis_swarm_best_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_score\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswarm_best_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5b917433c528>\u001b[0m in \u001b[0;36mevaluate_particle\u001b[0;34m(self, x_data, y_data)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                    steps=steps)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1777\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m                                steps=steps)\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network = Feed_Forward_Neural_Network(nr_hidden_layers, nr_neurons_per_layer, input_features, output_features, \n",
    "                                      activationFunction)\n",
    "network.pso_setup({'inertiaWeightMin': 0.1})\n",
    "network.pso_train(nIterations, x_train_norm, y_train_norm, x_val_norm, y_val_norm, speed_check=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualisation of prediction strength for when we have 2 input features\n",
    "if plot_threeD and len(input_features) == 2:\n",
    "    predictedY = PredictFunc(bestWeightList, bestBiasList, model, 'test')\n",
    "    fig = plt.figure(2, figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               y_test[:,0], s=3)\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               predictedY, s=3)\n",
    "    ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[0]))\n",
    "    ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[1]))\n",
    "    ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nBins = 8\n",
    "bin_edges = np.linspace(halo_min_mass, halo_max_mass, nBins+1)\n",
    "\n",
    "predictedY = model.predict(x_test)\n",
    "\n",
    "for i, feat in enumerate(output_features):\n",
    "    \n",
    "    \n",
    "    ### Plot 1\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax = plt.subplot(211)\n",
    "    plt.plot(y_test[:,i], y_test[:,i], 'k.')\n",
    "    plt.plot(predictedY[:,i], y_test[:,i], 'g.')\n",
    "    plt.ylabel('True %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('Predicted %s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.legend(['Ideal result', 'predicted ' + feat], loc='upper center')\n",
    "    plt.title('nIterations: %d, training set size: %d, test mse score: %.2e\\n' % (nIterations, \n",
    "        train_size, testScore) + \n",
    "        '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "        len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "        test_size), y=1.03, fontsize=20)\n",
    "    plt.show\n",
    "        \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'pso_output_scatter_%d_plot_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')\n",
    "    \n",
    "    ### Plot 2 - boxplot\n",
    "    \n",
    "    # bin_means contain (0: mean of the binned values, 1: bin edges, 2: numbers pointing each example to a bin)\n",
    "    bin_means_true = stats.binned_statistic(x_test[:,i], y_test[:,i], bins=bin_edges)\n",
    "    bin_means_pred = stats.binned_statistic(x_test[:,i], predictedY[:,i].flatten(), bins=bin_edges)\n",
    "    bin_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        bin_centers.append((bin_means_true[1][iBin] + bin_means_true[1][iBin+1]) / 2)\n",
    "    sorted_true_y_data = []\n",
    "    sorted_pred_y_data = []\n",
    "    for iBin in range(1,nBins+1):\n",
    "        sorted_true_y_data.append(y_test[bin_means_true[2] == iBin, i])\n",
    "        sorted_pred_y_data.append(predictedY[bin_means_pred[2] == iBin,i])\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(212)\n",
    "\n",
    "    bin_pos = np.array([-2,-1]) # (because this makes it work)\n",
    "    x_label_centers = []\n",
    "    for iBin in range(nBins):\n",
    "        # Every boxplot adds 2 boxes, one from the true data and one from the predicted data\n",
    "        bin_pos += 3 \n",
    "        plt.boxplot([sorted_true_y_data[iBin], sorted_pred_y_data[iBin]] , positions = bin_pos, widths = 0.9)\n",
    "        x_label_centers.append(np.mean(bin_pos))\n",
    "    \n",
    "    plt.ylabel('%s %s' % (feat, unit_dict[feat]), fontsize=15)\n",
    "    plt.xlabel('True Halo mass log($M_{G}/M_{S}$)', fontsize=15)\n",
    "    ax.set_xlim(left=x_label_centers[0]-2, right=x_label_centers[-1]+2)\n",
    "    #xlim(0,bin_pos[1] + 1)\n",
    "    plt.xticks(x_label_centers, bin_centers) TODO fixa siffrorna\n",
    "    plt.text(12,7,'Left: true data. Right: predicted data.', fontsize=20)\n",
    "    \n",
    "    if feat == 'SFR':\n",
    "        ax.axhline(y=0, linestyle='--')\n",
    "    \n",
    "    #plt.title('nIterations: %d, training set size: %d, test mse score: %.2e\\n' % (nIterations, \n",
    "    #    train_size, testScore) + \n",
    "    #    '%d input feature(s): [%s]\\n%d output feature(s): [%s]\\n%d test data points (test) shown' % (\n",
    "    #    len(input_features), ', '.join(input_features), len(output_features), ', '.join(output_features),\n",
    "    #    test_size), y=1.03, fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save_figs:\n",
    "        fig.savefig(fig_dir+'pso_output_boxplot_%d_from_' % (i+1)+'_and_'.join(input_features)+'_to_'+\n",
    "            '_and_'.join(output_features)+'_with_'+param_string+'.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(trainingScoreHistory, 'b')\n",
    "plt.plot(validationScoreHistory, 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_Neural_Network():\n",
    "    \n",
    "    def __init__(self, nr_hidden_layers, nr_neurons_per_lay, input_features, output_features, \n",
    "                 activation_function):\n",
    "        \n",
    "        self.nr_hidden_layers = nr_hidden_layers\n",
    "        self.nr_neurons_per_lay = nr_neurons_per_lay\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "    def pso_setup(self, pso_param_dict={}):\n",
    "        \n",
    "        self.pso_swarm = PSO_Swarm(self, self.nr_hidden_layers, self.nr_neurons_per_lay, self.input_features, \n",
    "                                   self.output_features, self.activation_function, pso_param_dict=pso_param_dict)\n",
    "        \n",
    "    def pso_train(self, nr_iterations, x_train, y_train, x_val, y_val, speed_check=False):\n",
    "        \n",
    "        self.pso_swarm.train_network(nr_iterations, x_train, y_train, x_val, y_val, speed_check)\n",
    "\n",
    "\n",
    "class PSO_Swarm(Feed_Forward_Neural_Network):\n",
    "    \n",
    "    def __init__(self, parent, nr_hidden_layers, nr_neurons_per_lay, input_features, output_features, \n",
    "                 activation_function, loss_function='mse', metric='mse', pso_param_dict=None):\n",
    "        self.pso_param_dict = {\n",
    "            'nr_particles': 40,\n",
    "            'xMin': -10,\n",
    "            'xMax': 10,\n",
    "            'alpha': 1,\n",
    "            'deltaT': 1,\n",
    "            'c1': 2,\n",
    "            'c2': 2,\n",
    "            'inertiaWeightStart': 1.4,\n",
    "            'inertiaWeightMin': 0.3,\n",
    "            'explorationFraction': 0.8,\n",
    "            'min_std_tol': 0.01\n",
    "        }\n",
    "    \n",
    "        if pso_param_dict is not None:\n",
    "            for key in pso_param_dict:\n",
    "                if key in self.pso_param_dict:\n",
    "                    self.pso_param_dict[key] = pso_param_dict[key]\n",
    "                else:\n",
    "                    print('\\'%s\\ is not a valid key. Choose between:' % (key), self.pso_param_dict.keys())\n",
    "                    break\n",
    "        \n",
    "        self.parent = parent\n",
    "        self.nr_variables = (nr_hidden_layers-1)*nr_neurons_per_lay**2 + \\\n",
    "            (len(input_features)+len(output_features)+nr_hidden_layers)*nr_neurons_per_lay + len(output_features)\n",
    "        self.nr_hidden_layers = nr_hidden_layers\n",
    "        self.nr_neurons_per_lay = nr_neurons_per_lay\n",
    "        self.activation_function = activation_function\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        self.best_weights = None\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.inertia_weight = self.pso_param_dict['inertiaWeightStart']\n",
    "        self.vMax = (self.pso_param_dict['xMax']-self.pso_param_dict['xMin']) / self.pso_param_dict['deltaT']\n",
    "        \n",
    "        self.set_up_model()\n",
    "        \n",
    "        self.initialise_swarm()\n",
    "        \n",
    "    def set_up_model(self):\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(self.nr_neurons_per_lay, input_dim = len(self.input_features), \n",
    "                             activation = self.activation_function))\n",
    "    \n",
    "        for i in range(0, self.nr_hidden_layers-1):\n",
    "            self.model.add(Dense(self.nr_neurons_per_lay, activation = self.activation_function))\n",
    "\n",
    "        self.model.add(Dense(len(self.output_features), activation = None))\n",
    "                \n",
    "        self.model.compile(loss=self.loss_function, metrics=[self.metric], optimizer='adam')\n",
    "        \n",
    "    def train_network(self, nr_iterations, x_train, y_train, x_val, y_val, speed_check):\n",
    "\n",
    "        self.nr_iterations_trained = nr_iterations\n",
    "        self.nr_train_points_used = np.size(x_train, 0)\n",
    "        self.nr_val_points_used = np.size(x_val, 0)\n",
    "        \n",
    "        with open('progress.txt', 'w+') as f:\n",
    "\n",
    "            # make sure the output isn't just the same\n",
    "            shouldStartFresh = 1\n",
    "            while shouldStartFresh:\n",
    "                shouldStartFresh = 0\n",
    "\n",
    "                inertia_weight_reduction = np.exp(np.log(self.pso_param_dict['inertiaWeightMin'] / \n",
    "                                            self.pso_param_dict['inertiaWeightStart']) / \n",
    "                                            self.pso_param_dict['explorationFraction'] / nr_iterations)\n",
    "                inertia_weight = self.pso_param_dict['inertiaWeightStart']\n",
    "\n",
    "                self.validationScoreHistory = []\n",
    "                self.trainingScoreHistory = []\n",
    "                \n",
    "                self.avg_speed_before_history = []\n",
    "                self.avg_speed_after_history = []\n",
    "                    \n",
    "                lastTimeSwarmBest = 0\n",
    "                \n",
    "                self.initialise_swarm()\n",
    "\n",
    "                glob_start = time.time()\n",
    "                for iteration in range(nr_iterations):\n",
    "\n",
    "                    if (int(iteration/10) == iteration/10) and (iteration > 0):\n",
    "                        # see if network has run into a local minima                        \n",
    "                        if (iteration - lastTimeSwarmBest) > nr_iters_before_restart_check:\n",
    "                            self.set_weights(self.best_weights)\n",
    "                            y_pred = self.predict_output(x_val_norm)\n",
    "\n",
    "                            stds = np.std(y_pred, axis=0)\n",
    "                            print('standard deviations of predicted parameters: ', stds)\n",
    "                            shouldStartFresh = np.any(stds < self.pso_param_dict['min_std_tol'])\n",
    "                            if shouldStartFresh:\n",
    "                                break\n",
    "\n",
    "                        progress_end = time.time()\n",
    "                        elapsed_so_far = (progress_end - glob_start) / 60\n",
    "                        time_remaining = elapsed_so_far / iteration * (self.nr_iterations_trained - iteration)\n",
    "\n",
    "                        print('Iteration %d' % (iteration))\n",
    "                        f.write('%s      ' % (datetime.datetime.now().strftime(\"%H:%M:%S\")))\n",
    "                        f.write('Iterations tried: %d/%d     ' % (iteration, self.nr_iterations_trained))\n",
    "                        f.write('Elapsed time: %dmin     ' % (elapsed_so_far))\n",
    "                        f.write('Time remaining: %dmin.\\n' % (time_remaining))\n",
    "                        f.flush()\n",
    "\n",
    "                    for iParticle, particle in enumerate(self.particle_list):\n",
    "                        \n",
    "                        train_score = particle.evaluate_particle(x_train, y_train)\n",
    "\n",
    "                        is_swarm_best_train = (train_score < self.swarm_best_train)\n",
    "                        \n",
    "                        if is_swarm_best_train:\n",
    "                            \n",
    "                        \n",
    "                            lastTimeSwarmBest = iteration\n",
    "                            self.swarm_best_train = train_score\n",
    "                            self.swarm_best_position = particle.position\n",
    "                            \n",
    "                            val_score = particle.evaluate_particle(x_val, y_val)\n",
    "                            is_swarm_best_val = (val_score < self.swarm_best_val)\n",
    "                            if is_swarm_best_val: # only update best weights after val highscore\n",
    "                                self.best_weights = particle.get_weights()\n",
    "                            \n",
    "                            \n",
    "                            self.validationScoreHistory.append(val_score)\n",
    "                            self.trainingScoreHistory.append(train_score)\n",
    "\n",
    "                            print('Iteration %d, particle %d, new swarm best. Train: %.3f, Val: %.3f' % (iteration, \n",
    "                                                            iParticle, train_score, val_score))\n",
    "                            f.write('Iteration %d, particle %d, new swarm best. Train: %.3f, Val: %.3f\\n' % (iteration, \n",
    "                                                            iParticle, train_score, val_score))\n",
    "                            f.flush()\n",
    "\n",
    "\n",
    "                    self.update_swarm(speed_check, f)\n",
    "                    \n",
    "                    inertia_weight = self.update_inertia_weight(inertia_weight, inertia_weight_reduction, \n",
    "                                                                iteration, f)\n",
    "                    \n",
    "                    \n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "    def update_inertia_weight(self, inertia_weight, inertia_weight_reduction, iteration, f):\n",
    "        \n",
    "        isExploring = (inertia_weight > self.pso_param_dict['inertiaWeightMin'])\n",
    "        if isExploring:\n",
    "            inertia_weight = inertia_weight * inertia_weight_reduction\n",
    "            isExploring = (inertia_weight > self.pso_param_dict['inertiaWeightMin'])\n",
    "            if not isExploring:\n",
    "                print('SWITCH TO EPLOIT! Iteration %d/%d.' % (iteration, self.nr_iterations_trained))\n",
    "                f.write('SWITCH TO EPLOIT! Iteration %d/%d.\\n' % (iteration, self.nr_iterations_trained))\n",
    "                f.flush()\n",
    "        return inertia_weight\n",
    "        \n",
    "    def predict_output(self, x_data):\n",
    "    \n",
    "        y_pred = self.model.predict(x_data)  # always contains the best model so far\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def initialise_swarm(self):\n",
    "        \n",
    "        self.particle_list = []\n",
    "        \n",
    "        for i in range(self.pso_param_dict['nr_particles']):\n",
    "            \n",
    "            particle = PSO_Particle(self)\n",
    "            self.particle_list.append(particle)\n",
    "            \n",
    "        self.swarm_best_train = 1e20\n",
    "        self.swarm_best_val = 1e20\n",
    "        self.swarm_best_position = self.particle_list[0].best_position  # arbitrarily take the first position\n",
    "        self.best_particle_nr = 0\n",
    "        \n",
    "    def update_swarm(self, speed_check, f):\n",
    "        \n",
    "        self.speeds_before = []\n",
    "        self.speeds_after = []\n",
    "        #self.term_one = []\n",
    "        #self.term_two = []\n",
    "        #self.too_fast_count = 0\n",
    "        #self.mean_particle_best_difference = []\n",
    "        #self.mean_swarm_best_difference = []\n",
    "        \n",
    "        #q = np.random.uniform(size = self.nr_variables)\n",
    "        #r = np.random.uniform(size = self.nr_variables)\n",
    "        \n",
    "        for particle in self.particle_list:\n",
    "            particle.update_particle()\n",
    "            \n",
    "        #print('term 1: ', np.mean(self.term_one))\n",
    "        #print('term 2:', np.mean(self.term_two)) \n",
    "        #print('%d/%d particles were too fast.' % (self.too_fast_count, self.pso_param_dict['nr_particles']))\n",
    "        #print('mean particle best diff: %.2f'% (np.mean(self.mean_particle_best_difference)))\n",
    "        #print('mean swarm best diff: %.2f'% (np.mean(self.mean_swarm_best_difference)))\n",
    "        #print('q: ', np.mean(q))\n",
    "        #print('r: ', np.mean(r))\n",
    "        avg_speed_before = np.mean(self.speeds_before)\n",
    "        avg_speed_after = np.mean(self.speeds_after)\n",
    "        self.avg_speed_before_history.append(avg_speed_before)\n",
    "        self.avg_speed_after_history.append(avg_speed_after)\n",
    "        \n",
    "        if speed_check:\n",
    "            print('Average speed of the particles before normalization is: ', avg_speed_before)\n",
    "            print('Average speed of the particles after normalization is: ', avg_speed_after)\n",
    "            f.write('Average speed of the particles before normalization is: %.2f' % (avg_speed_before))\n",
    "            f.write('Average speed of the particles after normalization is: %.2f' % (avg_speed_after))\n",
    "            f.flush()\n",
    "            \n",
    "            \n",
    "    def set_weights(self, weightList):\n",
    "        \n",
    "        weightMatrixList = weightList[0]\n",
    "        biasList = weightList[1]\n",
    "        for i in range(len(weightMatrixList)):\n",
    "            self.model.layers[i].set_weights([weightMatrixList[i], biasList[i]])\n",
    "            \n",
    "        \n",
    "class PSO_Particle(PSO_Swarm):\n",
    "        \n",
    "    def __init__(self, parent):\n",
    "        \n",
    "        self.parent = parent\n",
    "            \n",
    "        r1 = np.random.uniform(size=(self.parent.nr_variables))\n",
    "        r2 = np.random.uniform(size=(self.parent.nr_variables))\n",
    "\n",
    "        self.position = self.parent.pso_param_dict['xMin'] + r1 * (self.parent.pso_param_dict['xMax'] - \n",
    "                                self.parent.pso_param_dict['xMin'])\n",
    "        self.velocity = self.parent.pso_param_dict['alpha']/self.parent.pso_param_dict['deltaT'] * \\\n",
    "                        ((self.parent.pso_param_dict['xMin'] - self.parent.pso_param_dict['xMax'])/2 + r2 * \n",
    "                         (self.parent.pso_param_dict['xMax'] - self.parent.pso_param_dict['xMin']))\n",
    "        \n",
    "        self.best_score = 1e20\n",
    "        self.best_position = self.position\n",
    "        \n",
    "        \n",
    "        \n",
    "    def evaluate_particle(self, x_data, y_data):\n",
    "        \n",
    "        weightList = self.get_weights()\n",
    "        self.parent.set_weights(weightList)\n",
    "        \n",
    "        score = self.parent.model.evaluate(x_data, y_data, verbose=0)\n",
    "        if score[0] < self.best_score:\n",
    "            self.best_score = score[0]\n",
    "            self.best_position = self.position\n",
    "            \n",
    "        return score[0]\n",
    "        \n",
    "    def get_weights(self): # sets the weights from the current pos in parameter space\n",
    "        \n",
    "        weightMatrixList = [] # will contain a list of all the weight matrices \n",
    "        biasList = []   # will contain a list of all the biases\n",
    "\n",
    "        weightCounter = 0 # to help assign weights and biases to their correct matrix\n",
    "\n",
    "        ### Extract weight matrices\n",
    "        input_dim = len(self.parent.input_features)\n",
    "        output_dim = len(self.parent.output_features)\n",
    "        weightMatrix = np.zeros((input_dim, self.parent.nr_neurons_per_lay)) \n",
    "        for i in range(input_dim):  \n",
    "            weightMatrix[i,:] = self.position[weightCounter:weightCounter+self.parent.nr_neurons_per_lay]\n",
    "            weightCounter += self.parent.nr_neurons_per_lay\n",
    "        weightMatrixList.append(weightMatrix)\n",
    "\n",
    "        \n",
    "        for iLayer in range(self.parent.nr_hidden_layers-1):\n",
    "            weightMatrix = np.zeros((self.parent.nr_neurons_per_lay, self.parent.nr_neurons_per_lay))\n",
    "            for iNeuron in range(self.parent.nr_neurons_per_lay):\n",
    "\n",
    "                weightMatrix[iNeuron,:] = self.position[weightCounter:weightCounter+self.parent.nr_neurons_per_lay]\n",
    "                weightCounter += self.parent.nr_neurons_per_lay\n",
    "\n",
    "            weightMatrixList.append(weightMatrix)\n",
    "\n",
    "        weightMatrix = np.zeros((self.parent.nr_neurons_per_lay, output_dim))\n",
    "        for i in range(self.parent.nr_neurons_per_lay):  \n",
    "            weightMatrix[i,:] = self.position[weightCounter:weightCounter+output_dim]\n",
    "            weightCounter += output_dim\n",
    "\n",
    "        weightMatrixList.append(weightMatrix)\n",
    "\n",
    "        ### Extract bias vectors\n",
    "        for iLayer in range(self.parent.nr_hidden_layers):\n",
    "\n",
    "            biasVector = self.position[weightCounter:weightCounter+self.parent.nr_neurons_per_lay]\n",
    "            weightCounter += self.parent.nr_neurons_per_lay\n",
    "\n",
    "            biasList.append(biasVector)\n",
    "\n",
    "        biasVector = np.zeros(output_dim)\n",
    "        biasVector = self.position[weightCounter:weightCounter+output_dim] # for the output layer\n",
    "        biasList.append(biasVector)\n",
    "\n",
    "        weightCounter += output_dim\n",
    "        \n",
    "        weightList = [weightMatrixList, biasList]\n",
    "\n",
    "        #print(weightCounter == len(self.position))  # a check if the number of variables is correct\n",
    "        \n",
    "        return weightList\n",
    "\n",
    "    def update_particle(self):\n",
    "\n",
    "        q = np.random.uniform()#size = self.parent.nr_variables)\n",
    "        r = np.random.uniform()#size = self.parent.nr_variables)\n",
    "        #print(q)\n",
    "        #print(r)\n",
    "        particle_best_difference = self.best_position - self.position\n",
    "        swarm_best_difference = self.parent.swarm_best_position - self.position\n",
    "        \n",
    "        #self.parent.mean_particle_best_difference.append(np.mean(np.abs(particle_best_difference)))\n",
    "        #self.parent.mean_swarm_best_difference.append(np.mean(np.abs(swarm_best_difference)))\n",
    "\n",
    "        self.velocity = self.parent.inertia_weight * self.velocity + self.parent.pso_param_dict['c1'] * q * \\\n",
    "                        particle_best_difference / self.parent.pso_param_dict['deltaT'] + \\\n",
    "                        self.parent.pso_param_dict['c2'] * r * swarm_best_difference / \\\n",
    "                        self.parent.pso_param_dict['deltaT']\n",
    "                    \n",
    "        #self.parent.term_one.append(np.mean(np.abs(self.parent.pso_param_dict['c1'] * q * \\\n",
    "        #                particle_best_difference / self.parent.pso_param_dict['deltaT'])))\n",
    "        #self.parent.term_two.append(np.mean(np.abs(self.parent.pso_param_dict['c2'] * r * swarm_best_difference / \\\n",
    "        #                self.parent.pso_param_dict['deltaT'])))\n",
    "\n",
    "        # now limit velocity to vMax\n",
    "        absolute_velocity_before_normalization = np.sqrt(np.sum(np.power(self.velocity, 2)))\n",
    "        is_too_fast = absolute_velocity_before_normalization > self.parent.vMax\n",
    "        if is_too_fast:\n",
    "            #self.parent.too_fast_count += 1\n",
    "            self.velocity = self.velocity * self.parent.vMax / absolute_velocity_before_normalization\n",
    "            \n",
    "        absolute_velocity_after_normalization = np.sqrt(np.sum(np.power(self.velocity, 2)))\n",
    "\n",
    "        self.parent.speeds_before.append(absolute_velocity_before_normalization)\n",
    "        self.parent.speeds_after.append(absolute_velocity_after_normalization)\n",
    "            \n",
    "        self.position = self.position + self.velocity * self.parent.pso_param_dict['deltaT']\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
